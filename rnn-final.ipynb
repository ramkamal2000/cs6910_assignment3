{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/ramkamal2000/cs6910_assignment3/blob/main/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Mount Drive","metadata":{"id":"so1dZJS0GYJD"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"-aAtygLPGbR6","outputId":"4503ff2d-ec9b-4360-a617-4aae33a59f80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installing Required Packages","metadata":{"id":"ebofaXxJGdxw"}},{"cell_type":"code","source":"!pip install editdistance\n!pip install wandb","metadata":{"id":"boxed-david","papermill":{"duration":13.232379,"end_time":"2021-05-17T07:58:43.754273","exception":false,"start_time":"2021-05-17T07:58:30.521894","status":"completed"},"scrolled":true,"tags":[],"outputId":"094c8025-09bf-4d3c-b8db-ed2aff69c35a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{"id":"q_BXG5ejGwc5"}},{"cell_type":"code","source":"import tarfile\nimport os\nimport pandas as pd\nimport editdistance\nimport keras\nimport numpy as np\nimport wandb\nimport tensorflow as tf\nimport shutil\nimport pickle\nfrom keras import layers\nfrom keras.layers import LSTM, Dense, Embedding, Input, TimeDistributed, Dropout\nfrom keras.models import Model, save_model, load_model\nfrom keras.utils.vis_utils import plot_model\nfrom tqdm.auto import tqdm\nfrom keras.layers import Lambda\nfrom keras import backend as K\nfrom math import ceil\nfrom pprint import pprint","metadata":{"id":"removed-sucking","papermill":{"duration":5.537908,"end_time":"2021-05-17T07:58:49.339068","exception":false,"start_time":"2021-05-17T07:58:43.80116","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Current Directory","metadata":{"id":"SlssnZaqG0Eq"}},{"cell_type":"code","source":"# MAKE REQURIRED CHANGES HERE\n\n# dir = '/content'\ndir = '/kaggle/working'\n# dir = os.getcwd()","metadata":{"id":"cognitive-browse","papermill":{"duration":0.033796,"end_time":"2021-05-17T07:58:49.400583","exception":false,"start_time":"2021-05-17T07:58:49.366787","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading Dataset","metadata":{"id":"kZQKCuu0G9WG"}},{"cell_type":"code","source":"!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n\nif not os.path.isdir(dir + '/dakshina_dataset_v1.0'):\n  tarfile.open(dir + '/dakshina_dataset_v1.0.tar').extractall()","metadata":{"id":"peaceful-bridge","papermill":{"duration":18.953473,"end_time":"2021-05-17T07:59:08.380973","exception":false,"start_time":"2021-05-17T07:58:49.4275","status":"completed"},"tags":[],"outputId":"9931f592-9ced-4349-c856-0ed93eb8ec97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logging Onto wandb","metadata":{"id":"Ju2vZCrgHEai"}},{"cell_type":"code","source":"wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c', force=True)","metadata":{"id":"engaging-hunger","papermill":{"duration":0.840661,"end_time":"2021-05-17T07:59:09.629243","exception":false,"start_time":"2021-05-17T07:59:08.788582","status":"completed"},"tags":[],"outputId":"c5d698d5-f3c1-4f9f-ff8b-5fd6ab05c064","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{"id":"adverse-prospect","papermill":{"duration":2.257279,"end_time":"2021-05-17T07:59:13.31615","exception":false,"start_time":"2021-05-17T07:59:11.058871","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class data_loader():\n\n  @staticmethod\n  def _load_raw_df(languages = ['ta']):\n    lex = dict()\n    lex['train'], lex['val'], lex['test'] = [], [], [] \n    column_names = ['output', 'input', 'count']\n    \n    for la in languages:\n      lex['train'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n      lex['val'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n      lex['test'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n\n    lex['train'] = pd.concat(lex['train'])\n    lex['val'] = pd.concat(lex['val'])\n    lex['test'] = pd.concat(lex['test'])\n\n    return lex    \n\n  @staticmethod\n  def _make_final_df(lex):\n    \n    for div in ['train', 'val']:\n    \n      # removing non max transliterations\n      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n      lex[div] = lex[div][idx].reset_index(drop=True)\n\n      # calclulating difference in lengths of various transliterations\n      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n\n      # removing transliterations that vary by a lot in length\n      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n      lex[div] = lex[div][idx].reset_index(drop=True)\n\n      # removing duplicates if any remain\n      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n\n      # removing redundant columns\n      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n\n      # shuffling the dataset i.e. rows of the dataset\n      lex[div] = lex[div].sample(frac=1, random_state=6910)\n      lex[div] = lex[div][idx].reset_index(drop=True)\n\n    lex['test'] = lex['test'].sample(frac=1, random_state=6910)\n    lex['test'].drop(labels=['count'], axis=1, inplace=True)\n    lex['test'] = lex['test'].reset_index(drop=True)\n    return lex\n\n  @staticmethod\n  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n\n    while True:\n        for j in range(0, len(X), batch_size):\n            \n            # placeholder data structures\n            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n\n            # assessing one batch at a time\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n\n                for t, word in enumerate(input_text):\n                  encoder_input_data[i, t] = word\n                for t, word in enumerate(target_text):\n                    if t<len(target_text)-1:\n                        # decoder input sequence\n                        # does not include the <EOW> token\n                        decoder_input_data[i, t] = word \n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the <SOW> token\n                        decoder_target_data[i, t - 1, word] = 1.\n                    \n            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n\n  @staticmethod\n  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n\n    while True:\n        for j in range(0, len(X), batch_size):\n\n            # placeholder data structures\n            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n            \n            # assessing one batch at a time\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text):\n                  encoder_input_data[i, t] = word\n                for t, word in enumerate(target_text):\n                    if t==0 :\n                        decoder_input_data[i, t] = 1 # decoder input seq\n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the START_ token\n                        decoder_target_data[i, t - 1, word] = 1.\n                    \n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","metadata":{"id":"adjustable-radio","papermill":{"duration":0.130224,"end_time":"2021-05-17T07:59:13.547911","exception":false,"start_time":"2021-05-17T07:59:13.417687","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the tokenzier class\nclass Tokenizer:\n\n  # initializing the tokenizer class\n  def __init__(self, df):\n\n    self.start_token = '<SOW>'\n    self.stop_token = '<EOW>'\n    self.unknown_token = '<UNK>'\n\n    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n\n    input_words = df.input.tolist()\n    output_words = df.output.tolist()\n\n    for word in input_words:\n      tokens = str(word)\n      for token in tokens:\n        if token not in self.input_corpus:\n          self.input_corpus.append(token)\n\n    for word in output_words:\n      tokens = str(word)\n      for token in tokens:\n        if token not in self.output_corpus:\n          self.output_corpus.append(token)\n    \n    # input language dictionaries\n    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n    \n    # output language dictionaries\n    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n    self.decode_dict_output.update({2:''})\n\n  # takes in lists of words and returns lists of integers\n  def encode(self, X, mode='input'):\n\n    if (mode=='input'):\n      input_list = []\n      for word in X:\n        word = str(word)\n        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n        input_list.append(integer_list)\n      \n      return input_list\n    \n    if (mode=='output'):\n      output_list = []\n      for word in X:\n        word = str(word)\n        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n        output_list.append(integer_list)\n      \n      return output_list\n    \n  # takes in lists of integers and returns lists of words\n  def decode(self, X, mode='input'):\n\n    if (mode=='input'):\n      input_list = []\n      for integers in X:\n        token_list=[]\n        for integer in integers :\n          if integer == 2 :\n            break\n          token_list.append(self.decode_dict_input.get(integer, '')) \n        input_list.append(''.join(token_list))\n      \n      return input_list\n\n    if (mode=='output'):\n      output_list = []\n      for integers in X:\n        token_list=[]\n        for integer in integers :\n          if integer == 2 :\n            break\n          token_list.append(self.decode_dict_output.get(integer, '')) \n        output_list.append(''.join(token_list))\n      \n      return output_list","metadata":{"id":"supposed-nation","papermill":{"duration":0.127225,"end_time":"2021-05-17T07:59:13.78608","exception":false,"start_time":"2021-05-17T07:59:13.658855","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uses the previous functions to return the final nested data structure\ndef return_data_dict(languages=['ta'], batch_size=32):\n\n  lex = data_loader._load_raw_df(languages)\n  lex = data_loader._make_final_df(lex)\n\n  data_dict = dict()\n\n  df_train = lex['train']\n  df_val = lex['val']\n  df_test = lex['test']\n\n  tk = Tokenizer(df_train)\n\n  data_dict['in_size'] = len(tk.input_corpus) + 1\n  data_dict['out_size'] = len(tk.output_corpus) + 1\n\n  X_train = tk.encode(df_train.input.tolist(), mode='input')\n  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n  \n  X_val = tk.encode(df_val.input.tolist(), mode='input')\n  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n\n  X_test = tk.encode(df_test.input.tolist(), mode='input')\n  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n\n  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n\n\n  data_dict['train']['df'] = df_train\n  data_dict['val']['df'] = df_val\n  data_dict['test']['df'] = df_test\n\n\n  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n  \n  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_val]))\n\n  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n\n  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'])\n  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'])\n\n  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n  data_dict['train']['batch_greedy_big'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], 1024)\n  \n  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n  data_dict['val']['batch_greedy_big'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], 1024)\n\n  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)\n  data_dict['test']['batch_greedy'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)\n  data_dict['test']['batch_greedy_big'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], len(X_test))\n\n  data_dict['tokenizer'] = tk\n\n  return data_dict","metadata":{"id":"aerial-direction","papermill":{"duration":1.490287,"end_time":"2021-05-17T07:59:26.211017","exception":false,"start_time":"2021-05-17T07:59:24.72073","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in case we decide to use more batch sizes in the future\ndict_data_dict = dict()\n\nfor batch_size in [32]:\n  dict_data_dict.update({batch_size: return_data_dict(batch_size=batch_size)})\n\ndata_dict = list(dict_data_dict.values())[0]","metadata":{"id":"determined-rouge","papermill":{"duration":6.65519,"end_time":"2021-05-17T07:59:34.482415","exception":false,"start_time":"2021-05-17T07:59:27.827225","status":"completed"},"tags":[],"outputId":"5e8064c1-efe6-43c3-b6dd-33dca61aa66f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 1\n","metadata":{"id":"recovered-things","papermill":{"duration":0.041762,"end_time":"2021-05-17T07:59:34.566554","exception":false,"start_time":"2021-05-17T07:59:34.524792","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# teacher forcing RNN class\nclass rnn():\n\n  # setting up the \n  def __init__(self, params):\n    \n    num_encode_layers = params['num_encode_layers']\n    num_decode_layers = params['num_decode_layers']\n    data_dict = params['data_dict']\n    in_size = params['data_dict']['in_size']\n    out_size = params['data_dict']['out_size']\n    cell_type = params['cell_type']\n    dropout = params['dropout']\n    embed_size = params['embed_size']\n    rep_size = params['rep_size']\n        \n    ###################### ENCODER NETWORK ######################\n    \n    encoder_inputs = Input(shape=(None,))\n    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n\n    encoder_layers = []\n    \n    for j in range(num_encode_layers-1) :   \n      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n      encoder_layers.append(curr_layer)\n      x = curr_layer(x)\n\n    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n    encoder_layers.append(curr_layer)\n    x, *encoder_states = curr_layer(x)\n\n    ###################### DECODER NETWORK ######################\n\n    decoder_inputs = Input(shape=(None,))\n\n    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n    x = decoder_embedding(decoder_inputs)\n\n    decoder_layers = []    \n    \n    for j in range(num_decode_layers) :\n      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n      decoder_layers.append(curr_layer)\n      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n\n    x = Dropout(dropout)(x)\n    decoder_dense = TimeDistributed(Dense(units=out_size, activation='softmax'))\n    decoder_outputs = decoder_dense(x)\n\n    # define the model that will turn `encoder_inputs` & `decoder_inputs` into `decoder_outputs`\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    self.model = model\n    self.encoder_inputs = encoder_inputs\n    # self.encoder_layers = encoder_layers\n    self.decoder_inputs = decoder_inputs\n    self.decoder_embedding = decoder_embedding\n    self.decoder_layers = decoder_layers\n    self.decoder_dense = decoder_dense\n    self.encoder_states = encoder_states\n    self.params = params\n    self.details = {\n        'model' : self.model,\n        'encoder_inputs' : self.encoder_inputs,\n        # 'encoder_layers' :self.encoder_layers ,\n        'decoder_inputs' :self.decoder_inputs ,\n        'decoder_embedding' : self.decoder_embedding,\n        'decoder_layers' : self.decoder_layers,\n        'decoder_dense' : self.decoder_dense,\n        'encoder_states' : self.encoder_states ,\n        'params' :self.params,\n    }\n\n  def compile_and_fit(self, data_dict, params):\n\n    # compiling the model\n    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n    \n    # printing the summary of the model\n    summary = self.model.summary()\n\n    # plotting the model figure\n    plot = plot_model(self.model, show_shapes=True)\n    \n    # total training samples\n    train_samples = len(data_dict['train']['df'])\n\n    # total validation samples\n    val_samples = len(data_dict['val']['df'])    \n    \n    # batch size\n    batch_size = params['batch_size']\n\n    # number of epochs\n    num_epochs = params['num_epochs']\n\n    # training the model\n    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n                                           steps_per_epoch = train_samples//batch_size,\n                                           epochs=num_epochs,\n                                           callbacks=[\n                                                      wandb.keras.WandbCallback()\n                                                      ],\n                                           validation_data = data_dict['val']['batch'], \n                                           validation_steps = val_samples//batch_size\n                                          )\n\n    return {\n        'run_details' : run_details\n    }\n\n    ","metadata":{"id":"intermediate-vinyl","papermill":{"duration":0.065401,"end_time":"2021-05-17T07:59:34.674168","exception":false,"start_time":"2021-05-17T07:59:34.608767","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RNN class withoout teacher forcing\nclass rnn_second() :\n  def __init__(self, details=None) :\n\n    if details is not None:\n      # copying required details\n      self.details = details\n\n      # copying decoder state input\n      decoder_state_input = self.details['encoder_states']\n\n      decoder_inputs = Input(shape=(1,))\n\n      # copying hidden representation size\n      rep_size = self.details['params']['rep_size']\n\n      # copying decoder inputs\n      # decoder_inputs = self.details['decoder_inputs']\n\n      # the decoder model\n      x = self.details['decoder_embedding'](decoder_inputs)\n    \n      all_outputs = []\n      for _ in range(self.details['params']['data_dict']['max_target_length']) :\n          for layer in self.details['decoder_layers'] :\n              x, *decoder_states = layer(x, initial_state=decoder_state_input)\n\n          x = self.details['decoder_dense'](x)\n\n          # appending the softmax output\n          all_outputs.append(x)\n          x = tf.math.argmax(x, 2) \n          x = self.details['decoder_embedding'](x)\n          \n          # decoder state input for the next time step\n          decoder_state_input = decoder_states\n\n      decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n      model = Model([self.details['encoder_inputs'], decoder_inputs], decoder_outputs)\n      self.model = model\n    \n    else:\n      self.details = None \n      self.model = None\n\n  def compile_and_fit(self, data_dict, params) :\n\n    # compiling the model\n    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\n     # printing the summary of the model   \n    summary = self.model.summary()\n\n    # plotting the model figure\n    plot = plot_model(self.model, show_shapes=True)\n    \n    # total training samples\n    train_samples = len(data_dict['train']['df'])\n\n    # total validation samples \n    val_samples = len(data_dict['val']['df'])\n\n    # batch size   \n    batch_size = params['batch_size']\n\n    # number of epochs\n    num_epochs = params['num_epochs_2']\n\n    # training the model\n    run_details = self.model.fit_generator(generator = data_dict['train']['batch_greedy'],\n                                            steps_per_epoch = train_samples//batch_size,\n                                            epochs=num_epochs,\n                                            callbacks=[\n                                                      wandb.keras.WandbCallback()\n                                                      ],\n                                            validation_data = data_dict['val']['batch_greedy'],\n                                            validation_steps = val_samples//batch_size)\n   \n    return {\n        'run_details' : run_details\n    }\n\n  def evaluate(self, data_dict, train=False) :\n\n    if train :\n      test_gen = data_dict['train']['batch_greedy_big']\n    \n      # number of test samples\n      test_samples = len(data_dict['train']['df']\n    \n      batch_size=1024\n\n      num_hits = 0\n      num_edits = 0\n\n      for i in tqdm(range(test_samples//batch_size)) :\n\n        (a,b),c = next(test_gen)\n        c = np.argmax(c, axis=2)\n        # print(c)\n        l1 = data_dict['tokenizer'].decode(c, mode='output')\n        out = self.model.predict([a,b])\n        out = np.argmax(out,axis=2) \n        l2 = data_dict['tokenizer'].decode(out, mode='output')\n        num_hits += np.sum(np.array(l1)==np.array(l2))\n        num_edits += get_score(l1,l2)\n\n      print(\"Final Train Acc \", num_hits/test_samples)\n      print(\"Editdistance Train Avg \",num_edits/test_samples)\n      wandb.log({\"final_train_acc\":  num_hits/test_samples, \n               \"editdistance_train_acc\":  num_edits/test_samples})\n\n    # test batch generator\n    test_gen = data_dict['val']['batch_greedy_big']\n     \n    # number of test samples\n    test_samples = len(data_dict['val']['df'])\n    # test_samples=100\n    batch_size=1024\n    \n    num_hits = 0\n    num_edits = 0\n    for _ in tqdm(range(test_samples//batch_size)) :\n      (a,b),c = next(test_gen)\n      c = np.argmax(c, axis=2)\n      # print(c)\n      l1 = data_dict['tokenizer'].decode(c, mode='output')\n      out = self.model.predict([a,b])\n      out = np.argmax(out,axis=2) \n      l2 = data_dict['tokenizer'].decode(out, mode='output')\n      num_hits += np.sum(np.array(l1)==np.array(l2))\n      num_edits += get_score(l1,l2)\n      # print(l1, l2)\n      # print(out)\n\n    print(\"Final Val Acc \", num_hits/test_samples)\n    print(\"Editdistance Val Avg \",num_edits/test_samples)\n    wandb.log({\"final_val_acc\":  num_hits/test_samples, \n               \"editdistance_val_acc\":  num_edits/test_samples})\n    \n  def evaluate_test(self, data_dict,filename):\n    test_gen = data_dict['test']['batch_greedy_big']\n     \n    # number of test samples\n    test_samples = len(data_dict['test']['df'])\n    # test_samples=100\n    batch_size=test_samples\n    \n    num_hits = 0\n    num_edits = 0\n\n    X = []\n    Y_true = []\n    Y_pred = []\n    outputs = []\n    \n    for _ in tqdm(range(test_samples//batch_size)) :\n      (a,b),c = next(test_gen)\n      c = np.argmax(c, axis=2)\n      # print(c)\n      l1 = data_dict['tokenizer'].decode(c, mode='output')\n      out = self.model.predict([a,b])\n      out = np.argmax(out,axis=2) \n      l2 = data_dict['tokenizer'].decode(out, mode='output')\n      l3 = data_dict['tokenizer'].decode(a)\n      ###############################################################\n      X.append(l3)\n      Y_true.append(l1)\n      Y_pred.append(l2)\n      \n      num_hits += np.sum(np.array(l1)==np.array(l2))\n      num_edits += get_score(l1,l2)\n      # print(l1, l2)\n      # print(out)\n    print(len(X), len(Y_true), len(Y_pred))\n    df = pd.DataFrame({\n      'X': X[0],\n      'Y_true': Y_true[0],\n      'Y_pred': Y_pred[0]\n    })\n\n    df.to_csv(filename)\n    try:\n      wandb.log({'rnn_greedy_csv': wandb.Table(dataframe=df)})\n    except:\n      pass\n\n#     '''\n#     with open(filename, 'wb') as f:\n#        pickle.dump([inputs,outputs], f)\n#     '''\n    \n    print(\"Final Test Acc \", num_hits/test_samples)\n    print(\"Editdistance Test Avg \",num_edits/test_samples)\n    wandb.log({\"final_test_acc\":  num_hits/test_samples, \n               \"editdistance_test_acc\":  num_edits/test_samples})","metadata":{"id":"divine-richmond","papermill":{"duration":0.075767,"end_time":"2021-05-17T07:59:34.797108","exception":false,"start_time":"2021-05-17T07:59:34.721341","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function For Editdistance","metadata":{"id":"xrHGZ341Piiq"}},{"cell_type":"code","source":"def get_score(A,B) :\n  fin = 0\n  for a,b in zip(A,B) :\n    if len(a)>0 or len(b)>0:   \n      j = editdistance.eval(a,b)\n      fin += 1 - j/max(len(a),len(b))\n  return fin","metadata":{"id":"personalized-festival","papermill":{"duration":0.051102,"end_time":"2021-05-17T07:59:34.892246","exception":false,"start_time":"2021-05-17T07:59:34.841144","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function For Assigning Sweep Parameters","metadata":{"id":"mWBXS9j9Ppb4"}},{"cell_type":"code","source":"class tools:\n  def init_params(config,data_dict):\n  \n    # returning parameters\n    params = {\n        'num_encode_layers' : config.num_encode_layers,\n        'num_decode_layers' : config.num_decode_layers,\n        'cell_type' : config.cell_type,\n        'rep_size' : config.rep_size,\n        'embed_size' : config.embed_size,\n        'dropout' : config.dropout,\n        'num_epochs' : config.num_epochs,\n        'data_dict' : data_dict,\n        'batch_size' : config.batch_size\n    }\n    return params","metadata":{"id":"local-entertainment","papermill":{"duration":0.121081,"end_time":"2021-05-17T07:59:35.344574","exception":false,"start_time":"2021-05-17T07:59:35.223493","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep configuration\nsweep_config = {\n    \n    'method' : 'bayes',\n    'metric' : {\n        'name' : 'val_acc',\n        'goal' : 'maximize'\n    },\n    \n    'parameters': {\n        'cell_type' : {\n            'values': ['LSTM', 'GRU', 'SimpleRNN']  \n        },\n        'embed_size': {\n            'values': [2, 4, 8, 16]\n        },\n        'rep_size': {\n            'values': [32, 64, 128, 256]\n        },\n        'dropout': {\n            'values': [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n        },\n        'batch_size': {\n            'values': [32]\n        },\n        'num_epochs': {\n            'values': [5, 15, 25]\n        },\n        'num_encode_layers': {\n            'values': [1, 2, 3]\n        },\n        'num_decode_layers': {\n            'values': [1, 2, 3]\n        }\n    }\n}","metadata":{"id":"addressed-dietary","papermill":{"duration":0.057882,"end_time":"2021-05-17T07:59:35.540589","exception":false,"start_time":"2021-05-17T07:59:35.482707","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep_config, project='dakshina_v6')","metadata":{"id":"objective-moisture","papermill":{"duration":0.049119,"end_time":"2021-05-17T07:59:35.632609","exception":false,"start_time":"2021-05-17T07:59:35.58349","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sweep_module:\n  @staticmethod\n  def train(config=None):\n\n    with wandb.init(config):\n      \n      # copying the config \n      config = wandb.config\n \n      # naming the run\n      wandb.run.name = 'typ:'+config['cell_type'][:4]+ '_' + 'dro:'+str(config['dropout'])+ '_' + 'enc:' + str(config['num_encode_layers'])+ '_' + 'dec:'+str(config['num_decode_layers'])\n      \n      # returning the data dictionairy\n      data_dict = dict_data_dict[config.batch_size]\n\n      # copying the parameters\n      params = tools.init_params(config,data_dict)\n\n      # creating and training the first model\n      network = rnn(params)\n      run_details = network.compile_and_fit(data_dict, params)\n\n      rnn_2 = rnn_second(network.details)\n      rnn_2.evaluate(data_dict,train=True)","metadata":{"id":"nutritional-madagascar","papermill":{"duration":0.051359,"end_time":"2021-05-17T07:59:35.72684","exception":false,"start_time":"2021-05-17T07:59:35.675481","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = '1gv485mq'","metadata":{"id":"passive-dispatch","papermill":{"duration":0.049495,"end_time":"2021-05-17T07:59:35.819508","exception":false,"start_time":"2021-05-17T07:59:35.770013","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# performing the sweep\n# wandb.agent(sweep_id, sweep_module.train)","metadata":{"id":"measured-fellowship","papermill":{"duration":0.049521,"end_time":"2021-05-17T07:59:35.911865","exception":false,"start_time":"2021-05-17T07:59:35.862344","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best Model","metadata":{"id":"brutal-virus","papermill":{"duration":0.043263,"end_time":"2021-05-17T07:59:35.998604","exception":false,"start_time":"2021-05-17T07:59:35.955341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"params = {\n    'num_encode_layers' : 3,\n    'num_decode_layers' : 1,\n    'cell_type' : 'LSTM', \n    'rep_size' : 128,\n    'embed_size' : 16,\n    'dropout' : 0.5,\n    'num_epochs' : 25,\n    'data_dict' : data_dict,\n    'batch_size' : 32\n}","metadata":{"id":"rolled-banner","papermill":{"duration":0.050485,"end_time":"2021-05-17T07:59:36.09211","exception":false,"start_time":"2021-05-17T07:59:36.041625","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing a run of wandb\nwandb.init(project='final_rnn')","metadata":{"id":"equipped-discretion","papermill":{"duration":3.336961,"end_time":"2021-05-17T07:59:39.472585","exception":false,"start_time":"2021-05-17T07:59:36.135624","status":"completed"},"tags":[],"outputId":"78e4ab6b-60d8-4503-8b90-39c3fb5e3f89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training first RNN\nnetwork = rnn(params)\nnetwork.compile_and_fit(data_dict, params)","metadata":{"id":"awful-polish","papermill":{"duration":1312.945565,"end_time":"2021-05-17T08:21:32.464927","exception":false,"start_time":"2021-05-17T07:59:39.519362","status":"completed"},"tags":[],"outputId":"ed98f98b-16f5-4113-847c-7e1af1e09a3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting up second RNN for inference\nrnn_2 = rnn_second(network.details)\nrnn_2.evaluate(data_dict, train=True)","metadata":{"id":"burning-absolute","papermill":{"duration":147.948201,"end_time":"2021-05-17T08:24:05.431753","exception":false,"start_time":"2021-05-17T08:21:37.483552","status":"completed"},"tags":[],"outputId":"d961a4eb-346e-4c45-8999-12fa129c7ddf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running on test data\nrnn_2.evaluate_test(data_dict,dir+'/rnn_greedy.csv')","metadata":{"id":"incorporate-creation","papermill":{"duration":45.924549,"end_time":"2021-05-17T08:24:56.272618","exception":false,"start_time":"2021-05-17T08:24:10.348069","status":"completed"},"tags":[],"outputId":"86a98bc5-4403-4861-8510-378130ee0633","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam search function 1\ndef decode_sequence_beam(input_seq, k, encoder_model, decoder_model, tk, max_target_length=20, alpha=0.7,getall=False):\n    # encode the input as state vectors\n    states_value = encoder_model.predict(input_seq)\n    # generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # populate the first character of target sequence with the start character.\n    target_seq[0, 0] = 1 \n    run_condition = [True for i in range(k)]\n    # print(len(states_value))\n    # print([target_seq] + [states_value])\n    results, *states_values_temp = decoder_model.predict([target_seq] + [states_value])\n    output_tokens = results\n\n    states_values_k = [states_values_temp for i in range(k)]\n    #get topk indices\n    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n    bestk_ind = ind\n    output_tokens = np.array(output_tokens[0, -1, :])\n    bestk_prob = np.log(output_tokens[ind])\n    bestk_tot = [[bestk_ind[i]] for i in range(k)]\n    # print(bestk_tot)\n\n    \n    while any(run_condition):\n        bestk_tot_new = []\n        bestk_prob_new = []\n        states_values_k_new = []\n        for i in range(k) :\n            if run_condition[i] :\n                a = bestk_tot[i]\n                b = bestk_prob[i]\n                target_seq[0,0] = a[-1]\n                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i],batch_size=1)\n                output_tokens = results\n\n                states_values_k_temp = [states_values_temp for m in range(k)]\n\n                states_values_k_new += states_values_k_temp\n                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n                bestk_ind = ind\n                output_tokens = np.array(output_tokens[0, -1, :])\n                bestk_prob_temp = output_tokens[ind]\n                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n                bestk_prob_temp2 = [(b*(np.power(len(bestk_tot_temp[j])-1,alpha)) + np.log(bestk_prob_temp[j]))/(np.power(len(bestk_tot_temp[j]),alpha)) for j in range(k)]\n                bestk_prob_new += bestk_prob_temp2\n                bestk_tot_new += bestk_tot_temp\n            \n            else :\n                a = bestk_tot[i]\n                b = bestk_prob[i]\n                bestk_tot_new += [bestk_tot[i]]\n                bestk_prob_new += [b]\n                states_values_k_new += [states_values_k[i]]\n\n        bestk_prob_new = np.array(bestk_prob_new)\n        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n        bestk_tot = [bestk_tot_new[i] for i in ind]\n        states_values_k = [states_values_k_new[i] for i in ind]\n        bestk_prob = bestk_prob_new[ind]\n        run_condition = []\n        for i in range(k) :\n            a = bestk_tot[i]\n            b = bestk_prob[i]\n            if a[-1]!= 2 and len(a)<=max_target_length :\n              run_condition.append(True)\n            else :\n              run_condition.append(False)\n\n    final_words = []\n    best_word = []\n    best = -5.0\n    for i in range(k) :\n      a = bestk_tot[i]\n      b = bestk_prob[i]\n      final_words += [a]\n      if b > best :\n        best_word = [a]\n        best = b\n\n    if getall :\n      return (tk.decode(final_words,'output'),best_word)\n    else :\n      return final_words,best_word","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:25:27.198009Z","iopub.status.busy":"2021-05-17T08:25:27.190415Z","iopub.status.idle":"2021-05-17T08:25:27.201479Z","shell.execute_reply":"2021-05-17T08:25:27.200895Z"},"id":"presidential-rochester","papermill":{"duration":4.901487,"end_time":"2021-05-17T08:25:27.201639","exception":false,"start_time":"2021-05-17T08:25:22.300152","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam search function 2\ndef beam_search(details,tokenizer,test_data,out_size,beam,data_dict) :\n  encoder_model = Model(details['encoder_inputs'], details['encoder_states'])\n  rep_size = details['params']['rep_size']\n  decoder_state_input = []\n  for i in range(len(details['encoder_states'])) :\n      new_state = Input(shape=(rep_size,))\n      decoder_state_input.append(new_state)\n  decoder_inputs = details['decoder_inputs']\n  x = details['decoder_embedding'](decoder_inputs)\n\n  for layer in details['decoder_layers'] :\n    x, *decoder_states = layer(x,initial_state=decoder_state_input)\n\n  x = details['decoder_dense'](x)\n  decoder_model = Model(\n      [decoder_inputs] + decoder_state_input,\n      [x] + decoder_states )\n  inp = tokenizer.encode(test_data['df'].input.tolist())\n  out = tokenizer.encode(test_data['df'].output.tolist(),mode='output')\n  val_gen = data_loader._generate_batch(inp,out,data_dict,out_size)\n  acc = 0\n  acc_k = 0\n  num_val = len(inp)\n  \n  X = []\n  Y_true = []\n  Y_pred = []\n\n  for i in tqdm(range(num_val)) :\n    (input_seq,ans) , _ = next(val_gen)\n    K,best = decode_sequence_beam(input_seq,beam,encoder_model,decoder_model,tokenizer,data_dict['max_target_length'],getall=True)\n    w1 = tokenizer.decode(best,mode='output')\n    w2 = tokenizer.decode([ans[0][1:]],mode='output')\n    w3 = tokenizer.decode(input_seq)\n    ###############################################################\n    X.append(w3[0])\n    Y_true.append(w2[0])\n    Y_pred.append(w1[0])\n\n    comp = (w1[0]==w2[0])\n    if comp :\n      acc += 1    \n    if w2[0] in K :\n      acc_k += 1\n\n  acc /= num_val\n  acc_k /= num_val\n\n  filename = dir+'/rnn_beam.csv'\n\n  df = pd.DataFrame({\n      'X': X,\n      'Y_true': Y_true,\n      'Y_pred': Y_pred\n  })\n\n  df.to_csv(filename)\n  try:\n    wandb.log({'rnn_beam_csv': wandb.Table(dataframe=df)})\n  except:\n    pass\n#   '''\n#   with open(filename, 'wb') as f:\n#     pickle.dump([inputs,outputs], f)\n#   '''\n\n  print(\"Val Accuracy : \"+str(acc))\n  print(\"Val Accuracy K : \"+str(acc_k))\n  \n  wandb.log({\"val_acc_rnn_beam\": acc,\n             \"val_acc_rnn_beam_K\" : acc_k})","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:25:37.405668Z","iopub.status.busy":"2021-05-17T08:25:37.404764Z","iopub.status.idle":"2021-05-17T08:25:37.406549Z","shell.execute_reply":"2021-05-17T08:25:37.407133Z"},"id":"boolean-rating","papermill":{"duration":5.059009,"end_time":"2021-05-17T08:25:37.407334","exception":false,"start_time":"2021-05-17T08:25:32.348325","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running beam search with a width of 10\nbeam_search(network.details,data_dict['tokenizer'], data_dict['test'], data_dict['out_size'], 10, data_dict)","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:25:47.799466Z","iopub.status.busy":"2021-05-17T08:25:47.797877Z","iopub.status.idle":"2021-05-17T08:25:47.800345Z","shell.execute_reply":"2021-05-17T08:25:47.801091Z"},"papermill":{"duration":4.868164,"end_time":"2021-05-17T08:25:47.801309","exception":false,"start_time":"2021-05-17T08:25:42.933145","status":"completed"},"tags":[],"id":"abstract-wiring"},"execution_count":null,"outputs":[]}]}