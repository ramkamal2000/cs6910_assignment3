{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d12f56af3c4a47a49361e8225bfbf954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_513f6cf5200848258eb0c5f54d3f6c80",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5e2a089d2fcf4c93b6818099a8b69206",
              "IPY_MODEL_bd416b2d705c462983109c65b3cc89c8"
            ]
          }
        },
        "513f6cf5200848258eb0c5f54d3f6c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e2a089d2fcf4c93b6818099a8b69206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_36b294facb5c4491bc79c4400134cbf1",
            "_dom_classes": [],
            "description": " 74%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c776951a38064de892b2d17ed6f2063f"
          }
        },
        "bd416b2d705c462983109c65b3cc89c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f93b174d75de4c2a83a040340a5a2864",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 147/200 [01:58&lt;00:46,  1.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb71bd7b4a9a421997512e9ca9799569"
          }
        },
        "36b294facb5c4491bc79c4400134cbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c776951a38064de892b2d17ed6f2063f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f93b174d75de4c2a83a040340a5a2864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb71bd7b4a9a421997512e9ca9799569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcVxQrUVxtS"
      },
      "source": [
        "# TO DO\n",
        "1. Figure out dataset\n",
        "2. Break down into individual characters\n",
        "3. Form corpus from dataset for input and output characters\n",
        "4. Assign a number to each character\n",
        "5. Model will have an embedding so it will handle it\n",
        "6. Output will be a bunch of integers so we will have to decode it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRAUpzh1BLEB",
        "outputId": "c45fc405-f60d-4e3f-9b59-1d0a019e3933"
      },
      "source": [
        "!pip install wandb\n",
        "# !pip install tensorflow\n",
        "# !pip install keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/ee/d755f9e5466df64c8416a2c6a860fb3aaa43ed6ea8e8e8e81460fda5788b/wandb-0.10.28-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=e145b1c5fd1bd97b0438b35f92df1fcdbc031d74c83b11d6e3bb3c56d57760f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=981375bdd1e666c76ba3f5c499cae9b7b9b29fe40873ab7a115740d682988c96\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: shortuuid, sentry-sdk, smmap, gitdb, GitPython, configparser, subprocess32, pathtools, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClAC3xAEhKS"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9sbR5xCVo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae14b05-e87b-4f09-d6a2-3061d993c187"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir('/content/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(\"/content/dakshina_dataset_v1.0.tar\").extractall()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-30 10:11:55--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.199.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  33.5MB/s    in 39s     \n",
            "\n",
            "2021-04-30 10:12:34 (49.4 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0fMpPf_BUUK",
        "outputId": "21f6d262-6fed-4b52-f69a-3dfa32fe26cc"
      },
      "source": [
        "wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BstzblcHmd5"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDJrkl5EUHX"
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = [\"hi\", \"mr\"]):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['input', 'output', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val', 'test']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "    ''' Generate a batch of data '''\n",
        "    assert len(X)==len(y)\n",
        "    ind = np.random.permutation(len(X))\n",
        "    X,y = [X[j] for j in ind],[y[j] for j in ind]\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        decoder_input_data[i, t] = word # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        #print(word)\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo1m6s3vDaN"
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<STR>'\n",
        "    self.stop_token = '<STP>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        # integer_list = [self.encode_dict_input[self.start_token]] + [self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word] + [self.encode_dict_input[self.stop_token]]\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "\n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[1:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mzVsRdBa4x"
      },
      "source": [
        "def return_data_dict(languages=['hi', 'mr'], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']\n",
        "  df_val = lex['val']\n",
        "  df_test = lex['test']\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "  X_test = tk.encode(df_test.input.tolist(), mode='input')\n",
        "  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n",
        "\n",
        "  max_source_length = np.max(np.array([len(x) for x in X_train]))\n",
        "  max_target_length = np.max(np.array([len(x) for x in Y_train]))\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n",
        "  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict['train'], data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict['val'], data_dict['out_size'], batch_size)\n",
        "  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict['test'], data_dict['out_size'], batch_size)  \n",
        "  \n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJozmDWedfG"
      },
      "source": [
        "data_dict = return_data_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQhr5hA6xJxv"
      },
      "source": [
        "# data = dict()\n",
        "# data['train'] = dict()\n",
        "# data['train']['X'] = X_train\n",
        "# data['train']['Y'] = Y_train\n",
        "# data['in_size'] = len(tk.input_corpus) + 1\n",
        "# data['out_size'] = len(tk.output_corpus) + 1\n",
        "# num_decoder_tokens = data['out_size'] \n",
        "# max_source_length = np.max(np.array([len(x) for x in X_train]))\n",
        "# max_target_length = np.max(np.array([len(x) for x in Y_train]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_63arXtzSt"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQl8eh2y35p"
      },
      "source": [
        "class BeamSearchCallBack(keras.callbacks.Callback):\n",
        "  def __init__(self,details,test_data,tokenizer,out_size,num_val=200,beam=3) :\n",
        "    self.details = details\n",
        "    self.test_data = test_data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.out_size = out_size\n",
        "    self.num_val = num_val\n",
        "    self.beam = beam\n",
        "\n",
        "  def on_epoch_end(self,epoch,logs=None) :\n",
        "    \n",
        "    encoder_model = Model(self.details['encoder_inputs'], self.details['encoder_states'])\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "    decoder_state_input = []\n",
        "    for i in range(len(self.details['encoder_states'])) :\n",
        "        new_state = Input(shape=(rep_size,))\n",
        "        decoder_state_input.append(new_state)\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "    \n",
        "    for layer in self.details['decoder_layers'] :\n",
        "      x, *decoder_states = layer(x,initial_state=decoder_state_input)\n",
        "\n",
        "    x = self.details['decoder_dense'](x)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_input,\n",
        "        [x] + decoder_states )\n",
        "    inp = self.tokenizer.encode(self.test_data['df'].input.tolist())\n",
        "    out = self.tokenizer.encode(self.test_data['df'].output.tolist(),mode='output')\n",
        "    out_size = self.out_size\n",
        "    val_gen = data_loader._generate_batch(inp,out,self.test_data,self.out_size)\n",
        "    acc = 0\n",
        "    for i in tqdm(range(self.num_val)) :\n",
        "      (input_seq,ans) , _ = next(val_gen)\n",
        "      _,best = decode_sequence_beam(input_seq,self.beam,encoder_model,decoder_model,self.tokenizer,self.test_data['max_target_length'])\n",
        "      w1 = self.tokenizer.decode(best,mode='output')\n",
        "      w2 = self.tokenizer.decode(ans,mode='output')\n",
        "      comp = (w1==w2)\n",
        "      if comp :\n",
        "        acc += 1    \n",
        "\n",
        "    acc /= len(inp)\n",
        "    print(\"Val Accuracy : \"+str(acc))\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cdgAqTLyAr"
      },
      "source": [
        "def decode_sequence_beam(input_seq,k,encoder_model,decoder_model,tk,max_target_length=20,getall=False):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq,batch_size=1,use_multiprocessing=True)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of \n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = 1 \n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    # print([target_seq] + [states_value])\n",
        "    results, *states_values_temp = decoder_model.predict([target_seq] + [states_value])\n",
        "    output_tokens = results\n",
        "\n",
        "    states_values_k = [states_values_temp for i in range(k)]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = output_tokens[ind]\n",
        "    bestk_tot = [[1,bestk_ind[i]] for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            if run_condition[i] :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i],batch_size=1)\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = [states_values_temp for m in range(k)]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n",
        "                bestk_prob_temp2 = [b*bestk_prob_temp[j] for j in range(k)]\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        bestk_prob = bestk_prob_new[ind]\n",
        "        run_condition = []\n",
        "        for i in range(k) :\n",
        "            a = bestk_tot[i]\n",
        "            b = bestk_prob[i]\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = 0.0\n",
        "    for i in range(k) :\n",
        "      a = bestk_tot[i]\n",
        "      b = bestk_prob[i]\n",
        "      final_words += [a]\n",
        "      if b > best :\n",
        "        best_word = [a]\n",
        "\n",
        "    if getall :\n",
        "      return (tk.decode(final_words,'output'),best_word)\n",
        "    else :\n",
        "      return final_words,best_word"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJm8eZrt5mA"
      },
      "source": [
        "class rnn():\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \n",
        "    num_encode_layers = params['num_encode_layers']\n",
        "    num_decode_layers = params['num_decode_layers']\n",
        "    data_dict = params['data_dict']\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    cell_type = params['cell_type']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "        \n",
        "    ###################### ENCODER NETWORK ######################\n",
        "    \n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    encoder_layers = []\n",
        "    \n",
        "    for j in range(num_encode_layers-1) :   \n",
        "      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n",
        "      encoder_layers.append(curr_layer)\n",
        "      x = curr_layer(x)\n",
        "\n",
        "    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n",
        "    encoder_layers.append(curr_layer)\n",
        "    x, *encoder_states = curr_layer(x)\n",
        "\n",
        "    ###################### DECODER NETWORK ######################\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_layers = []    \n",
        "    \n",
        "    for j in range(num_decode_layers) :\n",
        "      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n",
        "      decoder_layers.append(curr_layer)\n",
        "      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(units=out_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    self.model = model\n",
        "    self.encoder_inputs = encoder_inputs\n",
        "    self.encoder_layers = encoder_layers\n",
        "    self.decoder_inputs = decoder_inputs\n",
        "    self.decoder_embedding = decoder_embedding\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.decoder_dense = decoder_dense\n",
        "    self.encoder_states = encoder_states\n",
        "    self.params = params\n",
        "    self.details = {\n",
        "        'model' : self.model,\n",
        "        'encoder_inputs' : self.encoder_inputs,\n",
        "        'encoder_layers' :self.encoder_layers ,\n",
        "        'decoder_inputs' :self.decoder_inputs ,\n",
        "        'decoder_embedding' : self.decoder_embedding,\n",
        "        'decoder_layers' : self.decoder_layers,\n",
        "        'decoder_dense' : self.decoder_dense,\n",
        "        'encoder_states' : self.encoder_states ,\n",
        "        'params' :self.params\n",
        "    }\n",
        "  def compile_and_fit(self, data_dict, params):\n",
        "\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    summary = self.model.summary()\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    train_samples = len(data_dict['train']['df']) # Total Training samples\n",
        "    val_samples = len(data_dict['val']['df'])    # Total validation or test samples\n",
        "    batch_size = params['batch_size']\n",
        "    num_epochs = params['num_epochs']\n",
        "    beam = params['beam_width']\n",
        "    num_val_samples = params['val_test_samples']\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                                            steps_per_epoch = train_samples//batch_size,\n",
        "                                            epochs=num_epochs,\n",
        "                                            callbacks=[BeamSearchCallBack(self.details,data_dict['val'],data_dict['tokenizer'],data_dict['out_size'],num_val=num_val_samples,beam=beam)\n",
        "                                                        #,wandb.keras.WandbCallback()\n",
        "                                                        ]\n",
        "                                            )\n",
        "\n",
        "    # train_ds, val_ds = data['train'], data['val']\n",
        "    # optimizer, epochs = params['optimizer'], params['num_epochs']\n",
        "    \n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDLMoTWfBn86"
      },
      "source": [
        "class tools:\n",
        "  def init_params(config,data_dict):\n",
        "  \n",
        "    \n",
        "    # returning parameters\n",
        "    params = {\n",
        "        'num_encode_layers' : config.num_encode_layers,\n",
        "        'num_decode_layers' : config.num_decode_layers,\n",
        "        'cell_type' : config.cell_type,\n",
        "        'rep_size' : config.rep_size,\n",
        "        'embed_size' : config.embed_size,\n",
        "        'dropout' : config.dropout,\n",
        "        'num_epochs' : config.num_epochs,\n",
        "        'data_dict' : data_dict,\n",
        "        'batch_size' : config.batch_size,\n",
        "        'beam_width' : config.beam,\n",
        "        'val_test_samples' : config.num_val_samples\n",
        "    }\n",
        "    return params"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AWE1lLSDDXh"
      },
      "source": [
        "# sweep configuration\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'Val_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM','GRU','SimpleRNN']  \n",
        "        },\n",
        "        'beam': {\n",
        "            'values': [2,3,4,5]\n",
        "        },\n",
        "        'num_val_samples': {\n",
        "            'values': [200]\n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [16,32,64]\n",
        "        },\n",
        "        'rep_size': {\n",
        "            'values': [32,64,128,256]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0,0.2,0.4,0.5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_encode_layers': {\n",
        "            'values': [1,2,3,4,5]\n",
        "        },\n",
        "        'num_decode_layers': {\n",
        "            'values': [1,2,3,4,5]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqDamLUoDQWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7536cc5b-d84f-4792-ff51-0d9cfe0e8ad6"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='dakshina_v1')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 5to80pdt\n",
            "Sweep URL: https://wandb.ai/ramkamal/dakshina_v1/sweeps/5to80pdt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez3f-pjxDTU8"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      config = wandb.config\n",
        " \n",
        "      #wandb.run.name = 'fil:'+str(config['num_filters_'])+'_type:'+config['type_of_filters'][0]+'_aug:'+str(config['augmentation'])[0]+'_dro:'+str(config['dropout'])[0]\n",
        "      data_dict = return_data_dict(batch_size=config.batch_size)\n",
        "      params = tools.init_params(config,data_dict)\n",
        "      network = rnn(params)\n",
        "      run_details = network.compile_and_fit(data_dict, params)\n",
        "\n",
        "      if os.path.isdir('/content/wandb'): \n",
        "        shutil.rmtree('/content/wandb')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QBd1gJDV-R"
      },
      "source": [
        "sweep_id = '7g0porer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCq49t4zDZod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d12f56af3c4a47a49361e8225bfbf954",
            "513f6cf5200848258eb0c5f54d3f6c80",
            "5e2a089d2fcf4c93b6818099a8b69206",
            "bd416b2d705c462983109c65b3cc89c8",
            "36b294facb5c4491bc79c4400134cbf1",
            "c776951a38064de892b2d17ed6f2063f",
            "f93b174d75de4c2a83a040340a5a2864",
            "cb71bd7b4a9a421997512e9ca9799569"
          ]
        },
        "outputId": "2f7c4bba-80e8-4794-8072-f10722c3af03"
      },
      "source": [
        "# performing the sweep\n",
        "wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mrcgtrjy with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decode_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encode_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_val_samples: 200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trep_size: 128\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.28<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dauntless-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ramkamal/dakshina_v1\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/ramkamal/dakshina_v1/sweeps/5to80pdt\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v1/sweeps/5to80pdt</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/ramkamal/dakshina_v1/runs/mrcgtrjy\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v1/runs/mrcgtrjy</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210430_102126-mrcgtrjy</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 32)     2304        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (None, None, 128)    62208       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, None, 128)    99072       gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 32)     960         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     [(None, 128), (None, 99072       gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_3 (GRU)                     [(None, None, 128),  62208       embedding_1[0][0]                \n",
            "                                                                 gru_2[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_4 (GRU)                     [(None, None, 128),  99072       gru_3[0][0]                      \n",
            "                                                                 gru_2[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_5 (GRU)                     [(None, None, 128),  99072       gru_4[0][0]                      \n",
            "                                                                 gru_2[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     [(None, None, 128),  99072       gru_5[0][0]                      \n",
            "                                                                 gru_2[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 30)     3870        gru_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 626,910\n",
            "Trainable params: 626,910\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1420/1420 [==============================] - 106s 37ms/step - loss: 0.8601 - acc: 0.3721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d12f56af3c4a47a49361e8225bfbf954",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBotvdIuDczo"
      },
      "source": [
        "# Run One Model separate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNYvJntHnUSA"
      },
      "source": [
        "params1 = {\n",
        "    'num_encode_layers' : 2,\n",
        "    'num_decode_layers' : 1,\n",
        "    'cell_type' : 'LSTM', # SimpleRNN, LSTM\n",
        "    'rep_size' : 20,\n",
        "    'embed_size' : 20,\n",
        "    'dropout' : 0,\n",
        "    'num_epochs' : 1,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32,\n",
        "    'beam_width' : 3,\n",
        "    'val_test_samples' : 200\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfaOoDTt-Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61eda1e6-91dc-4f4f-a613-83bfa0e0af52"
      },
      "source": [
        "params = {\n",
        "    'num_encode_layers' : 2,\n",
        "    'num_decode_layers' : 1,\n",
        "    'cell_type' : 'LSTM', # SimpleRNN, LSTM\n",
        "    'rep_size' : 20,\n",
        "    'embed_size' : 20,\n",
        "    'dropout' : 0,\n",
        "    'num_epochs' : 10,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32,\n",
        "    'beam_width' : 3,\n",
        "    'val_test_samples' : 200\n",
        "}\n",
        "network = rnn(params)\n",
        "plot_model(network.model, show_shapes=True)\n",
        "network.compile_and_fit(data_dict, params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, None, 20)     1440        input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 20)     3280        embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, None, 20)     600         input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 20), (None,  3280        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 20), ( 3280        embedding_7[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, None, 30)     630         lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 12,510\n",
            "Trainable params: 12,510\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 33s 16ms/step - loss: 1.0020 - acc: 0.2854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/200 [00:03<12:46,  3.85s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "100%|██████████| 200/200 [03:01<00:00,  1.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 2/10\n",
            "   8/1420 [..............................] - ETA: 23s - loss: 0.8358 - acc: 0.3905"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 24s 17ms/step - loss: 0.7946 - acc: 0.3931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:15<00:00,  1.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 3/10\n",
            "  10/1420 [..............................] - ETA: 24s - loss: 0.6853 - acc: 0.4663"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 23s 16ms/step - loss: 0.6572 - acc: 0.4862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:15<00:00,  1.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 4/10\n",
            "  10/1420 [..............................] - ETA: 25s - loss: 0.5823 - acc: 0.5497"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 23s 16ms/step - loss: 0.5717 - acc: 0.5526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:13<00:00,  1.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 5/10\n",
            "   9/1420 [..............................] - ETA: 22s - loss: 0.5297 - acc: 0.5939"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 24s 17ms/step - loss: 0.5225 - acc: 0.5947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:06<00:00,  1.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 6/10\n",
            "   8/1420 [..............................] - ETA: 22s - loss: 0.4980 - acc: 0.6172"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 23s 16ms/step - loss: 0.4830 - acc: 0.6277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:06<00:00,  1.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 7/10\n",
            "  10/1420 [..............................] - ETA: 23s - loss: 0.4813 - acc: 0.6356"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 24s 17ms/step - loss: 0.4508 - acc: 0.6561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:10<00:00,  1.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 8/10\n",
            "  11/1420 [..............................] - ETA: 23s - loss: 0.4144 - acc: 0.6783"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 25s 18ms/step - loss: 0.4263 - acc: 0.6764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:11<00:00,  1.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 9/10\n",
            "   9/1420 [..............................] - ETA: 22s - loss: 0.3947 - acc: 0.6927"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 24s 17ms/step - loss: 0.4079 - acc: 0.6912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:16<00:00,  1.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n",
            "Epoch 10/10\n",
            "   8/1420 [..............................] - ETA: 23s - loss: 0.3487 - acc: 0.7154"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1420/1420 [==============================] - 24s 17ms/step - loss: 0.3918 - acc: 0.7043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:17<00:00,  1.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Accuracy : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'run_details': <tensorflow.python.keras.callbacks.History at 0x7f595b7b7510>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viovwIYlks_f"
      },
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "x = Embedding(data_dict['in_size'], 64,mask_zero=True)(encoder_inputs)\n",
        "x = LSTM(units=256,return_sequences=True)(x)\n",
        "x, *encoder_states = LSTM(units=256,\n",
        "                           return_state=True)(x)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "decoder_embedding =  Embedding(data_dict['out_size'], 64,mask_zero=True)\n",
        "x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "decoder_LSTM = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "x, *decoder_states = decoder_LSTM(x, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(units=data_dict['out_size'], activation='softmax')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBE6qh5T-xCi"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = decoder_embedding(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs_2, *decoder_states_2 = decoder_LSTM(dec_emb2\n",
        "                                                    ,initial_state=decoder_state_input\n",
        "                                                    )\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs_2 = decoder_dense(decoder_outputs_2)\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_state_input,\n",
        "    [decoder_outputs_2] + decoder_states_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpIBCBmmxqa5"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of \n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = 1\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    chars = [1]\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 2 :\n",
        "          stop_condition = True\n",
        "        chars.append(sampled_token_index)\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "    return tk.decode([chars],'output')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "VWVDjxxrLhhH",
        "outputId": "ec28cdf7-c367-4e50-ba13-ddfcb3aac517"
      },
      "source": [
        "plot_model(decoder_model,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-82f3040ad444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8wpmBFk-Bzd"
      },
      "source": [
        "test_gen = generate_batch(X_test, Y_test, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u014_k7MS48"
      },
      "source": [
        "x1 = tk.encode(['टोप्पर​'])\n",
        "x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtQr3YImNO7W"
      },
      "source": [
        "decode_sequence(x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9nFZEzPQ3uL"
      },
      "source": [
        "lex['test'].input.tolist()[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5L3BteyEHT"
      },
      "source": [
        "k += 1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "# print(input_seq)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input Source sentence:', tk.decode([X_test[k]])[0] )\n",
        "print('Actual Target Translation:', tk.decode([Y_test[k]],mode='output')[0])\n",
        "print('Predicted Target Translation:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItqoHkE2JTO1"
      },
      "source": [
        "# Romanized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8dFDOHGbEb"
      },
      "source": [
        "ta_rom = dict()\n",
        "ta_rom['rejoined'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['rejoined_aligned_cased'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv', sep='\\t', header=None, error_bad_lines=False) \n",
        "ta_rom['rejoined_aligned'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['split'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv', sep='\\t', header=None, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WGvG_RJqsr"
      },
      "source": [
        "list(ta_rom['rejoined'].iloc[0, 0])[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75rYpZkNCJV"
      },
      "source": [
        "ta_rom['rejoined_aligned_cased']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E9hQuQMULO"
      },
      "source": [
        "ta_rom['rejoined_aligned']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAnW0rAKDY5"
      },
      "source": [
        "ta_rom['split']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD7CedwSKaoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84677de7-0a8a-42b2-d53b-cce635ced12a"
      },
      "source": [
        "l1 = [1,4,2,3]\n",
        "l2 = [1,4,2,5]\n",
        "print(np.array(l1[1:-1])==np.array(l2[1:-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}