{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5904.958316,
      "end_time": "2021-05-17T08:56:56.004384",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-05-17T07:18:31.046068",
      "version": "2.3.2"
    },
    "colab": {
      "name": "Copy of notebook7d7c9814d9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramkamal2000/cs6910_assignment3/blob/main/attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0cs3veZGUgH"
      },
      "source": [
        "# Mount Drive"
      ],
      "id": "-0cs3veZGUgH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4DJDDt2GWB2",
        "outputId": "c72c7c74-d539-45a6-ee1e-4555aecd21f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Z4DJDDt2GWB2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRSrSGNGkC5"
      },
      "source": [
        "# Installing Required Packages"
      ],
      "id": "InRSrSGNGkC5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:18:36.257741Z",
          "iopub.status.busy": "2021-05-17T07:18:36.256383Z",
          "iopub.status.idle": "2021-05-17T07:18:36.260460Z",
          "shell.execute_reply": "2021-05-17T07:18:36.259960Z"
        },
        "id": "anticipated-fourth",
        "papermill": {
          "duration": 0.029045,
          "end_time": "2021-05-17T07:18:36.260580",
          "exception": false,
          "start_time": "2021-05-17T07:18:36.231535",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e207bd-77c0-47ea-8799-93a48e6a65b6"
      },
      "source": [
        "!pip install wandb"
      ],
      "id": "anticipated-fourth",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (8.0.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 42.5MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=3a0b18994faf8e367daf47e0f6e56ac111a36f36ce2d4d1ddaa0e606ebfbcb6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=10971eba60fe80ade897b410f3db90e43d08198c4271eec1a88cd9e2b401db77\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: configparser, docker-pycreds, shortuuid, subprocess32, pathtools, smmap, gitdb, GitPython, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WxEXheGtId"
      },
      "source": [
        "# Importing Required Libraries"
      ],
      "id": "p5WxEXheGtId"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:18:36.310139Z",
          "iopub.status.busy": "2021-05-17T07:18:36.309655Z",
          "iopub.status.idle": "2021-05-17T07:18:41.934179Z",
          "shell.execute_reply": "2021-05-17T07:18:41.933110Z"
        },
        "id": "standard-reasoning",
        "papermill": {
          "duration": 5.652578,
          "end_time": "2021-05-17T07:18:41.934358",
          "exception": false,
          "start_time": "2021-05-17T07:18:36.281780",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tqdm.auto import tqdm\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "import datetime\n",
        "from math import ceil\n",
        "from pprint import pprint\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Concatenate, TimeDistributed,Dropout\n",
        "from tensorflow.python.keras.models import Model"
      ],
      "id": "standard-reasoning",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJGE3rr4G4HF"
      },
      "source": [
        "# Setting Current Directory"
      ],
      "id": "CJGE3rr4G4HF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:18:41.984657Z",
          "iopub.status.busy": "2021-05-17T07:18:41.983710Z",
          "iopub.status.idle": "2021-05-17T07:18:41.986330Z",
          "shell.execute_reply": "2021-05-17T07:18:41.985902Z"
        },
        "papermill": {
          "duration": 0.028997,
          "end_time": "2021-05-17T07:18:41.986472",
          "exception": false,
          "start_time": "2021-05-17T07:18:41.957475",
          "status": "completed"
        },
        "tags": [],
        "id": "equivalent-certificate"
      },
      "source": [
        "dir = '/content'\n",
        "# dir = '/kaggle/working'"
      ],
      "id": "equivalent-certificate",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sU5GLNYG6tB"
      },
      "source": [
        "# Downloading Dataset"
      ],
      "id": "3sU5GLNYG6tB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:18:42.036397Z",
          "iopub.status.busy": "2021-05-17T07:18:42.035854Z",
          "iopub.status.idle": "2021-05-17T07:19:05.147896Z",
          "shell.execute_reply": "2021-05-17T07:19:05.147294Z"
        },
        "id": "continental-credit",
        "papermill": {
          "duration": 23.139598,
          "end_time": "2021-05-17T07:19:05.148041",
          "exception": false,
          "start_time": "2021-05-17T07:18:42.008443",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbfa3f2-0180-46b5-a02f-bacf64c4129e"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir(dir + '/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(dir + \"/dakshina_dataset_v1.0.tar\").extractall()"
      ],
      "id": "continental-credit",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘dakshina_dataset_v1.0.tar’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxFcLPvHCJI"
      },
      "source": [
        "# Logging Onto wandb"
      ],
      "id": "xvxFcLPvHCJI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:14.798215Z",
          "iopub.status.busy": "2021-05-17T07:19:14.797314Z",
          "iopub.status.idle": "2021-05-17T07:19:15.856203Z",
          "shell.execute_reply": "2021-05-17T07:19:15.855327Z"
        },
        "id": "royal-martin",
        "papermill": {
          "duration": 1.158591,
          "end_time": "2021-05-17T07:19:15.856331",
          "exception": false,
          "start_time": "2021-05-17T07:19:14.697740",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16331ace-cade-4fab-c6c9-95a6ba90f106"
      },
      "source": [
        "wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')\n",
        "wandb.init(project='final_attn')"
      ],
      "id": "royal-martin",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incoming-pound",
        "papermill": {
          "duration": 1.32883,
          "end_time": "2021-05-17T07:19:17.569451",
          "exception": false,
          "start_time": "2021-05-17T07:19:16.240621",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Loading Data"
      ],
      "id": "incoming-pound"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:17.732004Z",
          "iopub.status.busy": "2021-05-17T07:19:17.730951Z",
          "iopub.status.idle": "2021-05-17T07:19:17.763913Z",
          "shell.execute_reply": "2021-05-17T07:19:17.771634Z"
        },
        "id": "official-chick",
        "papermill": {
          "duration": 0.129157,
          "end_time": "2021-05-17T07:19:17.771877",
          "exception": false,
          "start_time": "2021-05-17T07:19:17.642720",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = [\"ta\"]):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['output', 'input', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "      # shuffling the dataset i.e. rows of the dataset\n",
        "      lex[div] = lex[div].sample(frac=1, random_state=6910)\n",
        "      lex[div] = lex[div].reset_index(drop=True)\n",
        "\n",
        "    lex['test'] = lex['test'].sample(frac=1, random_state=6910)\n",
        "    lex['test'].drop(labels=['count'], axis=1, inplace=True)\n",
        "    lex['test'] = lex['test'].reset_index(drop=True)\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            \n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "\n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        # decoder input sequence\n",
        "                        # does not include the <EOW> token\n",
        "                        decoder_input_data[i, t] = word \n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the <SOW> token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "            \n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t==0 :\n",
        "                        decoder_input_data[i, t] = 1 # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "id": "official-chick",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:17.974876Z",
          "iopub.status.busy": "2021-05-17T07:19:17.973905Z",
          "iopub.status.idle": "2021-05-17T07:19:17.989573Z",
          "shell.execute_reply": "2021-05-17T07:19:17.990717Z"
        },
        "id": "numeric-exception",
        "papermill": {
          "duration": 0.1239,
          "end_time": "2021-05-17T07:19:17.990940",
          "exception": false,
          "start_time": "2021-05-17T07:19:17.867040",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<SOW>'\n",
        "    self.stop_token = '<EOW>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "    self.decode_dict_output.update({2:''})\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "id": "numeric-exception",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:18.107632Z",
          "iopub.status.busy": "2021-05-17T07:19:18.106147Z",
          "iopub.status.idle": "2021-05-17T07:19:18.108674Z",
          "shell.execute_reply": "2021-05-17T07:19:18.109073Z"
        },
        "id": "instant-coordinate",
        "papermill": {
          "duration": 0.059174,
          "end_time": "2021-05-17T07:19:18.109215",
          "exception": false,
          "start_time": "2021-05-17T07:19:18.050041",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def return_data_dict(languages=[\"ta\"], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']\n",
        "  df_val = lex['val']\n",
        "  df_test = lex['test']\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  \n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "  \n",
        "  X_test = tk.encode(df_test.input.tolist(), mode='input')\n",
        "  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n",
        "\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  \n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "  \n",
        "  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n",
        "  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "\n",
        "\n",
        "  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'], data_dict['test']['max_source_length'])\n",
        "  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'], data_dict['test']['max_target_length'])\n",
        "\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  \n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "\n",
        "  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)  \n",
        "  data_dict['test']['batch_greedy'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)    \n",
        "  data_dict['test']['batch_1'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], 1)\n",
        "\n",
        "\n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "id": "instant-coordinate",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:18.192882Z",
          "iopub.status.busy": "2021-05-17T07:19:18.192062Z",
          "iopub.status.idle": "2021-05-17T07:19:23.183791Z",
          "shell.execute_reply": "2021-05-17T07:19:23.184245Z"
        },
        "id": "chubby-ferry",
        "papermill": {
          "duration": 5.036059,
          "end_time": "2021-05-17T07:19:23.184471",
          "exception": false,
          "start_time": "2021-05-17T07:19:18.148412",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "data_dict = return_data_dict()"
      ],
      "id": "chubby-ferry",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWxPL4iJa9t"
      },
      "source": [
        "# saving test dataframe for later use\n",
        "# data_dict['test']['df'].to_csv('/content/drive/MyDrive/Colab Notebooks/DL3/test.csv')"
      ],
      "id": "MqWxPL4iJa9t",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "christian-delicious",
        "papermill": {
          "duration": 3.302699,
          "end_time": "2021-05-17T07:19:26.531528",
          "exception": false,
          "start_time": "2021-05-17T07:19:23.228829",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Attention - Question 5"
      ],
      "id": "christian-delicious"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:28.248741Z",
          "iopub.status.busy": "2021-05-17T07:19:28.247733Z",
          "iopub.status.idle": "2021-05-17T07:19:28.291057Z",
          "shell.execute_reply": "2021-05-17T07:19:28.291766Z"
        },
        "id": "jewish-seattle",
        "papermill": {
          "duration": 0.141565,
          "end_time": "2021-05-17T07:19:28.291940",
          "exception": false,
          "start_time": "2021-05-17T07:19:28.150375",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        # print(input_shape[0][2],input_shape[1][2])\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        # print(\"attn called\")\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs"
      ],
      "id": "jewish-seattle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:32.955415Z",
          "iopub.status.busy": "2021-05-17T07:19:32.954355Z",
          "iopub.status.idle": "2021-05-17T07:19:32.958877Z",
          "shell.execute_reply": "2021-05-17T07:19:32.959970Z"
        },
        "id": "organic-location",
        "papermill": {
          "duration": 1.353483,
          "end_time": "2021-05-17T07:19:32.960198",
          "exception": false,
          "start_time": "2021-05-17T07:19:31.606715",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "class attn_rnn():\n",
        "  def __init__(self,params) :\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "\n",
        "    encoder_emb = Embedding(in_size, embed_size, input_length=None,mask_zero=True)\n",
        "    temp1 = encoder_emb(encoder_inputs)\n",
        "    encoder_lstm = LSTM(rep_size, return_sequences=True, return_state=True,dropout=dropout)\n",
        "    encoder_out, *encoder_state = encoder_lstm(temp1)\n",
        "    \n",
        "\n",
        "    decoder_emb = Embedding(out_size, embed_size , input_length=None,mask_zero=True)\n",
        "    temp2 = decoder_emb(decoder_inputs)\n",
        "    decoder_lstm = LSTM(rep_size, return_sequences=True, return_state=True,dropout=dropout)\n",
        "    decoder_out, *decoder_state = decoder_lstm(temp2, initial_state=encoder_state)\n",
        "\n",
        "    # Attention layer\n",
        "    attn_layer = AttentionLayer()\n",
        "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "    # Concat attention input and decoder GRU output\n",
        "    decoder_concat_input = Concatenate(axis=-1)([decoder_out, attn_out])\n",
        "\n",
        "    # Dense layer\n",
        "    drop = Dropout(dropout)\n",
        "    decoder_concat_input = drop(decoder_concat_input)\n",
        "    dense = Dense(out_size, activation='softmax')\n",
        "    dense_time = TimeDistributed(dense)\n",
        "    decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "    # Full model\n",
        "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "\n",
        "    batch_size = 1\n",
        "\n",
        "    \"\"\" Encoder (Inference) model \"\"\"\n",
        "    encoder_inf_inputs = Input(shape=(None,))\n",
        "    temp3 = encoder_emb(encoder_inf_inputs)\n",
        "    encoder_inf_out, *encoder_inf_state = encoder_lstm(temp3)\n",
        "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "    \"\"\" Decoder (Inference) model \"\"\"\n",
        "    decoder_inf_inputs = Input(shape=(None,))\n",
        "    encoder_inf_states = Input(shape=(None, rep_size))\n",
        "    decoder_init_stateh = Input(shape=(rep_size,))\n",
        "    decoder_init_statec = Input(shape=(rep_size,))\n",
        "    decoder_init_state = [decoder_init_stateh,decoder_init_statec]\n",
        "    temp = decoder_emb(decoder_inf_inputs)\n",
        "    decoder_inf_out, *decoder_inf_state = decoder_lstm(temp, initial_state=decoder_init_state)\n",
        "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "    decoder_inf_concat = Concatenate(axis=-1)([decoder_inf_out, attn_inf_out])\n",
        "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
        "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "\n",
        "    self.model = full_model\n",
        "    self.encoder = encoder_model\n",
        "    self.decoder = decoder_model\n",
        "\n",
        "  def compile_and_fit(self,data_dict,params):\n",
        "\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "    self.model.summary()\n",
        "    batch_size=params['batch_size']\n",
        "    epochs=params['num_epochs']\n",
        "    val_samples = len(data_dict['val']['df'])\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                 steps_per_epoch=train_samples//batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=data_dict['val']['batch'],\n",
        "                 validation_steps=val_samples//batch_size\n",
        "                 )\n",
        "    \n",
        "  def evaluate(self,data_dict,filename) :\n",
        "    generator = data_dict['test']['batch_1']\n",
        "    tk = data_dict['tokenizer']\n",
        "    acc = 0\n",
        "    num = len(data_dict['test']['df'])\n",
        "    \n",
        "    X = []\n",
        "    Y_true = []\n",
        "    Y_pred = []\n",
        "    attn = []\n",
        "    for j in tqdm(range(num)) :\n",
        "      (a,b),c = next(generator)\n",
        "      g = [np.argmax(c,axis=-1)[0]]\n",
        "      enc_outs, enc_last_state = self.encoder.predict(a)\n",
        "      dec_state = enc_last_state\n",
        "      attention_weights = []\n",
        "      word = []\n",
        "      for i in range(23):\n",
        "\n",
        "          dec_out, attention, dec_state = self.decoder.predict([enc_outs, dec_state, b])\n",
        "          dec_ind = np.argmax(dec_out, axis=-1)\n",
        "\n",
        "          word.append(dec_ind[0][0])\n",
        "          b = dec_ind\n",
        "          attention_weights.append((dec_ind, attention))\n",
        "          if dec_ind[0][0] == 2 :\n",
        "            break\n",
        "      str1 = tk.decode(g,mode='output')\n",
        "      str2 = tk.decode([word],mode='output')\n",
        "      \n",
        "      ###############################################################\n",
        "      <X.append(<INPUT STRING!>)>\n",
        "\n",
        "      Y_true.append(str1[0])\n",
        "      Y_pred.append(str2[0])\n",
        "\n",
        "      attn.append(attention_weights)\n",
        "      # print(word)\n",
        "      # print(str1[0],str2[0])\n",
        "      if str1[0] == str2[0] :\n",
        "        acc += 1\n",
        "   \n",
        "   df = pd.DataFrame({\n",
        "      'X': X,\n",
        "      'Y_true': Y_true,\n",
        "      'Y_pred': Y_pred,\n",
        "      'Weights': attn_weights\n",
        "    })\n",
        "\n",
        "    df.to_csv(filename)\n",
        "    try:\n",
        "      wandb.log({'attn_greedy_csv': df})\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "   '''\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump([inputs,outputs,attn], f)\n",
        "    '''\n",
        "    \n",
        "    print(\"Test accuracy : \",acc/num)\n",
        "\n",
        "  def get_attn_weights(self, a, b) :\n",
        "    enc_outs, enc_last_state = self.encoder.predict(a)\n",
        "    dec_state = enc_last_state\n",
        "    attention_weights = []\n",
        "    word = []\n",
        "    for i in range(23):\n",
        "\n",
        "        dec_out, attention, dec_state = self.decoder.predict([enc_outs, dec_state, b])\n",
        "        dec_ind = np.argmax(dec_out, axis=-1)\n",
        "\n",
        "        word.append(dec_ind[0][0])\n",
        "        b = dec_ind\n",
        "        attention_weights.append((dec_ind, attention))\n",
        "        if dec_ind[0][0] == 2 :\n",
        "          break\n",
        "\n",
        "    return attention_weights\n"
      ],
      "id": "organic-location",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isADhSUYKdwL"
      },
      "source": [
        "# Attention Model Params"
      ],
      "id": "isADhSUYKdwL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:33.146872Z",
          "iopub.status.busy": "2021-05-17T07:19:33.145844Z",
          "iopub.status.idle": "2021-05-17T07:19:33.148446Z",
          "shell.execute_reply": "2021-05-17T07:19:33.147690Z"
        },
        "id": "hybrid-bearing",
        "papermill": {
          "duration": 0.079244,
          "end_time": "2021-05-17T07:19:33.148624",
          "exception": false,
          "start_time": "2021-05-17T07:19:33.069380",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "params = {\n",
        "    'rep_size' : 128,\n",
        "    'embed_size' : 16,\n",
        "    'dropout' : 0.5,\n",
        "    'num_epochs' : 15,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32\n",
        "}"
      ],
      "id": "hybrid-bearing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T07:19:33.301333Z",
          "iopub.status.busy": "2021-05-17T07:19:33.300497Z",
          "iopub.status.idle": "2021-05-17T08:01:21.775013Z",
          "shell.execute_reply": "2021-05-17T08:01:21.774469Z"
        },
        "id": "marked-process",
        "papermill": {
          "duration": 2508.542193,
          "end_time": "2021-05-17T08:01:21.775155",
          "exception": false,
          "start_time": "2021-05-17T07:19:33.232962",
          "status": "completed"
        },
        "tags": [],
        "outputId": "26ea0b93-08a5-44ab-9f3f-f45bd7af0ea0"
      },
      "source": [
        "network = attn_rnn(params)\n",
        "network.compile_and_fit(data_dict,params)"
      ],
      "id": "marked-process",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 16)     480         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 16)     800         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 128),  74240       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  74240       embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 128),  32896       lstm[0][0]                       \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, 256)    0           lstm_1[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 256)    0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 50)     12850       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 195,506\n",
            "Trainable params: 195,506\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2071/2071 [==============================] - 173s 78ms/step - loss: 0.8773 - acc: 0.3019 - val_loss: 0.3619 - val_acc: 0.6979\n",
            "Epoch 2/15\n",
            "2071/2071 [==============================] - 161s 78ms/step - loss: 0.3828 - acc: 0.6876 - val_loss: 0.1815 - val_acc: 0.8569\n",
            "Epoch 3/15\n",
            "2071/2071 [==============================] - 165s 80ms/step - loss: 0.2425 - acc: 0.8127 - val_loss: 0.1476 - val_acc: 0.8836\n",
            "Epoch 4/15\n",
            "2071/2071 [==============================] - 168s 81ms/step - loss: 0.2001 - acc: 0.8464 - val_loss: 0.1326 - val_acc: 0.8950\n",
            "Epoch 5/15\n",
            "2071/2071 [==============================] - 168s 81ms/step - loss: 0.1773 - acc: 0.8636 - val_loss: 0.1233 - val_acc: 0.9025\n",
            "Epoch 6/15\n",
            "2071/2071 [==============================] - 168s 81ms/step - loss: 0.1622 - acc: 0.8756 - val_loss: 0.1186 - val_acc: 0.9062\n",
            "Epoch 7/15\n",
            "2071/2071 [==============================] - 167s 81ms/step - loss: 0.1512 - acc: 0.8839 - val_loss: 0.1139 - val_acc: 0.9091\n",
            "Epoch 8/15\n",
            "2071/2071 [==============================] - 166s 80ms/step - loss: 0.1427 - acc: 0.8903 - val_loss: 0.1111 - val_acc: 0.9107\n",
            "Epoch 9/15\n",
            "2071/2071 [==============================] - 172s 83ms/step - loss: 0.1360 - acc: 0.8948 - val_loss: 0.1098 - val_acc: 0.9122\n",
            "Epoch 10/15\n",
            "2071/2071 [==============================] - 168s 81ms/step - loss: 0.1292 - acc: 0.8994 - val_loss: 0.1092 - val_acc: 0.9136\n",
            "Epoch 11/15\n",
            "2071/2071 [==============================] - 162s 78ms/step - loss: 0.1253 - acc: 0.9019 - val_loss: 0.1059 - val_acc: 0.9158\n",
            "Epoch 12/15\n",
            "2071/2071 [==============================] - 163s 79ms/step - loss: 0.1211 - acc: 0.9052 - val_loss: 0.1060 - val_acc: 0.9166\n",
            "Epoch 13/15\n",
            "2071/2071 [==============================] - 165s 79ms/step - loss: 0.1181 - acc: 0.9075 - val_loss: 0.1038 - val_acc: 0.9175\n",
            "Epoch 14/15\n",
            "2071/2071 [==============================] - 169s 82ms/step - loss: 0.1147 - acc: 0.9102 - val_loss: 0.1073 - val_acc: 0.9175\n",
            "Epoch 15/15\n",
            "2071/2071 [==============================] - 168s 81ms/step - loss: 0.1111 - acc: 0.9127 - val_loss: 0.1045 - val_acc: 0.9179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T08:01:38.398807Z",
          "iopub.status.busy": "2021-05-17T08:01:38.398116Z",
          "iopub.status.idle": "2021-05-17T08:52:36.886480Z",
          "shell.execute_reply": "2021-05-17T08:52:36.887208Z"
        },
        "id": "accessory-plaza",
        "papermill": {
          "duration": 3066.838908,
          "end_time": "2021-05-17T08:52:36.887486",
          "exception": false,
          "start_time": "2021-05-17T08:01:30.048578",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "6e98858b80b44987aef28d90fea112c7"
          ]
        },
        "outputId": "bc6f0034-4f53-41d9-f78b-2a0700915528"
      },
      "source": [
        "network.evaluate(data_dict,'/kaggle/working/attn_greedy.csv')"
      ],
      "id": "accessory-plaza",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e98858b80b44987aef28d90fea112c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6833 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy :  0.5332943070393678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T08:52:53.112362Z",
          "iopub.status.busy": "2021-05-17T08:52:53.110448Z",
          "iopub.status.idle": "2021-05-17T08:52:53.112913Z",
          "shell.execute_reply": "2021-05-17T08:52:53.113301Z"
        },
        "id": "creative-injection",
        "papermill": {
          "duration": 7.991566,
          "end_time": "2021-05-17T08:52:53.113453",
          "exception": false,
          "start_time": "2021-05-17T08:52:45.121887",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def plot_attention_weights(encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None):\n",
        "#     \"\"\"\n",
        "#     Plots attention weights\n",
        "#     :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
        "#     :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
        "#     :param en_id2word: dict\n",
        "#     :param fr_id2word: dict\n",
        "#     :return:\n",
        "#     \"\"\"\n",
        "\n",
        "#     if len(attention_weights) == 0:\n",
        "#         print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
        "#               '\\nPlease check if the decoder produced  a proper translation')\n",
        "#         return\n",
        "\n",
        "#     mats = []\n",
        "#     dec_inputs = []\n",
        "#     l = 0\n",
        "#     encoder_inputs = encoder_inputs.flatten()\n",
        "#     for i in range(len(encoder_inputs)) :\n",
        "#       l += 1\n",
        "#       if encoder_inputs[i] == 0 :\n",
        "#         break\n",
        "#     for dec_ind, attn in attention_weights:\n",
        "#         mats.append(attn.reshape(-1)[:l])\n",
        "#         dec_inputs.append(dec_ind)\n",
        "#     attention_mat = np.transpose(np.array(mats))\n",
        "\n",
        "\n",
        "#     fig, ax = plt.subplots(figsize=(10, 10))\n",
        "#     ax.imshow(attention_mat)\n",
        "\n",
        "#     ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
        "#     ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
        "#     encoder_inputs = encoder_inputs[:l]\n",
        "#     print([(inp,fr_id2word[inp]) for inp in np.array(dec_inputs).ravel()]) \n",
        "#     ax.set_xticklabels([inp for inp in np.array(dec_inputs).ravel()])\n",
        "#     ax.set_yticklabels([en_id2word[inp] if inp != 0 else \"Nan\" for inp in np.array(encoder_inputs).ravel()])\n",
        "\n",
        "#     ax.tick_params(labelsize=32)\n",
        "#     ax.tick_params(axis='x', labelrotation=90)\n",
        "\n",
        "#     if filename is None:\n",
        "#         plt.savefig('attention.png')\n"
      ],
      "id": "creative-injection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T08:53:09.975666Z",
          "iopub.status.busy": "2021-05-17T08:53:09.974805Z",
          "iopub.status.idle": "2021-05-17T08:53:09.976981Z",
          "shell.execute_reply": "2021-05-17T08:53:09.976359Z"
        },
        "id": "structured-advertising",
        "papermill": {
          "duration": 8.676944,
          "end_time": "2021-05-17T08:53:09.977127",
          "exception": false,
          "start_time": "2021-05-17T08:53:01.300183",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# plot_attention_weights(a,attention_weights,data_dict['tokenizer'].decode_dict_input,data_dict['tokenizer'].decode_dict_output)"
      ],
      "id": "structured-advertising",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T08:53:26.380064Z",
          "iopub.status.busy": "2021-05-17T08:53:26.378348Z",
          "iopub.status.idle": "2021-05-17T08:53:26.380757Z",
          "shell.execute_reply": "2021-05-17T08:53:26.381145Z"
        },
        "id": "attempted-nirvana",
        "papermill": {
          "duration": 8.108319,
          "end_time": "2021-05-17T08:53:26.381276",
          "exception": false,
          "start_time": "2021-05-17T08:53:18.272957",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def decode_sequence_beam_attn(input_seq,target_seq, k, encoder_model, decoder_model, tk, max_target_length=20, getall=False,alpha=0.7):\n",
        "    # encode the input as state vectors\n",
        "    enc_outs,states_value = encoder_model.predict(input_seq)\n",
        "    # generate empty target sequence of length 1.\n",
        "    # populate the first character of target sequence with the start character.\n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    # print([target_seq] + [states_value])\n",
        "    results, attn,states_values_temp = decoder_model.predict([enc_outs , states_value,target_seq])\n",
        "    output_tokens = results\n",
        "    # print(results)\n",
        "    states_values_k = [states_values_temp for i in range(k)]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    # print(ind)\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = np.log(output_tokens[ind])\n",
        "    bestk_tot = [[bestk_ind[i]] for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "    # print(bestk_prob)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        # print(\"##############\")\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            # print(\"**\")\n",
        "            if run_condition[i] :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,attn,states_values_temp = decoder_model.predict([enc_outs,states_values_k[i],target_seq],batch_size=1)\n",
        "\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = [states_values_temp for m in range(k)]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                # print(np.log(bestk_prob_temp))\n",
        "                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n",
        "                # print(bestk_tot_temp)\n",
        "                bestk_prob_temp2 = [(b*(np.power(len(bestk_tot_temp[j])-1,alpha)) + np.log(bestk_prob_temp[j]))/(np.power(len(bestk_tot_temp[j]),alpha)) for j in range(k)]\n",
        "                # print(bestk_prob_temp2)\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        bestk_prob = bestk_prob_new[ind]\n",
        "        run_condition = []\n",
        "        for i in range(k) :\n",
        "            a = bestk_tot[i]\n",
        "            b = bestk_prob[i]\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = -5.0\n",
        "    for i in range(k) :\n",
        "      a = bestk_tot[i]\n",
        "      b = bestk_prob[i]\n",
        "      final_words += [a]\n",
        "      if b > best :\n",
        "        best_word = [a]\n",
        "        best = b\n",
        "\n",
        "    if getall :\n",
        "      return (tk.decode(final_words,'output'),best_word)\n",
        "    else :\n",
        "      return final_words,bestk_prob, best_word"
      ],
      "id": "attempted-nirvana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-05-17T08:53:43.237083Z",
          "iopub.status.busy": "2021-05-17T08:53:43.235017Z",
          "iopub.status.idle": "2021-05-17T08:53:43.237798Z",
          "shell.execute_reply": "2021-05-17T08:53:43.238301Z"
        },
        "id": "round-stage",
        "papermill": {
          "duration": 8.642327,
          "end_time": "2021-05-17T08:53:43.238472",
          "exception": false,
          "start_time": "2021-05-17T08:53:34.596145",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "##run beam search\n",
        "\n",
        "# generator = data_dict['test']['batch_1']\n",
        "# tk = data_dict['tokenizer']\n",
        "# m2 = network.encoder\n",
        "# m3 = network.decoder\n",
        "# acc = 0\n",
        "# num = len(data_dict['test']['df'])\n",
        "# acc_k = 0\n",
        "# # num = 50\n",
        "# X = []\n",
        "# Y_true = []\n",
        "# Y_pred = []\n",
        "# for j in tqdm(range(num)) :\n",
        "#   (a,b),c = next(generator)\n",
        "#   g = [np.argmax(c,axis=-1)[0]]\n",
        "#   attention_weights = []\n",
        "#   K,word = decode_sequence_beam_attn(a,b,10,m2,m3,data_dict['tokenizer'],max_target_length=data_dict['max_target_length'],getall=True )\n",
        "#   str1 = tk.decode(g,mode='output')\n",
        "#   str2 = tk.decode(word,mode='output')\n",
        "    \n",
        "    ###############################################################\n",
        "    <X.append(<INPUT STRING!>)>\n",
        "\n",
        "#   Y_true.append(str1[0])\n",
        "#   Y_pred.append(str2[0])\n",
        "\n",
        "#   if str1[0] in K :\n",
        "#     acc_k += 1\n",
        "  \n",
        "#   if str1[0] == str2[0] :\n",
        "#     acc += 1\n",
        "    \n",
        "#   if (j+1) %500 == 0 :\n",
        "#     print(acc/num)\n",
        "#     print(acc_k/num)\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "    'X': X,\n",
        "    'Y_true': Y_true,\n",
        "    'Y_pred': Y_pred\n",
        "  })\n",
        "\n",
        "  df.to_csv(filename)\n",
        "  try:\n",
        "    wandb.log({'attn_beam_csv': df})\n",
        "  except:\n",
        "    pass\n",
        "  '''    \n",
        "# with open('/kaggle/working/beam_attn_temp', 'wb') as f:\n",
        "#         pickle.dump([inputs,outputs], f)\n",
        "  '''   \n",
        "\n",
        "# print(acc/num)\n",
        "# print(acc_k/num)"
      ],
      "id": "round-stage",
      "execution_count": null,
      "outputs": []
    }
  ]
}