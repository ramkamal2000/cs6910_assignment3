{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcVxQrUVxtS"
      },
      "source": [
        "# TO DO\n",
        "1. Figure out dataset\n",
        "2. Break down into individual characters\n",
        "3. Form corpus from dataset for input and output characters\n",
        "4. Assign a number to each character\n",
        "5. Model will have an embedding so it will handle it\n",
        "6. Output will be a bunch of integers so we will have to decode it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClAC3xAEhKS"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9sbR5xCVo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7b4ba1-ab67-41c9-f948-8fefe53676b6"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir('/content/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(\"/content/dakshina_dataset_v1.0.tar\").extractall() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 03:34:46--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 173.194.202.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  38.6MB/s    in 19s     \n",
            "\n",
            "2021-04-27 03:35:05 (100 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BstzblcHmd5"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDJrkl5EUHX"
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = [\"hi\", \"mr\"]):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['input', 'output', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val', 'test']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        decoder_input_data[i, t] = word # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        #print(word)\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo1m6s3vDaN"
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<STR>'\n",
        "    self.stop_token = '<STP>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        # integer_list = [self.encode_dict_input[self.start_token]] + [self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word] + [self.encode_dict_input[self.stop_token]]\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "\n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '0') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '0') for integer in integers[1:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mzVsRdBa4x"
      },
      "source": [
        "def return_data_dict(languages=['hi', 'mr'], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']\n",
        "  df_val = lex['val']\n",
        "  df_test = lex['test']\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "  X_test = tk.encode(df_test.input.tolist(), mode='input')\n",
        "  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n",
        "\n",
        "  max_source_length = np.max(np.array([len(x) for x in X_train]))\n",
        "  max_target_length = np.max(np.array([len(x) for x in Y_train]))\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n",
        "  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict['train'], data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict['val'], data_dict['out_size'], batch_size)\n",
        "  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict['test'], data_dict['out_size'], batch_size)  \n",
        "  \n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJozmDWedfG"
      },
      "source": [
        "data_dict = return_data_dict()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQhr5hA6xJxv"
      },
      "source": [
        "# data = dict()\n",
        "# data['train'] = dict()\n",
        "# data['train']['X'] = X_train\n",
        "# data['train']['Y'] = Y_train\n",
        "# data['in_size'] = len(tk.input_corpus) + 1\n",
        "# data['out_size'] = len(tk.output_corpus) + 1\n",
        "# num_decoder_tokens = data['out_size'] \n",
        "# max_source_length = np.max(np.array([len(x) for x in X_train]))\n",
        "# max_target_length = np.max(np.array([len(x) for x in Y_train]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_63arXtzSt"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQl8eh2y35p"
      },
      "source": [
        "class BeamSearchCallBack(keras.callbacks.Callback):\n",
        "  def __init__(self,details,test_data) :\n",
        "    self.details = details\n",
        "    self.test_data = test_data\n",
        "\n",
        "  def on_epoch_end(self,epoch,logs=None) :\n",
        "    \n",
        "    encoder_model = Model(self.details['encoder_inputs'], self.details['encoder_states'])\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "    decoder_state_input = []\n",
        "    for i in range(len(self.details['encoder_states'])) :\n",
        "        new_state = Input(shape=(rep_size,))\n",
        "        decoder_state_input.append(new_state)\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "    \n",
        "    for layer in self.details['decoder_layers'] :\n",
        "      x, *decoder_states = layer(x,initial_state=decoder_state_input)\n",
        "\n",
        "    x = self.details['decoder_dense'](x)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_input,\n",
        "        [x] + decoder_states )\n",
        "    \n",
        "    inp = self.test_data['df'].input.tolist()\n",
        "    out = self.test_data['df'].output.tolist()\n",
        "    acc = 0\n",
        "    \n",
        "    for i in range(len(inp)) :\n",
        "      if i%200==0 :\n",
        "        print(i)\n",
        "      input_seq = [inp[i]]\n",
        "\n",
        "      _,best = decode_sequence_beam(input_seq,3,encoder_model,decoder_model,self.test_data['max_target_length'])\n",
        "      if best == out[i] :\n",
        "        acc += 1\n",
        "\n",
        "    acc /= len(inp)\n",
        "    print(\"Val Accuracy : \"+str(acc))\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cdgAqTLyAr"
      },
      "source": [
        "def decode_sequence_beam(input_seq,k,encoder_model,decoder_model,max_target_length=50):\n",
        "    # Encode the input as state vectors.\n",
        "    *states_value, = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of \n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = 1 \n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    results, *states_values_temp = decoder_model.predict([target_seq] + states_value)\n",
        "    output_tokens = results\n",
        "\n",
        "    states_values_k = []\n",
        "    for m in range(k) :\n",
        "      states_values_k += [states_values_temp]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = output_tokens[ind]\n",
        "    bestk_tot = [([bestk_ind[i]],bestk_prob[i],1) for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            if run_condition[i] :\n",
        "                a,b,c = bestk_tot[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i])\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = []\n",
        "                for m in range(k) :\n",
        "                  states_values_k_temp += [states_values_temp]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                bestk_tot_temp = [(a+[bestk_ind[j]],b*bestk_prob_temp[j],c+1) for j in range(k)]\n",
        "                bestk_prob_temp2 = [b*bestk_prob_temp[j] for j in range(k)]\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a,b,c = bestk_tot[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        run_condition = []\n",
        "        for ele in bestk_tot :\n",
        "            a,b,c = ele\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = 0.0\n",
        "    for ele in bestk_tot :\n",
        "      a,b,c = ele\n",
        "      final_words += [[1]+a]\n",
        "      if b > best :\n",
        "        best_word = [[1]+b]\n",
        "\n",
        "\n",
        "    return (tk.decode(final_words,'output'),best_word)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJm8eZrt5mA"
      },
      "source": [
        "class rnn():\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \n",
        "    num_encode_layers = params['num_encode_layers']\n",
        "    num_decode_layers = params['num_decode_layers']\n",
        "    data_dict = params['data_dict']\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    cell_type = params['cell_type']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "        \n",
        "    ###################### ENCODER NETWORK ######################\n",
        "    \n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    encoder_layers = []\n",
        "    \n",
        "    for j in range(num_encode_layers-1) :   \n",
        "      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n",
        "      encoder_layers.append(curr_layer)\n",
        "      x = curr_layer(x)\n",
        "\n",
        "    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n",
        "    encoder_layers.append(curr_layer)\n",
        "    x, *encoder_states = curr_layer(x)\n",
        "\n",
        "    ###################### DECODER NETWORK ######################\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_layers = []    \n",
        "    \n",
        "    for j in range(num_decode_layers) :\n",
        "      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n",
        "      decoder_layers.append(curr_layer)\n",
        "      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(units=out_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    self.model = model\n",
        "    self.encoder_inputs = encoder_inputs\n",
        "    self.encoder_layers = encoder_layers\n",
        "    self.decoder_inputs = decoder_inputs\n",
        "    self.decoder_embedding = decoder_embedding\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.decoder_dense = decoder_dense\n",
        "    self.encoder_states = encoder_states\n",
        "    self.params = params\n",
        "    self.details = {\n",
        "        'model' : self.model,\n",
        "        'encoder_inputs' : self.encoder_inputs,\n",
        "        'encoder_layers' :self.encoder_layers ,\n",
        "        'decoder_inputs' :self.decoder_inputs ,\n",
        "        'decoder_embedding' : self.decoder_embedding,\n",
        "        'decoder_layers' : self.decoder_layers,\n",
        "        'decoder_dense' : self.decoder_dense,\n",
        "        'encoder_states' : self.encoder_states ,\n",
        "        'params' :self.params\n",
        "    }\n",
        "  def compile_and_fit(self, data_dict, params):\n",
        "\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    summary = self.model.summary()\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    train_samples = len(data_dict['train']['df']) # Total Training samples\n",
        "    val_samples = len(data_dict['val']['df'])    # Total validation or test samples\n",
        "    batch_size = params['batch_size']\n",
        "    num_epochs = params['num_epochs']\n",
        "    \n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                                            steps_per_epoch = train_samples//batch_size,\n",
        "                                            epochs=num_epochs,\n",
        "                                            callbacks=[BeamSearchCallBack(self.details,data_dict['val'])]\n",
        "                                            )\n",
        "\n",
        "    # train_ds, val_ds = data['train'], data['val']\n",
        "    # optimizer, epochs = params['optimizer'], params['num_epochs']\n",
        "    \n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfaOoDTt-Zd"
      },
      "source": [
        "params = {\n",
        "    'num_encode_layers' : 3,\n",
        "    'num_decode_layers' : 1,\n",
        "    'cell_type' : 'GRU', # SimpleRNN, LSTM\n",
        "    'rep_size' : 20,\n",
        "    'embed_size' : 20,\n",
        "    'dropout' : 0,\n",
        "    'num_epochs' : 10,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GdrR7RD-qzs"
      },
      "source": [
        "network = rnn(params)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYBDqHM__5Z3"
      },
      "source": [
        "plot_model(network.model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYmCF1R6Afg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8c6fa41-a8fd-4919-9e32-be560911f31b"
      },
      "source": [
        "network.compile_and_fit(data_dict, params)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 20)     1440        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, None, 20)     2520        embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_9 (GRU)                     (None, None, 20)     2520        gru_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 20)     600         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_10 (GRU)                    [(None, 20), (None,  2520        gru_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_11 (GRU)                    [(None, None, 20), ( 2520        embedding_5[0][0]                \n",
            "                                                                 gru_10[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 30)     630         gru_11[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 12,750\n",
            "Trainable params: 12,750\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1420/1420 [==============================] - 39s 19ms/step - loss: 0.9852 - acc: 0.2687\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ed90fdfb8c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-c53a8bbe1e4b>\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(self, data_dict, params)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                             \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBeamSearchCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                                             )\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-3e2c9f01927b>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_target_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ce59ef28d500>\u001b[0m in \u001b[0;36mdecode_sequence_beam\u001b[0;34m(input_seq, k, encoder_model, decoder_model, max_target_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Encode the input as state vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mstates_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: 2 root error(s) found.\n  (0) Unimplemented:  Cast string to float is not supported\n\t [[node model_6/Cast (defined at <ipython-input-34-ce59ef28d500>:3) ]]\n  (1) Cancelled:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_96507]\n\nFunction call stack:\npredict_function -> predict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viovwIYlks_f"
      },
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "x = Embedding(data_dict['in_size'], 64,mask_zero=True)(encoder_inputs)\n",
        "x = LSTM(units=256,return_sequences=True)(x)\n",
        "x, *encoder_states = LSTM(units=256,\n",
        "                           return_state=True)(x)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "decoder_embedding =  Embedding(data_dict['out_size'], 64,mask_zero=True)\n",
        "x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "decoder_LSTM = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "x, *decoder_states = decoder_LSTM(x, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(units=data_dict['out_size'], activation='softmax')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBE6qh5T-xCi"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = decoder_embedding(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs_2, *decoder_states_2 = decoder_LSTM(dec_emb2\n",
        "                                                    ,initial_state=decoder_state_input\n",
        "                                                    )\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs_2 = decoder_dense(decoder_outputs_2)\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_state_input,\n",
        "    [decoder_outputs_2] + decoder_states_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpIBCBmmxqa5"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of \n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = 1\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    chars = [1]\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 2 :\n",
        "          stop_condition = True\n",
        "        chars.append(sampled_token_index)\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "    return tk.decode([chars],'output')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8wpmBFk-Bzd"
      },
      "source": [
        "test_gen = generate_batch(X_test, Y_test, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u014_k7MS48"
      },
      "source": [
        "x1 = tk.encode(['टोप्पर​'])\n",
        "x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtQr3YImNO7W"
      },
      "source": [
        "decode_sequence(x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9nFZEzPQ3uL"
      },
      "source": [
        "lex['test'].input.tolist()[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5L3BteyEHT"
      },
      "source": [
        "k += 1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "# print(input_seq)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input Source sentence:', tk.decode([X_test[k]])[0] )\n",
        "print('Actual Target Translation:', tk.decode([Y_test[k]],mode='output')[0])\n",
        "print('Predicted Target Translation:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItqoHkE2JTO1"
      },
      "source": [
        "# Romanized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8dFDOHGbEb"
      },
      "source": [
        "ta_rom = dict()\n",
        "ta_rom['rejoined'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['rejoined_aligned_cased'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv', sep='\\t', header=None, error_bad_lines=False) \n",
        "ta_rom['rejoined_aligned'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['split'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv', sep='\\t', header=None, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WGvG_RJqsr"
      },
      "source": [
        "list(ta_rom['rejoined'].iloc[0, 0])[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75rYpZkNCJV"
      },
      "source": [
        "ta_rom['rejoined_aligned_cased']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E9hQuQMULO"
      },
      "source": [
        "ta_rom['rejoined_aligned']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAnW0rAKDY5"
      },
      "source": [
        "ta_rom['split']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD7CedwSKaoS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}