{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6cfe8515eed41749f066a226edcc1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a92c10ae05a413bb4e234b0ab2fff14",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_15207177ae8b48a0854a6ff1a7b6d999",
              "IPY_MODEL_8222a86c790746b68e8c3bfe7f2074b8"
            ]
          }
        },
        "5a92c10ae05a413bb4e234b0ab2fff14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15207177ae8b48a0854a6ff1a7b6d999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_ca2f6bb784a54dcc84cf5776be916d94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92d3744e772848bb86d4b0318191e69a"
          }
        },
        "8222a86c790746b68e8c3bfe7f2074b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6a59a853ae3443a99d3dc217ba107e0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c79e0be058f14da8a83bf7b8102aa208"
          }
        },
        "ca2f6bb784a54dcc84cf5776be916d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92d3744e772848bb86d4b0318191e69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a59a853ae3443a99d3dc217ba107e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c79e0be058f14da8a83bf7b8102aa208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcVxQrUVxtS"
      },
      "source": [
        "# TO DO\n",
        "1. Figure out dataset\n",
        "2. Break down into individual characters\n",
        "3. Form corpus from dataset for input and output characters\n",
        "4. Assign a number to each character\n",
        "5. Model will have an embedding so it will handle it\n",
        "6. Output will be a bunch of integers so we will have to decode it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAUpzh1BLEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f24e58d-e191-405c-97a8-03aae4cde817"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClAC3xAEhKS"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tqdm.auto import tqdm\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "import datetime"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9sbR5xCVo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6edbdc1-d9bd-4e35-88b1-31c0dd7b9aeb"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir('/content/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(\"/content/dakshina_dataset_v1.0.tar\").extractall()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘dakshina_dataset_v1.0.tar’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0fMpPf_BUUK",
        "outputId": "d175ea2c-f73d-4681-951c-1efd7b5f122c"
      },
      "source": [
        "wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BstzblcHmd5"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDJrkl5EUHX"
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = [\"mr\",\"hi\"]):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['output', 'input', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val', 'test']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "      # shuffling the dataset i.e. rows of the dataset\n",
        "      lex[div] = lex[div].sample(frac=1)\n",
        "\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            \n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "\n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        # decoder input sequence\n",
        "                        # does not include the <EOW> token\n",
        "                        decoder_input_data[i, t] = word \n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the <SOW> token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, 23, num_decoder_tokens),dtype='float32')\n",
        "            \n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t==0 :\n",
        "                        decoder_input_data[i, t] = 1 # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo1m6s3vDaN"
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<SOW>'\n",
        "    self.stop_token = '<EOW>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "    self.decode_dict_output.update({2:''})\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "\n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[1:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mzVsRdBa4x"
      },
      "source": [
        "def return_data_dict(languages=['mr','hi'], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']\n",
        "  df_val = lex['val']\n",
        "  df_test = lex['test']\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  \n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "  \n",
        "  X_test = tk.encode(df_test.input.tolist(), mode='input')\n",
        "  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n",
        "\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  \n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "  \n",
        "  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n",
        "  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "\n",
        "\n",
        "  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'], data_dict['test']['max_source_length'])\n",
        "  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'], data_dict['test']['max_target_length'])\n",
        "\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict['train'], data_dict['out_size'], batch_size)\n",
        "  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict['train'], data_dict['out_size'], batch_size)\n",
        "  \n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict['val'], data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict['val'], data_dict['out_size'], batch_size)\n",
        "\n",
        "  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict['test'], data_dict['out_size'], batch_size)  \n",
        "  data_dict['test']['batch_greedy'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict['test'], data_dict['out_size'], batch_size)    \n",
        "  data_dict['test']['batch_1'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict['test'], data_dict['out_size'], 1)\n",
        "\n",
        "\n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJozmDWedfG"
      },
      "source": [
        "data_dict = return_data_dict()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_63arXtzSt"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQl8eh2y35p"
      },
      "source": [
        "class BeamSearchCallBack(keras.callbacks.Callback):\n",
        "  def __init__(self, details, test_data, tokenizer, out_size, num_val=200, beam=3) :\n",
        "    self.details = details\n",
        "    self.test_data = test_data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.out_size = out_size\n",
        "    self.num_val = num_val\n",
        "    self.beam = beam\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None) :\n",
        "    \n",
        "    # defining the encoder model\n",
        "    encoder_model = Model(self.details['encoder_inputs'], self.details['encoder_states'])\n",
        "    \n",
        "    # hidden representation size\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "\n",
        "    # initializing decoder state input\n",
        "    decoder_state_input = []\n",
        "\n",
        "    \n",
        "    for i in range(len(self.details['encoder_states'])) :\n",
        "        new_state = Input(shape=(rep_size,))\n",
        "        decoder_state_input.append(new_state)\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "    \n",
        "    for layer in self.details['decoder_layers'] :\n",
        "      x, *decoder_states = layer(x,initial_state=decoder_state_input)\n",
        "\n",
        "    x = self.details['decoder_dense'](x)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_input,\n",
        "        [x] + decoder_states )\n",
        "    inp = self.tokenizer.encode(self.test_data['df'].input.tolist())\n",
        "    out = self.tokenizer.encode(self.test_data['df'].output.tolist(),mode='output')\n",
        "    out_size = self.out_size\n",
        "    val_gen = data_loader._generate_batch(inp,out,self.test_data,self.out_size)\n",
        "    acc = 0\n",
        "    for i in tqdm(range(self.num_val)) :\n",
        "      (input_seq,ans) , _ = next(val_gen)\n",
        "      _,best = decode_sequence_beam(input_seq,self.beam,encoder_model,decoder_model,self.tokenizer,self.test_data['max_target_length'])\n",
        "      w1 = self.tokenizer.decode(best,mode='output')\n",
        "      w2 = self.tokenizer.decode(ans,mode='output')\n",
        "      comp = (w1==w2)\n",
        "      if comp :\n",
        "        acc += 1    \n",
        "\n",
        "    acc /= len(inp)\n",
        "    print(\"Val Accuracy : \"+str(acc))\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cdgAqTLyAr"
      },
      "source": [
        "def decode_sequence_beam(input_seq, k, encoder_model, decoder_model, tk, max_target_length=20, getall=False):\n",
        "    # encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq,batch_size=1,use_multiprocessing=True)\n",
        "    # generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = 1 \n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    # print([target_seq] + [states_value])\n",
        "    results, *states_values_temp = decoder_model.predict([target_seq] + [states_value])\n",
        "    output_tokens = results\n",
        "\n",
        "    states_values_k = [states_values_temp for i in range(k)]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = output_tokens[ind]\n",
        "    bestk_tot = [[1,bestk_ind[i]] for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            if run_condition[i] :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i],batch_size=1)\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = [states_values_temp for m in range(k)]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n",
        "                bestk_prob_temp2 = [b*bestk_prob_temp[j] for j in range(k)]\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        bestk_prob = bestk_prob_new[ind]\n",
        "        run_condition = []\n",
        "        for i in range(k) :\n",
        "            a = bestk_tot[i]\n",
        "            b = bestk_prob[i]\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = 0.0\n",
        "    for i in range(k) :\n",
        "      a = bestk_tot[i]\n",
        "      b = bestk_prob[i]\n",
        "      final_words += [a]\n",
        "      if b > best :\n",
        "        best_word = [a]\n",
        "\n",
        "    if getall :\n",
        "      return (tk.decode(final_words,'output'),best_word)\n",
        "    else :\n",
        "      return final_words,best_word"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJm8eZrt5mA"
      },
      "source": [
        "class rnn():\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \n",
        "    num_encode_layers = params['num_encode_layers']\n",
        "    num_decode_layers = params['num_decode_layers']\n",
        "    data_dict = params['data_dict']\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    cell_type = params['cell_type']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "        \n",
        "    ###################### ENCODER NETWORK ######################\n",
        "    \n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    encoder_layers = []\n",
        "    \n",
        "    for j in range(num_encode_layers-1) :   \n",
        "      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n",
        "      encoder_layers.append(curr_layer)\n",
        "      x = curr_layer(x)\n",
        "\n",
        "    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n",
        "    encoder_layers.append(curr_layer)\n",
        "    x, *encoder_states = curr_layer(x)\n",
        "\n",
        "    ###################### DECODER NETWORK ######################\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_layers = []    \n",
        "    \n",
        "    for j in range(num_decode_layers) :\n",
        "      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n",
        "      decoder_layers.append(curr_layer)\n",
        "      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(units=out_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "    # define the model that will turn `encoder_inputs` & `decoder_inputs` into `decoder_outputs`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "    self.model = model\n",
        "    self.encoder_inputs = encoder_inputs\n",
        "    self.encoder_layers = encoder_layers\n",
        "    self.decoder_inputs = decoder_inputs\n",
        "    self.decoder_embedding = decoder_embedding\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.decoder_dense = decoder_dense\n",
        "    self.encoder_states = encoder_states\n",
        "    self.params = params\n",
        "    self.details = {\n",
        "        'model' : self.model,\n",
        "        'encoder_inputs' : self.encoder_inputs,\n",
        "        'encoder_layers' :self.encoder_layers ,\n",
        "        'decoder_inputs' :self.decoder_inputs ,\n",
        "        'decoder_embedding' : self.decoder_embedding,\n",
        "        'decoder_layers' : self.decoder_layers,\n",
        "        'decoder_dense' : self.decoder_dense,\n",
        "        'encoder_states' : self.encoder_states ,\n",
        "        'params' :self.params,\n",
        "    }\n",
        "\n",
        "  def compile_and_fit(self, data_dict, params):\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    # printing the summary of the model\n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples\n",
        "    val_samples = len(data_dict['val']['df'])    \n",
        "    \n",
        "    # batch size\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs']\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                                           ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "                                           steps_per_epoch = train_samples//batch_size,\n",
        "                                           epochs=num_epochs,\n",
        "                                           # callbacks=[\n",
        "                                           # BeamSearchCallBack(self.details,data_dict['val'],data_dict['tokenizer'],data_dict['out_size'],num_val=num_val_samples,beam=beam),\n",
        "                                                        # wandb.keras.WandbCallback()\n",
        "                                                        # ],\n",
        "                                           validation_data = data_dict['val']['batch'],\n",
        "                                           ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "                                           validation_steps = val_samples//batch_size\n",
        "                                          )\n",
        "\n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0m1582SU6W8"
      },
      "source": [
        "class rnn_second() :\n",
        "  def __init__(self, details) :\n",
        "\n",
        "    # copying required details\n",
        "    self.details = details\n",
        "\n",
        "    # copying decoder state input\n",
        "    decoder_state_input = self.details['encoder_states']\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "\n",
        "    # copying hidden representation size\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "\n",
        "    # copying decoder inputs\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "\n",
        "    # the decoder model\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "  \n",
        "    all_outputs = []\n",
        "    for _ in range(self.details['params']['data_dict']['max_target_length']) :\n",
        "        for layer in self.details['decoder_layers'] :\n",
        "            x, *decoder_states = layer(x, initial_state=decoder_state_input)\n",
        "\n",
        "        x = self.details['decoder_dense'](x)\n",
        "\n",
        "        # appending the softmax output\n",
        "        all_outputs.append(x)\n",
        "\n",
        "        # taking the argmax to feed into the next time step\n",
        "        x = tf.math.argmax(x,2)  \n",
        "        x = self.details['decoder_embedding'](x)\n",
        "        \n",
        "        # decoder state input for the next time step\n",
        "        decoder_state_input = decoder_states\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    # where do we evaluate stop condition?\n",
        "\n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    model = Model([self.details['encoder_inputs'], decoder_inputs], decoder_outputs)\n",
        "    self.model = model\n",
        "\n",
        "  def compile_and_fit(self,data_dict,params) :\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "     # printing the summary of the model   \n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples \n",
        "    val_samples = len(data_dict['val']['df'])\n",
        "\n",
        "    # batch size   \n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs_2']\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch_greedy'],\n",
        "                                            steps_per_epoch = train_samples//batch_size,\n",
        "                                            epochs=num_epochs)\n",
        "                                            # callbacks=[\n",
        "                                                      # BeamSearchCallBack(self.details,data_dict['val'],data_dict['tokenizer'],data_dict['out_size'],num_val=num_val_samples,beam=beam),\n",
        "                                                      # wandb.keras.WandbCallback()\n",
        "                                                      # ],\n",
        "                                            # validation_data = data_dict['val']['batch_greedy'],\n",
        "                                            # validation_steps = val_samples//batch_size)\n",
        "   \n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n",
        "  def evaluate(self, data_dict) :\n",
        "\n",
        "    # test batch generator\n",
        "    test_gen = data_dict['test']['batch_greedy']\n",
        "    \n",
        "    # number of test samples\n",
        "    test_samples = len(data_dict['test']['df'])\n",
        "    \n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    batch_size=32\n",
        "\n",
        "    num_hits = 0\n",
        "    \n",
        "    for _ in range(test_samples//batch_size) :\n",
        "      (a,b),c = next(test_gen)\n",
        "      l1 = data_dict['tokenizer'].decode(np.argmax(c, axis=2), mode='output')\n",
        "      out = self.model.predict([a,b])\n",
        "      out = np.argmax(out,axis=2)\n",
        "      l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "      num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "    \n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    print(\"Test Acc \",acc/test_samples)\n",
        "    wandb.log({\"Final Test Accuracy\" : acc/test_samples})"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDLMoTWfBn86"
      },
      "source": [
        "class tools:\n",
        "  def init_params(config,data_dict):\n",
        "  \n",
        "    # returning parameters\n",
        "    params = {\n",
        "        'num_encode_layers' : config.num_encode_layers,\n",
        "        'num_decode_layers' : config.num_decode_layers,\n",
        "        'cell_type' : config.cell_type,\n",
        "        'rep_size' : config.rep_size,\n",
        "        'embed_size' : config.embed_size,\n",
        "        'dropout' : config.dropout,\n",
        "        'num_epochs' : config.num_epochs,\n",
        "        'num_epochs_2' : config.num_epochs_2,\n",
        "        'data_dict' : data_dict,\n",
        "        'batch_size' : config.batch_size\n",
        "    }\n",
        "    return params"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AWE1lLSDDXh"
      },
      "source": [
        "# sweep configuration\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'val_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM', 'GRU', 'SimpleRNN']  \n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'rep_size': {\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_epochs_2' : {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_encode_layers': {\n",
        "            'values': [1, 2, 4]\n",
        "        },\n",
        "        'num_decode_layers': {\n",
        "            'values': [1, 2, 4]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeqg-Otpnbo"
      },
      "source": [
        "# sweep configuration\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'val_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM']  \n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'rep_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_epochs_2' : {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_encode_layers': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_decode_layers': {\n",
        "            'values': [1]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqDamLUoDQWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0390cba7-f754-4e0c-fa0c-bd67e227fff2"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='dakshina_v2')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 6bi9leuo\n",
            "Sweep URL: https://wandb.ai/ramkamal/dakshina_v2/sweeps/6bi9leuo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez3f-pjxDTU8"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      # copying the config \n",
        "      config = wandb.config\n",
        " \n",
        "      # naming the run\n",
        "      # wandb.run.name = 'fil:'+str(config['num_filters_'])+'_type:'+config['type_of_filters'][0]+'_aug:'+str(config['augmentation'])[0]+'_dro:'+str(config['dropout'])[0]\n",
        "      wandb.run.name = 'typ:'+config['cell_type'][:4]+ '_' + 'emb:'+str(config['embed_size'])+ '_' + 'enc:' + str(config['num_encode_layers'])+ '_' + 'dec:'+str(config['num_decode_layers'])\n",
        "      \n",
        "      # returning the data dictionairy\n",
        "      ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "      data_dict = return_data_dict(batch_size=config.batch_size)\n",
        "\n",
        "      # copying the parameters\n",
        "      params = tools.init_params(config,data_dict)\n",
        "\n",
        "      # creating and training the first model\n",
        "      network = rnn(params)\n",
        "      run_details = network.compile_and_fit(data_dict, params)\n",
        "\n",
        "      # creating and training/ fine-tuning the second model\n",
        "      rnn_2 = rnn_second(network.details)\n",
        "      run_details_2 = rnn_2.compile_and_fit(data_dict,params)\n",
        "      \n",
        "      ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "      rnn_2.evaluate(data_dict)\n",
        "\n",
        "      if os.path.isdir('/content/wandb'): \n",
        "        shutil.rmtree('/content/wandb')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QBd1gJDV-R"
      },
      "source": [
        "# sweep_id = '7g0porer'"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCq49t4zDZod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d6cfe8515eed41749f066a226edcc1f6",
            "5a92c10ae05a413bb4e234b0ab2fff14",
            "15207177ae8b48a0854a6ff1a7b6d999",
            "8222a86c790746b68e8c3bfe7f2074b8",
            "ca2f6bb784a54dcc84cf5776be916d94",
            "92d3744e772848bb86d4b0318191e69a",
            "6a59a853ae3443a99d3dc217ba107e0b",
            "c79e0be058f14da8a83bf7b8102aa208"
          ]
        },
        "outputId": "4064cf42-1ef0-4162-f3a8-531e718d0a55"
      },
      "source": [
        "# performing the sweep\n",
        "wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3p2q4v17 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decode_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encode_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs_2: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trep_size: 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">rosy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2/sweeps/6bi9leuo\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2/sweeps/6bi9leuo</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2/runs/3p2q4v17\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2/runs/3p2q4v17</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210511_135419-3p2q4v17</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     700         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 32), (None,  5504        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 32), ( 5504        embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 70)     2310        lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 14,318\n",
            "Trainable params: 14,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2766/2766 [==============================] - 48s 15ms/step - loss: 1.0662 - acc: 0.2327 - val_loss: 0.9204 - val_acc: 0.3592\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     700         input_2[0][0]                    \n",
            "                                                                 tf.math.argmax[0][0]             \n",
            "                                                                 tf.math.argmax_1[0][0]           \n",
            "                                                                 tf.math.argmax_2[0][0]           \n",
            "                                                                 tf.math.argmax_3[0][0]           \n",
            "                                                                 tf.math.argmax_4[0][0]           \n",
            "                                                                 tf.math.argmax_5[0][0]           \n",
            "                                                                 tf.math.argmax_6[0][0]           \n",
            "                                                                 tf.math.argmax_7[0][0]           \n",
            "                                                                 tf.math.argmax_8[0][0]           \n",
            "                                                                 tf.math.argmax_9[0][0]           \n",
            "                                                                 tf.math.argmax_10[0][0]          \n",
            "                                                                 tf.math.argmax_11[0][0]          \n",
            "                                                                 tf.math.argmax_12[0][0]          \n",
            "                                                                 tf.math.argmax_13[0][0]          \n",
            "                                                                 tf.math.argmax_14[0][0]          \n",
            "                                                                 tf.math.argmax_15[0][0]          \n",
            "                                                                 tf.math.argmax_16[0][0]          \n",
            "                                                                 tf.math.argmax_17[0][0]          \n",
            "                                                                 tf.math.argmax_18[0][0]          \n",
            "                                                                 tf.math.argmax_19[0][0]          \n",
            "                                                                 tf.math.argmax_20[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 32), (None,  5504        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 32), ( 5504        embedding_1[1][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 lstm_1[1][1]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 lstm_1[2][1]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 lstm_1[3][1]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 embedding_1[5][0]                \n",
            "                                                                 lstm_1[4][1]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 embedding_1[6][0]                \n",
            "                                                                 lstm_1[5][1]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 embedding_1[7][0]                \n",
            "                                                                 lstm_1[6][1]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 embedding_1[8][0]                \n",
            "                                                                 lstm_1[7][1]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 embedding_1[9][0]                \n",
            "                                                                 lstm_1[8][1]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "                                                                 embedding_1[10][0]               \n",
            "                                                                 lstm_1[9][1]                     \n",
            "                                                                 lstm_1[9][2]                     \n",
            "                                                                 embedding_1[11][0]               \n",
            "                                                                 lstm_1[10][1]                    \n",
            "                                                                 lstm_1[10][2]                    \n",
            "                                                                 embedding_1[12][0]               \n",
            "                                                                 lstm_1[11][1]                    \n",
            "                                                                 lstm_1[11][2]                    \n",
            "                                                                 embedding_1[13][0]               \n",
            "                                                                 lstm_1[12][1]                    \n",
            "                                                                 lstm_1[12][2]                    \n",
            "                                                                 embedding_1[14][0]               \n",
            "                                                                 lstm_1[13][1]                    \n",
            "                                                                 lstm_1[13][2]                    \n",
            "                                                                 embedding_1[15][0]               \n",
            "                                                                 lstm_1[14][1]                    \n",
            "                                                                 lstm_1[14][2]                    \n",
            "                                                                 embedding_1[16][0]               \n",
            "                                                                 lstm_1[15][1]                    \n",
            "                                                                 lstm_1[15][2]                    \n",
            "                                                                 embedding_1[17][0]               \n",
            "                                                                 lstm_1[16][1]                    \n",
            "                                                                 lstm_1[16][2]                    \n",
            "                                                                 embedding_1[18][0]               \n",
            "                                                                 lstm_1[17][1]                    \n",
            "                                                                 lstm_1[17][2]                    \n",
            "                                                                 embedding_1[19][0]               \n",
            "                                                                 lstm_1[18][1]                    \n",
            "                                                                 lstm_1[18][2]                    \n",
            "                                                                 embedding_1[20][0]               \n",
            "                                                                 lstm_1[19][1]                    \n",
            "                                                                 lstm_1[19][2]                    \n",
            "                                                                 embedding_1[21][0]               \n",
            "                                                                 lstm_1[20][1]                    \n",
            "                                                                 lstm_1[20][2]                    \n",
            "                                                                 embedding_1[22][0]               \n",
            "                                                                 lstm_1[21][1]                    \n",
            "                                                                 lstm_1[21][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 70)     2310        lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "                                                                 lstm_1[10][0]                    \n",
            "                                                                 lstm_1[11][0]                    \n",
            "                                                                 lstm_1[12][0]                    \n",
            "                                                                 lstm_1[13][0]                    \n",
            "                                                                 lstm_1[14][0]                    \n",
            "                                                                 lstm_1[15][0]                    \n",
            "                                                                 lstm_1[16][0]                    \n",
            "                                                                 lstm_1[17][0]                    \n",
            "                                                                 lstm_1[18][0]                    \n",
            "                                                                 lstm_1[19][0]                    \n",
            "                                                                 lstm_1[20][0]                    \n",
            "                                                                 lstm_1[21][0]                    \n",
            "                                                                 lstm_1[22][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax (TFOpLambda)     (None, None)         0           dense[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_1 (TFOpLambda)   (None, None)         0           dense[2][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_2 (TFOpLambda)   (None, None)         0           dense[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_3 (TFOpLambda)   (None, None)         0           dense[4][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_4 (TFOpLambda)   (None, None)         0           dense[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_5 (TFOpLambda)   (None, None)         0           dense[6][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_6 (TFOpLambda)   (None, None)         0           dense[7][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_7 (TFOpLambda)   (None, None)         0           dense[8][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_8 (TFOpLambda)   (None, None)         0           dense[9][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_9 (TFOpLambda)   (None, None)         0           dense[10][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_10 (TFOpLambda)  (None, None)         0           dense[11][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_11 (TFOpLambda)  (None, None)         0           dense[12][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_12 (TFOpLambda)  (None, None)         0           dense[13][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_13 (TFOpLambda)  (None, None)         0           dense[14][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_14 (TFOpLambda)  (None, None)         0           dense[15][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_15 (TFOpLambda)  (None, None)         0           dense[16][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_16 (TFOpLambda)  (None, None)         0           dense[17][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_17 (TFOpLambda)  (None, None)         0           dense[18][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_18 (TFOpLambda)  (None, None)         0           dense[19][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_19 (TFOpLambda)  (None, None)         0           dense[20][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_20 (TFOpLambda)  (None, None)         0           dense[21][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 70)     0           dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "                                                                 dense[6][0]                      \n",
            "                                                                 dense[7][0]                      \n",
            "                                                                 dense[8][0]                      \n",
            "                                                                 dense[9][0]                      \n",
            "                                                                 dense[10][0]                     \n",
            "                                                                 dense[11][0]                     \n",
            "                                                                 dense[12][0]                     \n",
            "                                                                 dense[13][0]                     \n",
            "                                                                 dense[14][0]                     \n",
            "                                                                 dense[15][0]                     \n",
            "                                                                 dense[16][0]                     \n",
            "                                                                 dense[17][0]                     \n",
            "                                                                 dense[18][0]                     \n",
            "                                                                 dense[19][0]                     \n",
            "                                                                 dense[20][0]                     \n",
            "                                                                 dense[21][0]                     \n",
            "                                                                 dense[22][0]                     \n",
            "==================================================================================================\n",
            "Total params: 14,318\n",
            "Trainable params: 14,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 658<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6cfe8515eed41749f066a226edcc1f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210511_135419-3p2q4v17/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210511_135419-3p2q4v17/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">rosy-sweep-1</strong>: <a href=\"https://wandb.ai/ramkamal/dakshina_v2/runs/3p2q4v17\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2/runs/3p2q4v17</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Run 3p2q4v17 errored: InvalidArgumentError()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3p2q4v17 errored: InvalidArgumentError()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r0823n3r with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decode_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encode_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs_2: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trep_size: 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">silver-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2/sweeps/6bi9leuo\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2/sweeps/6bi9leuo</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/ramkamal/dakshina_v2/runs/r0823n3r\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v2/runs/r0823n3r</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210511_135645-r0823n3r</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     700         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 32), (None,  5504        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 32), ( 5504        embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 70)     2310        lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 14,318\n",
            "Trainable params: 14,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2766/2766 [==============================] - 43s 13ms/step - loss: 1.0618 - acc: 0.2383 - val_loss: 0.9256 - val_acc: 0.3570\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     700         input_2[0][0]                    \n",
            "                                                                 tf.math.argmax_22[0][0]          \n",
            "                                                                 tf.math.argmax_23[0][0]          \n",
            "                                                                 tf.math.argmax_24[0][0]          \n",
            "                                                                 tf.math.argmax_25[0][0]          \n",
            "                                                                 tf.math.argmax_26[0][0]          \n",
            "                                                                 tf.math.argmax_27[0][0]          \n",
            "                                                                 tf.math.argmax_28[0][0]          \n",
            "                                                                 tf.math.argmax_29[0][0]          \n",
            "                                                                 tf.math.argmax_30[0][0]          \n",
            "                                                                 tf.math.argmax_31[0][0]          \n",
            "                                                                 tf.math.argmax_32[0][0]          \n",
            "                                                                 tf.math.argmax_33[0][0]          \n",
            "                                                                 tf.math.argmax_34[0][0]          \n",
            "                                                                 tf.math.argmax_35[0][0]          \n",
            "                                                                 tf.math.argmax_36[0][0]          \n",
            "                                                                 tf.math.argmax_37[0][0]          \n",
            "                                                                 tf.math.argmax_38[0][0]          \n",
            "                                                                 tf.math.argmax_39[0][0]          \n",
            "                                                                 tf.math.argmax_40[0][0]          \n",
            "                                                                 tf.math.argmax_41[0][0]          \n",
            "                                                                 tf.math.argmax_42[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 32), (None,  5504        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 32), ( 5504        embedding_1[1][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 lstm_1[1][1]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 lstm_1[2][1]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 lstm_1[3][1]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 embedding_1[5][0]                \n",
            "                                                                 lstm_1[4][1]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 embedding_1[6][0]                \n",
            "                                                                 lstm_1[5][1]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 embedding_1[7][0]                \n",
            "                                                                 lstm_1[6][1]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 embedding_1[8][0]                \n",
            "                                                                 lstm_1[7][1]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 embedding_1[9][0]                \n",
            "                                                                 lstm_1[8][1]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "                                                                 embedding_1[10][0]               \n",
            "                                                                 lstm_1[9][1]                     \n",
            "                                                                 lstm_1[9][2]                     \n",
            "                                                                 embedding_1[11][0]               \n",
            "                                                                 lstm_1[10][1]                    \n",
            "                                                                 lstm_1[10][2]                    \n",
            "                                                                 embedding_1[12][0]               \n",
            "                                                                 lstm_1[11][1]                    \n",
            "                                                                 lstm_1[11][2]                    \n",
            "                                                                 embedding_1[13][0]               \n",
            "                                                                 lstm_1[12][1]                    \n",
            "                                                                 lstm_1[12][2]                    \n",
            "                                                                 embedding_1[14][0]               \n",
            "                                                                 lstm_1[13][1]                    \n",
            "                                                                 lstm_1[13][2]                    \n",
            "                                                                 embedding_1[15][0]               \n",
            "                                                                 lstm_1[14][1]                    \n",
            "                                                                 lstm_1[14][2]                    \n",
            "                                                                 embedding_1[16][0]               \n",
            "                                                                 lstm_1[15][1]                    \n",
            "                                                                 lstm_1[15][2]                    \n",
            "                                                                 embedding_1[17][0]               \n",
            "                                                                 lstm_1[16][1]                    \n",
            "                                                                 lstm_1[16][2]                    \n",
            "                                                                 embedding_1[18][0]               \n",
            "                                                                 lstm_1[17][1]                    \n",
            "                                                                 lstm_1[17][2]                    \n",
            "                                                                 embedding_1[19][0]               \n",
            "                                                                 lstm_1[18][1]                    \n",
            "                                                                 lstm_1[18][2]                    \n",
            "                                                                 embedding_1[20][0]               \n",
            "                                                                 lstm_1[19][1]                    \n",
            "                                                                 lstm_1[19][2]                    \n",
            "                                                                 embedding_1[21][0]               \n",
            "                                                                 lstm_1[20][1]                    \n",
            "                                                                 lstm_1[20][2]                    \n",
            "                                                                 embedding_1[22][0]               \n",
            "                                                                 lstm_1[21][1]                    \n",
            "                                                                 lstm_1[21][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 70)     2310        lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "                                                                 lstm_1[10][0]                    \n",
            "                                                                 lstm_1[11][0]                    \n",
            "                                                                 lstm_1[12][0]                    \n",
            "                                                                 lstm_1[13][0]                    \n",
            "                                                                 lstm_1[14][0]                    \n",
            "                                                                 lstm_1[15][0]                    \n",
            "                                                                 lstm_1[16][0]                    \n",
            "                                                                 lstm_1[17][0]                    \n",
            "                                                                 lstm_1[18][0]                    \n",
            "                                                                 lstm_1[19][0]                    \n",
            "                                                                 lstm_1[20][0]                    \n",
            "                                                                 lstm_1[21][0]                    \n",
            "                                                                 lstm_1[22][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_22 (TFOpLambda)  (None, None)         0           dense[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_23 (TFOpLambda)  (None, None)         0           dense[2][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_24 (TFOpLambda)  (None, None)         0           dense[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_25 (TFOpLambda)  (None, None)         0           dense[4][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_26 (TFOpLambda)  (None, None)         0           dense[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_27 (TFOpLambda)  (None, None)         0           dense[6][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_28 (TFOpLambda)  (None, None)         0           dense[7][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_29 (TFOpLambda)  (None, None)         0           dense[8][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_30 (TFOpLambda)  (None, None)         0           dense[9][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_31 (TFOpLambda)  (None, None)         0           dense[10][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_32 (TFOpLambda)  (None, None)         0           dense[11][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_33 (TFOpLambda)  (None, None)         0           dense[12][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_34 (TFOpLambda)  (None, None)         0           dense[13][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_35 (TFOpLambda)  (None, None)         0           dense[14][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_36 (TFOpLambda)  (None, None)         0           dense[15][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_37 (TFOpLambda)  (None, None)         0           dense[16][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_38 (TFOpLambda)  (None, None)         0           dense[17][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_39 (TFOpLambda)  (None, None)         0           dense[18][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_40 (TFOpLambda)  (None, None)         0           dense[19][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_41 (TFOpLambda)  (None, None)         0           dense[20][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_42 (TFOpLambda)  (None, None)         0           dense[21][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 70)     0           dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "                                                                 dense[6][0]                      \n",
            "                                                                 dense[7][0]                      \n",
            "                                                                 dense[8][0]                      \n",
            "                                                                 dense[9][0]                      \n",
            "                                                                 dense[10][0]                     \n",
            "                                                                 dense[11][0]                     \n",
            "                                                                 dense[12][0]                     \n",
            "                                                                 dense[13][0]                     \n",
            "                                                                 dense[14][0]                     \n",
            "                                                                 dense[15][0]                     \n",
            "                                                                 dense[16][0]                     \n",
            "                                                                 dense[17][0]                     \n",
            "                                                                 dense[18][0]                     \n",
            "                                                                 dense[19][0]                     \n",
            "                                                                 dense[20][0]                     \n",
            "                                                                 dense[21][0]                     \n",
            "                                                                 dense[22][0]                     \n",
            "==================================================================================================\n",
            "Total params: 14,318\n",
            "Trainable params: 14,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBotvdIuDczo"
      },
      "source": [
        "# Run One Model separate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfaOoDTt-Zd"
      },
      "source": [
        "params = {\n",
        "    'num_encode_layers' : 2,\n",
        "    'num_decode_layers' : 1,\n",
        "    'cell_type' : 'LSTM', \n",
        "    'rep_size' : 256,\n",
        "    'embed_size' : 20,\n",
        "    'dropout' : 0,\n",
        "    'num_epochs' : 5,\n",
        "    'num_epochs_2' : 5,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32\n",
        "}\n",
        "network = rnn(params)\n",
        "plot_model(network.model, show_shapes=True)\n",
        "network.compile_and_fit(data_dict, params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMHkTtHSTBmD"
      },
      "source": [
        "plot_model(rnn_2.model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdPRy5NOA03"
      },
      "source": [
        "rnn_2 = rnn_second(network.details)\n",
        "run_details2 = rnn_2.compile_and_fit(data_dict,params)\n",
        "rnn_2.evaluate(data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YHfKL5_YTKk"
      },
      "source": [
        "# !pip install editdistance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtqXB5sXLLYV"
      },
      "source": [
        "test_gen = data_dict['test']['batch_greedy']\n",
        "test_samples = len(data_dict['test']['df'])\n",
        "batch_size=32\n",
        "acc = 0\n",
        "for _ in range(test_samples//batch_size) :\n",
        "  (a,b),c = next(test_gen)\n",
        "  l1 = data_dict['tokenizer'].decode(np.argmax(c,axis=2),mode='output')\n",
        "  out = rnn_2.model.predict([a,b])\n",
        "  out = np.argmax(out,axis=2)\n",
        "  l2 = data_dict['tokenizer'].decode(out,mode='output')\n",
        "  acc += np.sum(np.array(l1)==np.array(l2))\n",
        "\n",
        "print(\"Val Accuracy : \",acc/test_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yd31nMSSdxh"
      },
      "source": [
        "l1 = ['andiranchi', 'ake', 'irangula', 'ndha', 'ulakhatee', 'alal', 'urvajanche', 'urunath', 'amjhunga', 'maging', 'tutya', 'stitvachi', 'nudit', 'addhtipeksha', 'aam', 'uloos', 'eneth', 'eevandan', 'ndreas', 'ugnata', 'alvon', 'stitvatil', 'anchyabaddal', 'otipeksha', 'umak', 'haktte', 'ubhati', 'hhavi', 'ang', 'azipur', 'illi', 'hodi']\n",
        "l2 = ['andyyaachi', 'ake', 'irnngda', 'ndha', 'ulakatii', 'alalt', 'urvajaache', 'urunach', 'amaaaaagt', 'minangt', 'tutya', 'sttyaache', 'nduit', 'adhititaassh', 'amttt', 'ulustt', 'anich', 'ivanaaa', 'ndyiiist', 'uganta', 'alvoo', 'stttaaaii', 'anthaaaaaadd', 'otipissha', 'umak', 'hakttt', 'ubhate', 'hhali', 'ang', 'ajiprrt', 'hlll', 'hodit']\n",
        "np.sum(np.array(l1)==np.array(l2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzbbHG-33UB"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "num_encoder_tokens = data_dict['in_size']\n",
        "latent_dim = 256\n",
        "num_decoder_tokens = data_dict['out_size']\n",
        "batch_size=32\n",
        "epochs=20\n",
        "val_samples = 4981\n",
        "train_samples = 45444\n",
        "\n",
        "# The first part is unchanged\n",
        "encoder_inputs1 = Input(shape=(None,))\n",
        "encoder_inputs=Embedding(data_dict['in_size'], 64,mask_zero=True)(encoder_inputs1)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, which will only process one timestep at a time.\n",
        "decoder_inputs1 = Input(shape=(1,))\n",
        "decoder_embedding =  Embedding(data_dict['out_size'], 64,mask_zero=True)\n",
        "decoder_inputs = decoder_embedding(decoder_inputs1)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "all_outputs = []\n",
        "inputs = decoder_inputs\n",
        "for _ in range(23):\n",
        "    # Run the decoder on one timestep\n",
        "    outputs, state_h, state_c = decoder_lstm(inputs,\n",
        "                                             initial_state=states)\n",
        "    outputs = decoder_dense(outputs)\n",
        "    # Store the current prediction (we will concatenate all predictions later)\n",
        "    all_outputs.append(outputs)\n",
        "    # Reinject the outputs as inputs for the next loop iteration\n",
        "    # as well as update the states\n",
        "    outputs1 = tf.math.argmax(outputs,2)\n",
        "    inputs = decoder_embedding(outputs1)\n",
        "    states = [state_h, state_c]\n",
        "\n",
        "# Concatenate all predictions\n",
        "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "\n",
        "# Define and compile model as previously\n",
        "model = Model([encoder_inputs1, decoder_inputs1], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Prepare decoder input data that just contains the start character\n",
        "# Note that we could have made it a constant hard-coded in the model\n",
        "\n",
        "# # Train model as previously\n",
        "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_split=0.2)\n",
        "\n",
        "model.fit_generator(generator = data_dict['train']['batch_greedy'],\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = data_dict['val']['batch_greedy'],\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjtrtMiHDbiy"
      },
      "source": [
        "generator=data_dict['test']['batch_greedy']\n",
        "(a,b),c = next(generator)\n",
        "print(a,b,np.argmax(c,axis=2))\n",
        "out = model.predict([a,b])\n",
        "out = np.argmax(out,axis=2)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7o9Y_NBzelI"
      },
      "source": [
        "plot_model(model,to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqiICTfUid26"
      },
      "source": [
        "# Attention?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glswCuYJlume"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NWc0APTli6i"
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        print(input_shape[0][2],input_shape[1][2])\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS-7MTO4lyP7"
      },
      "source": [
        "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.python.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7drOpPqmltW3"
      },
      "source": [
        "def define_nmt(hidden_size, batch_size, max_source_length, source_vsize, max_target_length, target_vsize):\n",
        "\n",
        "    # Define an input sequence and process it\n",
        "    if batch_size:\n",
        "        encoder_inputs = Input(batch_shape=(batch_size, max_source_length, source_vsize))\n",
        "        decoder_inputs = Input(batch_shape=(batch_size, max_target_length - 1, target_vsize))\n",
        "    else:\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    # Encoder GRU\n",
        "    encoder_emb = Embedding(data_dict['in_size'],source_vsize,input_length=max_source_length)\n",
        "    temp1 = encoder_emb(encoder_inputs)\n",
        "    encoder_gru = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "    encoder_out, encoder_stateh,encoder_statec = encoder_gru(temp1)\n",
        "    encoder_state = [encoder_stateh,encoder_statec]\n",
        "    \n",
        "    # Set up the decoder GRU, using `encoder_states` as initial state\n",
        "    decoder_emb = Embedding(data_dict['out_size'],target_vsize,input_length=max_target_length)\n",
        "    temp2 = decoder_emb(decoder_inputs)\n",
        "    decoder_gru = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "    decoder_out, *decoder_state = decoder_gru(temp2, initial_state=encoder_state)\n",
        "\n",
        "    # Attention layer\n",
        "    attn_layer = AttentionLayer()\n",
        "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "    # Concat attention input and decoder GRU output\n",
        "    decoder_concat_input = Concatenate(axis=-1)([decoder_out, attn_out])\n",
        "\n",
        "    # Dense layer\n",
        "    dense = Dense(target_vsize, activation='softmax')\n",
        "    dense_time = TimeDistributed(dense)\n",
        "    decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "    # Full model\n",
        "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "    full_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "    full_model.summary()\n",
        "\n",
        "    \"\"\" Inference model \"\"\"\n",
        "    batch_size = 1\n",
        "\n",
        "    \"\"\" Encoder (Inference) model \"\"\"\n",
        "    encoder_inf_inputs = Input(shape=(max_source_length,))\n",
        "    temp3 = encoder_emb(encoder_inf_inputs)\n",
        "    encoder_inf_out, *encoder_inf_state = encoder_gru(temp3)\n",
        "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "    \"\"\" Decoder (Inference) model \"\"\"\n",
        "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1))\n",
        "    encoder_inf_states = Input(batch_shape=(batch_size, None, hidden_size))\n",
        "    decoder_init_stateh = Input(batch_shape=(batch_size, hidden_size))\n",
        "    decoder_init_statec = Input(batch_shape=(batch_size, hidden_size))\n",
        "    decoder_init_state = [decoder_init_stateh,decoder_init_statec]\n",
        "    temp = decoder_emb(decoder_inf_inputs)\n",
        "    decoder_inf_out, *decoder_inf_state = decoder_gru(temp, initial_state=decoder_init_state)\n",
        "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "    decoder_inf_concat = Concatenate(axis=-1)([decoder_inf_out, attn_inf_out])\n",
        "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
        "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "\n",
        "    return full_model, encoder_model, decoder_model\n",
        "    # return full_model\n",
        "\n",
        "\n",
        "m1, m2, m3 = define_nmt(256, None, None, 30, None, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJlOmqBwpWAS"
      },
      "source": [
        "plot_model(m3, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-FArLdsIXsv"
      },
      "source": [
        "data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0QSXRudzevC"
      },
      "source": [
        "batch_size=32\n",
        "epochs=10\n",
        "val_samples = 4981\n",
        "train_samples = 45444\n",
        "\n",
        "m1.fit_generator(generator = data_dict['train']['batch'],\n",
        "                 steps_per_epoch=train_samples//batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=data_dict['val']['batch'],\n",
        "                 validation_steps=val_samples//batch_size\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "namCfz0sFqRx"
      },
      "source": [
        "acc = 0\n",
        "generator = data_dict['test']['batch_1']\n",
        "tk = data_dict['tokenizer']\n",
        "for j in tqdm(range(4967)) :\n",
        "  (a,b),c = next(generator)\n",
        "  g = [[1] + list(np.argmax(c,axis=-1)[0])]\n",
        "  enc_outs, enc_last_state = m2.predict(a)\n",
        "  dec_state = enc_last_state\n",
        "  attention_weights = []\n",
        "  word = [1]\n",
        "  for i in range(23):\n",
        "\n",
        "      dec_out, attention, dec_state = m3.predict([enc_outs, dec_state, b])\n",
        "      dec_ind = np.argmax(dec_out, axis=-1)\n",
        "\n",
        "      word.append(dec_ind[0][0])\n",
        "      b = dec_ind\n",
        "      attention_weights.append((dec_ind, attention))\n",
        "      if dec_ind[0][0] == 2 :\n",
        "        break\n",
        "  str1 = tk.decode(g,mode='output')\n",
        "  str2 = tk.decode([word],mode='output')\n",
        "  if str1 == str2 :\n",
        "    acc += 1\n",
        "\n",
        "print(acc/4967)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItqoHkE2JTO1"
      },
      "source": [
        "# Romanized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8dFDOHGbEb"
      },
      "source": [
        "ta_rom = dict()\n",
        "ta_rom['rejoined'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['rejoined_aligned_cased'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv', sep='\\t', header=None, error_bad_lines=False) \n",
        "ta_rom['rejoined_aligned'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['split'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv', sep='\\t', header=None, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WGvG_RJqsr"
      },
      "source": [
        "list(ta_rom['rejoined'].iloc[0, 0])[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75rYpZkNCJV"
      },
      "source": [
        "ta_rom['rejoined_aligned_cased']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E9hQuQMULO"
      },
      "source": [
        "ta_rom['rejoined_aligned']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAnW0rAKDY5"
      },
      "source": [
        "ta_rom['split']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD7CedwSKaoS"
      },
      "source": [
        "l1 = [1,4,2,3]\n",
        "l2 = [1,4,2,5]\n",
        "print(np.array(l1[1:-1])==np.array(l2[1:-1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}