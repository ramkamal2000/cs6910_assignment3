{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "rnn_second_v9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcVxQrUVxtS"
      },
      "source": [
        "# TO DO\n",
        "1. Bearm search with log and normalization\n",
        "2. Change structure of test dataset\n",
        "3. Bleu score calculation\n",
        "4. Training, saving and loading best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJUDIplVr2Wi"
      },
      "source": [
        "# TO DO ATTENTION\n",
        "1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAUpzh1BLEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ddfa4e-f64d-44f4-b7ae-b0235685402a"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\r\u001b[K     |▏                               | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 22.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 20.7MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 12.6MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 12.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 12.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 13.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92kB 13.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 358kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 368kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 378kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 389kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 399kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 409kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 419kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 430kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 440kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 450kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 460kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 471kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 481kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 491kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 501kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 522kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 532kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 542kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 552kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 573kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 583kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 593kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 604kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 614kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 624kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 634kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 645kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 655kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 665kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 686kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 696kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 706kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 716kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 727kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 737kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 747kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 757kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 768kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 778kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 788kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 798kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 808kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 819kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 829kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 839kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 849kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 860kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 870kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 880kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 890kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 901kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 911kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 921kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 931kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 942kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 952kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 962kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 972kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 983kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 993kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 31.0MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.5MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=a70acbf8b1b1151cfeecfe2327fe0eed31710a389faa182f325847cfee6928c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=d538137f97e3fe15f374de2aacc961625d25eedb9d3c3ae5fe67127bc3666929\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: pathtools, smmap, gitdb, GitPython, configparser, subprocess32, sentry-sdk, docker-pycreds, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClAC3xAEhKS"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tqdm.auto import tqdm\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "import datetime\n",
        "from math import ceil\n",
        "from pprint import pprint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9sbR5xCVo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5365ad54-8687-43f5-a36f-b0f36d93abe2"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir('/content/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(\"/content/dakshina_dataset_v1.0.tar\").extractall()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-14 06:39:30--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 66.102.1.128, 172.253.120.128, 142.250.13.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|66.102.1.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   121MB/s    in 25s     \n",
            "\n",
            "2021-05-14 06:39:57 (75.3 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0fMpPf_BUUK",
        "outputId": "bcb002af-0e27-4ee7-939c-09ec1a1e04cd"
      },
      "source": [
        "wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BstzblcHmd5"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDJrkl5EUHX"
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = ['ta']):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['output', 'input', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "      # shuffling the dataset i.e. rows of the dataset\n",
        "      lex[div] = lex[div].sample(frac=1)\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "    ##### something for the test dataset\n",
        "\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            \n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "\n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        # decoder input sequence\n",
        "                        # does not include the <EOW> token\n",
        "                        decoder_input_data[i, t] = word \n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the <SOW> token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "            \n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t==0 :\n",
        "                        decoder_input_data[i, t] = 1 # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo1m6s3vDaN"
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<SOW>'\n",
        "    self.stop_token = '<EOW>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "    self.decode_dict_output.update({2:''})\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mzVsRdBa4x"
      },
      "source": [
        "def return_data_dict(languages=['ta'], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']\n",
        "  df_val = lex['val']\n",
        "  df_test = lex['test']\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  \n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  \n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_val]))\n",
        "\n",
        "  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'])\n",
        "  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'])\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  \n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "\n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJozmDWedfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e740f01-2099-434d-861f-2ab40a14a44a"
      },
      "source": [
        "dict_data_dict = dict()\n",
        "\n",
        "for batch_size in [32]:\n",
        "  dict_data_dict.update({batch_size: return_data_dict(batch_size=batch_size)})\n",
        "\n",
        "data_dict = list(dict_data_dict.values())[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_63arXtzSt"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cdgAqTLyAr"
      },
      "source": [
        "def decode_sequence_beam(input_seq, k, encoder_model, decoder_model, tk, max_target_length=20, getall=False):\n",
        "    # encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq,batch_size=1,use_multiprocessing=True)\n",
        "    # generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = 1 \n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    # print([target_seq] + [states_value])\n",
        "    results, *states_values_temp = decoder_model.predict([target_seq] + [states_value])\n",
        "    output_tokens = results\n",
        "\n",
        "    states_values_k = [states_values_temp for i in range(k)]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = output_tokens[ind]\n",
        "    bestk_tot = [[1,bestk_ind[i]] for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            if run_condition[i] :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i],batch_size=1)\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = [states_values_temp for m in range(k)]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n",
        "                bestk_prob_temp2 = [b*bestk_prob_temp[j] for j in range(k)]\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        bestk_prob = bestk_prob_new[ind]\n",
        "        run_condition = []\n",
        "        for i in range(k) :\n",
        "            a = bestk_tot[i]\n",
        "            b = bestk_prob[i]\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = 0.0\n",
        "    for i in range(k) :\n",
        "      a = bestk_tot[i]\n",
        "      b = bestk_prob[i]\n",
        "      final_words += [a]\n",
        "      if b > best :\n",
        "        best_word = [a]\n",
        "\n",
        "    if getall :\n",
        "      return (tk.decode(final_words,'output'),best_word)\n",
        "    else :\n",
        "      return final_words,best_word"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJm8eZrt5mA"
      },
      "source": [
        "class rnn():\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \n",
        "    num_encode_layers = params['num_encode_layers']\n",
        "    num_decode_layers = params['num_decode_layers']\n",
        "    data_dict = params['data_dict']\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    cell_type = params['cell_type']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "        \n",
        "    ###################### ENCODER NETWORK ######################\n",
        "    \n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    encoder_layers = []\n",
        "    \n",
        "    for j in range(num_encode_layers-1) :   \n",
        "      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n",
        "      encoder_layers.append(curr_layer)\n",
        "      x = curr_layer(x)\n",
        "\n",
        "    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n",
        "    encoder_layers.append(curr_layer)\n",
        "    x, *encoder_states = curr_layer(x)\n",
        "\n",
        "    ###################### DECODER NETWORK ######################\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_layers = []    \n",
        "    \n",
        "    for j in range(num_decode_layers) :\n",
        "      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n",
        "      decoder_layers.append(curr_layer)\n",
        "      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(units=out_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "    # define the model that will turn `encoder_inputs` & `decoder_inputs` into `decoder_outputs`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "    self.model = model\n",
        "    self.encoder_inputs = encoder_inputs\n",
        "    self.encoder_layers = encoder_layers\n",
        "    self.decoder_inputs = decoder_inputs\n",
        "    self.decoder_embedding = decoder_embedding\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.decoder_dense = decoder_dense\n",
        "    self.encoder_states = encoder_states\n",
        "    self.params = params\n",
        "    self.details = {\n",
        "        'model' : self.model,\n",
        "        'encoder_inputs' : self.encoder_inputs,\n",
        "        'encoder_layers' :self.encoder_layers ,\n",
        "        'decoder_inputs' :self.decoder_inputs ,\n",
        "        'decoder_embedding' : self.decoder_embedding,\n",
        "        'decoder_layers' : self.decoder_layers,\n",
        "        'decoder_dense' : self.decoder_dense,\n",
        "        'encoder_states' : self.encoder_states ,\n",
        "        'params' :self.params,\n",
        "    }\n",
        "\n",
        "  def compile_and_fit(self, data_dict, params):\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    # printing the summary of the model\n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples\n",
        "    val_samples = len(data_dict['val']['df'])    \n",
        "    \n",
        "    # batch size\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs']\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                                           steps_per_epoch = train_samples//batch_size,\n",
        "                                           epochs=num_epochs,\n",
        "                                           callbacks=[\n",
        "                                                      wandb.keras.WandbCallback()\n",
        "                                                      ],\n",
        "                                           validation_data = data_dict['val']['batch'], \n",
        "                                           validation_steps = val_samples//batch_size\n",
        "                                          )\n",
        "\n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0m1582SU6W8"
      },
      "source": [
        "class rnn_second() :\n",
        "  def __init__(self, details) :\n",
        "\n",
        "    # copying required details\n",
        "    self.details = details\n",
        "\n",
        "    # copying decoder state input\n",
        "    decoder_state_input = self.details['encoder_states']\n",
        "\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "\n",
        "    # copying hidden representation size\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "\n",
        "    # copying decoder inputs\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "\n",
        "    # the decoder model\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "  \n",
        "    all_outputs = []\n",
        "    for _ in range(self.details['params']['data_dict']['max_target_length']) :\n",
        "        for layer in self.details['decoder_layers'] :\n",
        "            x, *decoder_states = layer(x, initial_state=decoder_state_input)\n",
        "\n",
        "        x = self.details['decoder_dense'](x)\n",
        "\n",
        "        # appending the softmax output\n",
        "        all_outputs.append(x)\n",
        "\n",
        "        # taking the argmax to feed into the next time step\n",
        "        # print(\"Hello \",tf.math.argmax(x, 2))\n",
        "        # if int(tf.math.argmax(x, 2))==2:\n",
        "        #     x = 0\n",
        "        # else:\n",
        "        #     x = tf.math.argmax(x, 2)\n",
        "        x = tf.math.argmax(x, 2) \n",
        "        x = self.details['decoder_embedding'](x)\n",
        "        \n",
        "        # decoder state input for the next time step\n",
        "        decoder_state_input = decoder_states\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    # where do we evaluate stop condition?\n",
        "\n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    model = Model([self.details['encoder_inputs'], decoder_inputs], decoder_outputs)\n",
        "    self.model = model\n",
        "\n",
        "  def compile_and_fit(self,data_dict,params) :\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "     # printing the summary of the model   \n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples \n",
        "    val_samples = len(data_dict['val']['df'])\n",
        "\n",
        "    # batch size   \n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs_2']\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch_greedy'],\n",
        "                                            steps_per_epoch = train_samples//batch_size,\n",
        "                                            epochs=num_epochs,\n",
        "                                            callbacks=[\n",
        "                                                      wandb.keras.WandbCallback()\n",
        "                                                      ],\n",
        "                                            validation_data = data_dict['val']['batch_greedy'],\n",
        "                                            validation_steps = val_samples//batch_size)\n",
        "   \n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n",
        "  def evaluate(self, data_dict, train=False) :\n",
        "\n",
        "    if train :\n",
        "      test_gen = data_dict['train']['batch_greedy']\n",
        "    \n",
        "      # number of test samples\n",
        "      test_samples = len(data_dict['train']['df'])\n",
        "    \n",
        "      batch_size=self.details['params']['batch_size']\n",
        "\n",
        "      num_hits = 0\n",
        "      \n",
        "      for _ in range(test_samples//batch_size) :\n",
        "        (a,b),c = next(test_gen)\n",
        "        c = np.argmax(c, axis=2)\n",
        "        # print(c)\n",
        "        l1 = data_dict['tokenizer'].decode(c, mode='output')\n",
        "        out = self.model.predict([a,b])\n",
        "        out = np.argmax(out,axis=2) \n",
        "        l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "        num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "        # print(l1, l2)\n",
        "\n",
        "      print(\"Final Train Acc \", num_hits/test_samples)\n",
        "\n",
        "    # test batch generator\n",
        "    test_gen = data_dict['val']['batch_greedy']\n",
        "     \n",
        "    # number of test samples\n",
        "    test_samples = len(data_dict['val']['df'])\n",
        "    \n",
        "    batch_size=self.details['params']['batch_size']\n",
        "    \n",
        "    num_hits = 0\n",
        "    \n",
        "    for _ in range(test_samples//batch_size) :\n",
        "      (a,b),c = next(test_gen)\n",
        "      c = np.argmax(c, axis=2)\n",
        "      # print(c)\n",
        "      l1 = data_dict['tokenizer'].decode(c, mode='output')\n",
        "      out = self.model.predict([a,b])\n",
        "      out = np.argmax(out,axis=2) \n",
        "      l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "      num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "      # print(l1, l2)\n",
        "\n",
        "    print(\"Final Val Acc \", num_hits/test_samples)\n",
        "\n",
        "    wandb.log({\"final_val_acc\":  num_hits/test_samples, \n",
        "               \"final_train_acc\":  num_hits/test_samples})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5_HtEoSFcu"
      },
      "source": [
        "# def evaluate(net, data_dict) :\n",
        "\n",
        "#   # test batch generator\n",
        "#   test_gen = data_dict['val']['batch_greedy']\n",
        "  \n",
        "#   # number of test samples\n",
        "#   test_samples = len(data_dict['val']['df'])\n",
        "\n",
        "#   batch_size=32\n",
        "\n",
        "#   num_hits = 0\n",
        "  \n",
        "#   for _ in range(test_samples//batch_size) :\n",
        "#     (a,b),c = next(test_gen)\n",
        "#     c = np.argmax(c, axis=2)\n",
        "#     # print(c)\n",
        "#     l1 = data_dict['tokenizer'].decode(c, mode='output')\n",
        "#     out = net.model.predict([a,b])\n",
        "#     out = np.argmax(out,axis=2) \n",
        "#     l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "#     print(l1)\n",
        "#     print(l2)\n",
        "#     print()\n",
        "#     num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "#     # break\n",
        "\n",
        "#   print(\"Val Acc \", num_hits/test_samples)\n",
        "#   wandb.log({\"final_val_acc\":  num_hits/test_samples})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDLMoTWfBn86"
      },
      "source": [
        "class tools:\n",
        "  def init_params(config,data_dict):\n",
        "  \n",
        "    # returning parameters\n",
        "    params = {\n",
        "        'num_encode_layers' : config.num_encode_layers,\n",
        "        'num_decode_layers' : config.num_decode_layers,\n",
        "        'cell_type' : config.cell_type,\n",
        "        'rep_size' : config.rep_size,\n",
        "        'embed_size' : config.embed_size,\n",
        "        'dropout' : config.dropout,\n",
        "        'num_epochs' : config.num_epochs,\n",
        "        'num_epochs_2' : config.num_epochs_2,\n",
        "        'data_dict' : data_dict,\n",
        "        'batch_size' : config.batch_size\n",
        "    }\n",
        "    return params"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AWE1lLSDDXh"
      },
      "source": [
        "# sweep configuration\n",
        "sweep_config = {\n",
        "    \n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'val_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    \n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM', 'GRU', 'SimpleRNN']  \n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'rep_size': {\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_epochs_2' : {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_encode_layers': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'num_decode_layers': {\n",
        "            'values': [1, 2, 3]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqDamLUoDQWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089b0838-3d76-4dd0-94ec-3d68782bec1a"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='dakshina_v3')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: jaerzcyc\n",
            "Sweep URL: https://wandb.ai/ramkamal/dakshina_v3/sweeps/jaerzcyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez3f-pjxDTU8"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      # copying the config \n",
        "      config = wandb.config\n",
        " \n",
        "      # naming the run\n",
        "      wandb.run.name = 'typ:'+config['cell_type'][:4]+ '_' + 'dro:'+str(config['dropout'])+ '_' + 'enc:' + str(config['num_encode_layers'])+ '_' + 'dec:'+str(config['num_decode_layers'])\n",
        "      \n",
        "      # returning the data dictionairy\n",
        "      data_dict = dict_data_dict[config.batch_size]\n",
        "\n",
        "      # copying the parameters\n",
        "      params = tools.init_params(config,data_dict)\n",
        "\n",
        "      # creating and training the first model\n",
        "      network = rnn(params)\n",
        "      run_details = network.compile_and_fit(data_dict, params)\n",
        "\n",
        "      # creating and training/ fine-tuning the second model\n",
        "      # rnn_2 = rnn_second(network.details)\n",
        "      # run_details_2 = rnn_2.compile_and_fit(data_dict, params)\n",
        "      \n",
        "      # rnn_2.evaluate(data_dict, train=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QBd1gJDV-R"
      },
      "source": [
        "# sweep_id = '7g0porer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCq49t4zDZod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32627a3f-b355-45b7-eba9-6c344446fdec"
      },
      "source": [
        "# performing the sweep\n",
        "wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pnvlhque with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decode_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encode_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs_2: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trep_size: 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">balmy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ramkamal/dakshina_v3\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v3</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/ramkamal/dakshina_v3/sweeps/jaerzcyc\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v3/sweeps/jaerzcyc</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/ramkamal/dakshina_v3/runs/pnvlhque\" target=\"_blank\">https://wandb.ai/ramkamal/dakshina_v3/runs/pnvlhque</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210514_073617-pnvlhque</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 64)     19200       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 64)     33024       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     500         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 64), (None,  33024       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 64), ( 19200       embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 50)     3250        lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 108,498\n",
            "Trainable params: 108,498\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2056/2056 [==============================] - 116s 47ms/step - loss: 0.8919 - acc: 0.2937 - val_loss: 0.5927 - val_acc: 0.4760\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 64)     19200       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 64)     33024       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     500         input_2[0][0]                    \n",
            "                                                                 tf.math.argmax_28[0][0]          \n",
            "                                                                 tf.math.argmax_29[0][0]          \n",
            "                                                                 tf.math.argmax_30[0][0]          \n",
            "                                                                 tf.math.argmax_31[0][0]          \n",
            "                                                                 tf.math.argmax_32[0][0]          \n",
            "                                                                 tf.math.argmax_33[0][0]          \n",
            "                                                                 tf.math.argmax_34[0][0]          \n",
            "                                                                 tf.math.argmax_35[0][0]          \n",
            "                                                                 tf.math.argmax_36[0][0]          \n",
            "                                                                 tf.math.argmax_37[0][0]          \n",
            "                                                                 tf.math.argmax_38[0][0]          \n",
            "                                                                 tf.math.argmax_39[0][0]          \n",
            "                                                                 tf.math.argmax_40[0][0]          \n",
            "                                                                 tf.math.argmax_41[0][0]          \n",
            "                                                                 tf.math.argmax_42[0][0]          \n",
            "                                                                 tf.math.argmax_43[0][0]          \n",
            "                                                                 tf.math.argmax_44[0][0]          \n",
            "                                                                 tf.math.argmax_45[0][0]          \n",
            "                                                                 tf.math.argmax_46[0][0]          \n",
            "                                                                 tf.math.argmax_47[0][0]          \n",
            "                                                                 tf.math.argmax_48[0][0]          \n",
            "                                                                 tf.math.argmax_49[0][0]          \n",
            "                                                                 tf.math.argmax_50[0][0]          \n",
            "                                                                 tf.math.argmax_51[0][0]          \n",
            "                                                                 tf.math.argmax_52[0][0]          \n",
            "                                                                 tf.math.argmax_53[0][0]          \n",
            "                                                                 tf.math.argmax_54[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 64), (None,  33024       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 64), ( 19200       embedding_1[1][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 lstm_3[1][1]                     \n",
            "                                                                 lstm_3[1][2]                     \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 lstm_3[2][1]                     \n",
            "                                                                 lstm_3[2][2]                     \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 lstm_3[3][1]                     \n",
            "                                                                 lstm_3[3][2]                     \n",
            "                                                                 embedding_1[5][0]                \n",
            "                                                                 lstm_3[4][1]                     \n",
            "                                                                 lstm_3[4][2]                     \n",
            "                                                                 embedding_1[6][0]                \n",
            "                                                                 lstm_3[5][1]                     \n",
            "                                                                 lstm_3[5][2]                     \n",
            "                                                                 embedding_1[7][0]                \n",
            "                                                                 lstm_3[6][1]                     \n",
            "                                                                 lstm_3[6][2]                     \n",
            "                                                                 embedding_1[8][0]                \n",
            "                                                                 lstm_3[7][1]                     \n",
            "                                                                 lstm_3[7][2]                     \n",
            "                                                                 embedding_1[9][0]                \n",
            "                                                                 lstm_3[8][1]                     \n",
            "                                                                 lstm_3[8][2]                     \n",
            "                                                                 embedding_1[10][0]               \n",
            "                                                                 lstm_3[9][1]                     \n",
            "                                                                 lstm_3[9][2]                     \n",
            "                                                                 embedding_1[11][0]               \n",
            "                                                                 lstm_3[10][1]                    \n",
            "                                                                 lstm_3[10][2]                    \n",
            "                                                                 embedding_1[12][0]               \n",
            "                                                                 lstm_3[11][1]                    \n",
            "                                                                 lstm_3[11][2]                    \n",
            "                                                                 embedding_1[13][0]               \n",
            "                                                                 lstm_3[12][1]                    \n",
            "                                                                 lstm_3[12][2]                    \n",
            "                                                                 embedding_1[14][0]               \n",
            "                                                                 lstm_3[13][1]                    \n",
            "                                                                 lstm_3[13][2]                    \n",
            "                                                                 embedding_1[15][0]               \n",
            "                                                                 lstm_3[14][1]                    \n",
            "                                                                 lstm_3[14][2]                    \n",
            "                                                                 embedding_1[16][0]               \n",
            "                                                                 lstm_3[15][1]                    \n",
            "                                                                 lstm_3[15][2]                    \n",
            "                                                                 embedding_1[17][0]               \n",
            "                                                                 lstm_3[16][1]                    \n",
            "                                                                 lstm_3[16][2]                    \n",
            "                                                                 embedding_1[18][0]               \n",
            "                                                                 lstm_3[17][1]                    \n",
            "                                                                 lstm_3[17][2]                    \n",
            "                                                                 embedding_1[19][0]               \n",
            "                                                                 lstm_3[18][1]                    \n",
            "                                                                 lstm_3[18][2]                    \n",
            "                                                                 embedding_1[20][0]               \n",
            "                                                                 lstm_3[19][1]                    \n",
            "                                                                 lstm_3[19][2]                    \n",
            "                                                                 embedding_1[21][0]               \n",
            "                                                                 lstm_3[20][1]                    \n",
            "                                                                 lstm_3[20][2]                    \n",
            "                                                                 embedding_1[22][0]               \n",
            "                                                                 lstm_3[21][1]                    \n",
            "                                                                 lstm_3[21][2]                    \n",
            "                                                                 embedding_1[23][0]               \n",
            "                                                                 lstm_3[22][1]                    \n",
            "                                                                 lstm_3[22][2]                    \n",
            "                                                                 embedding_1[24][0]               \n",
            "                                                                 lstm_3[23][1]                    \n",
            "                                                                 lstm_3[23][2]                    \n",
            "                                                                 embedding_1[25][0]               \n",
            "                                                                 lstm_3[24][1]                    \n",
            "                                                                 lstm_3[24][2]                    \n",
            "                                                                 embedding_1[26][0]               \n",
            "                                                                 lstm_3[25][1]                    \n",
            "                                                                 lstm_3[25][2]                    \n",
            "                                                                 embedding_1[27][0]               \n",
            "                                                                 lstm_3[26][1]                    \n",
            "                                                                 lstm_3[26][2]                    \n",
            "                                                                 embedding_1[28][0]               \n",
            "                                                                 lstm_3[27][1]                    \n",
            "                                                                 lstm_3[27][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 50)     3250        lstm_3[1][0]                     \n",
            "                                                                 lstm_3[2][0]                     \n",
            "                                                                 lstm_3[3][0]                     \n",
            "                                                                 lstm_3[4][0]                     \n",
            "                                                                 lstm_3[5][0]                     \n",
            "                                                                 lstm_3[6][0]                     \n",
            "                                                                 lstm_3[7][0]                     \n",
            "                                                                 lstm_3[8][0]                     \n",
            "                                                                 lstm_3[9][0]                     \n",
            "                                                                 lstm_3[10][0]                    \n",
            "                                                                 lstm_3[11][0]                    \n",
            "                                                                 lstm_3[12][0]                    \n",
            "                                                                 lstm_3[13][0]                    \n",
            "                                                                 lstm_3[14][0]                    \n",
            "                                                                 lstm_3[15][0]                    \n",
            "                                                                 lstm_3[16][0]                    \n",
            "                                                                 lstm_3[17][0]                    \n",
            "                                                                 lstm_3[18][0]                    \n",
            "                                                                 lstm_3[19][0]                    \n",
            "                                                                 lstm_3[20][0]                    \n",
            "                                                                 lstm_3[21][0]                    \n",
            "                                                                 lstm_3[22][0]                    \n",
            "                                                                 lstm_3[23][0]                    \n",
            "                                                                 lstm_3[24][0]                    \n",
            "                                                                 lstm_3[25][0]                    \n",
            "                                                                 lstm_3[26][0]                    \n",
            "                                                                 lstm_3[27][0]                    \n",
            "                                                                 lstm_3[28][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_28 (TFOpLambda)  (None, None)         0           dense[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_29 (TFOpLambda)  (None, None)         0           dense[2][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_30 (TFOpLambda)  (None, None)         0           dense[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_31 (TFOpLambda)  (None, None)         0           dense[4][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_32 (TFOpLambda)  (None, None)         0           dense[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_33 (TFOpLambda)  (None, None)         0           dense[6][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_34 (TFOpLambda)  (None, None)         0           dense[7][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_35 (TFOpLambda)  (None, None)         0           dense[8][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_36 (TFOpLambda)  (None, None)         0           dense[9][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_37 (TFOpLambda)  (None, None)         0           dense[10][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_38 (TFOpLambda)  (None, None)         0           dense[11][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_39 (TFOpLambda)  (None, None)         0           dense[12][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_40 (TFOpLambda)  (None, None)         0           dense[13][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_41 (TFOpLambda)  (None, None)         0           dense[14][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_42 (TFOpLambda)  (None, None)         0           dense[15][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_43 (TFOpLambda)  (None, None)         0           dense[16][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_44 (TFOpLambda)  (None, None)         0           dense[17][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_45 (TFOpLambda)  (None, None)         0           dense[18][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_46 (TFOpLambda)  (None, None)         0           dense[19][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_47 (TFOpLambda)  (None, None)         0           dense[20][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_48 (TFOpLambda)  (None, None)         0           dense[21][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_49 (TFOpLambda)  (None, None)         0           dense[22][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_50 (TFOpLambda)  (None, None)         0           dense[23][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_51 (TFOpLambda)  (None, None)         0           dense[24][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_52 (TFOpLambda)  (None, None)         0           dense[25][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_53 (TFOpLambda)  (None, None)         0           dense[26][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_54 (TFOpLambda)  (None, None)         0           dense[27][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 50)     0           dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "                                                                 dense[6][0]                      \n",
            "                                                                 dense[7][0]                      \n",
            "                                                                 dense[8][0]                      \n",
            "                                                                 dense[9][0]                      \n",
            "                                                                 dense[10][0]                     \n",
            "                                                                 dense[11][0]                     \n",
            "                                                                 dense[12][0]                     \n",
            "                                                                 dense[13][0]                     \n",
            "                                                                 dense[14][0]                     \n",
            "                                                                 dense[15][0]                     \n",
            "                                                                 dense[16][0]                     \n",
            "                                                                 dense[17][0]                     \n",
            "                                                                 dense[18][0]                     \n",
            "                                                                 dense[19][0]                     \n",
            "                                                                 dense[20][0]                     \n",
            "                                                                 dense[21][0]                     \n",
            "                                                                 dense[22][0]                     \n",
            "                                                                 dense[23][0]                     \n",
            "                                                                 dense[24][0]                     \n",
            "                                                                 dense[25][0]                     \n",
            "                                                                 dense[26][0]                     \n",
            "                                                                 dense[27][0]                     \n",
            "                                                                 dense[28][0]                     \n",
            "==================================================================================================\n",
            "Total params: 108,498\n",
            "Trainable params: 108,498\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/25\n",
            "  60/2056 [..............................] - ETA: 6:51 - loss: 1.0793 - acc: 0.0883"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBotvdIuDczo"
      },
      "source": [
        "# Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfaOoDTt-Zd"
      },
      "source": [
        "# params = {\n",
        "#     'num_encode_layers' : 2,\n",
        "#     'num_decode_layers' : 2,\n",
        "#     'cell_type' : 'LSTM', \n",
        "#     'rep_size' : 256,\n",
        "#     'embed_size' : 10,\n",
        "#     'dropout' : 0,\n",
        "#     'num_epochs' : 5,\n",
        "#     'num_epochs_2' : 1,\n",
        "#     'data_dict' : data_dict,\n",
        "#     'batch_size' : 32\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gar8YRXwLV94"
      },
      "source": [
        "# wandb.init()\n",
        "# network = rnn(params)\n",
        "# plot_model(network.model, show_shapes=True)\n",
        "# network.compile_and_fit(data_dict, params)\n",
        "# rnn_2 = rnn_second(network.details)\n",
        "# run_details_2 = rnn_2.compile_and_fit(data_dict, params)\n",
        "# rnn_2.evaluate(data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89DZv8QtQrc-"
      },
      "source": [
        "# evaluate(rnn_2, data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoVmYJXQV2Fg"
      },
      "source": [
        "## Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvJ0o0TDV4ns"
      },
      "source": [
        "def evaluate_test:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3crs0giYyAo"
      },
      "source": [
        "# ROUGH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLTe3CV0YxXS"
      },
      "source": [
        "data_dict['tokenizer']."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyGGRLtSY0sl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}