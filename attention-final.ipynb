{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/ramkamal2000/cs6910_assignment3/blob/main/attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Mount Drive","metadata":{"id":"-0cs3veZGUgH"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"Z4DJDDt2GWB2","outputId":"c72c7c74-d539-45a6-ee1e-4555aecd21f2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installing Required Packages","metadata":{"id":"InRSrSGNGkC5"}},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:18:36.257741Z","iopub.status.busy":"2021-05-17T07:18:36.256383Z","iopub.status.idle":"2021-05-17T07:18:36.26046Z","shell.execute_reply":"2021-05-17T07:18:36.25996Z"},"id":"anticipated-fourth","papermill":{"duration":0.029045,"end_time":"2021-05-17T07:18:36.26058","exception":false,"start_time":"2021-05-17T07:18:36.231535","status":"completed"},"tags":[],"outputId":"b8e207bd-77c0-47ea-8799-93a48e6a65b6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{"id":"p5WxEXheGtId"}},{"cell_type":"code","source":"import tarfile\nimport os\nimport pandas as pd\nimport keras\nimport numpy as np\nimport wandb\nimport tensorflow as tf\nimport pickle\nimport shutil\nfrom keras import layers\nfrom keras.layers import LSTM, Dense, Embedding, Input\nfrom keras.models import Model\nfrom keras.utils.vis_utils import plot_model\nfrom tqdm.auto import tqdm\nfrom keras.layers import Lambda\nfrom keras import backend as K\nimport datetime\nfrom math import ceil\nfrom pprint import pprint\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Concatenate, TimeDistributed,Dropout\nfrom tensorflow.python.keras.models import Model","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:18:36.310139Z","iopub.status.busy":"2021-05-17T07:18:36.309655Z","iopub.status.idle":"2021-05-17T07:18:41.934179Z","shell.execute_reply":"2021-05-17T07:18:41.93311Z"},"id":"standard-reasoning","papermill":{"duration":5.652578,"end_time":"2021-05-17T07:18:41.934358","exception":false,"start_time":"2021-05-17T07:18:36.28178","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Current Directory","metadata":{"id":"CJGE3rr4G4HF"}},{"cell_type":"code","source":"# MAKE REQUIRED CHANGES\n\n# dir = '/content'\ndir = '/kaggle/working'\n# dir = os.getcwd()","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:18:41.984657Z","iopub.status.busy":"2021-05-17T07:18:41.98371Z","iopub.status.idle":"2021-05-17T07:18:41.98633Z","shell.execute_reply":"2021-05-17T07:18:41.985902Z"},"papermill":{"duration":0.028997,"end_time":"2021-05-17T07:18:41.986472","exception":false,"start_time":"2021-05-17T07:18:41.957475","status":"completed"},"tags":[],"id":"equivalent-certificate"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading Dataset","metadata":{"id":"3sU5GLNYG6tB"}},{"cell_type":"code","source":"!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n\nif not os.path.isdir(dir + '/dakshina_dataset_v1.0'):\n  tarfile.open(dir + \"/dakshina_dataset_v1.0.tar\").extractall()","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:18:42.036397Z","iopub.status.busy":"2021-05-17T07:18:42.035854Z","iopub.status.idle":"2021-05-17T07:19:05.147896Z","shell.execute_reply":"2021-05-17T07:19:05.147294Z"},"id":"continental-credit","papermill":{"duration":23.139598,"end_time":"2021-05-17T07:19:05.148041","exception":false,"start_time":"2021-05-17T07:18:42.008443","status":"completed"},"tags":[],"outputId":"6bbfa3f2-0180-46b5-a02f-bacf64c4129e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logging Onto wandb","metadata":{"id":"xvxFcLPvHCJI"}},{"cell_type":"code","source":"wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')\nwandb.init(project='final_attn')","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:14.798215Z","iopub.status.busy":"2021-05-17T07:19:14.797314Z","iopub.status.idle":"2021-05-17T07:19:15.856203Z","shell.execute_reply":"2021-05-17T07:19:15.855327Z"},"id":"royal-martin","papermill":{"duration":1.158591,"end_time":"2021-05-17T07:19:15.856331","exception":false,"start_time":"2021-05-17T07:19:14.69774","status":"completed"},"tags":[],"outputId":"16331ace-cade-4fab-c6c9-95a6ba90f106"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{"id":"incoming-pound","papermill":{"duration":1.32883,"end_time":"2021-05-17T07:19:17.569451","exception":false,"start_time":"2021-05-17T07:19:16.240621","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class data_loader():\n\n  @staticmethod\n  def _load_raw_df(languages = [\"ta\"]):\n    lex = dict()\n    lex['train'], lex['val'], lex['test'] = [], [], [] \n    column_names = ['output', 'input', 'count']\n    \n    for la in languages:\n      lex['train'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n      lex['val'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n      lex['test'].append(pd.read_csv(dir + '/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n\n    lex['train'] = pd.concat(lex['train'])\n    lex['val'] = pd.concat(lex['val'])\n    lex['test'] = pd.concat(lex['test'])\n\n    return lex    \n\n  @staticmethod\n  def _make_final_df(lex):\n    \n    for div in ['train', 'val']:\n    \n      # removing non max transliterations\n      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n      lex[div] = lex[div][idx].reset_index(drop=True)\n\n      # calclulating difference in lengths of various transliterations\n      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n\n      # removing transliterations that vary by a lot in length\n      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n      lex[div] = lex[div][idx].reset_index(drop=True)\n\n      # removing duplicates if any remain\n      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n\n      # removing redundant columns\n      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n\n      # shuffling the dataset i.e. rows of the dataset\n      lex[div] = lex[div].sample(frac=1, random_state=6910)\n      lex[div] = lex[div].reset_index(drop=True)\n\n    lex['test'] = lex['test'].sample(frac=1, random_state=6910)\n    lex['test'].drop(labels=['count'], axis=1, inplace=True)\n    lex['test'] = lex['test'].reset_index(drop=True)\n    return lex\n\n  @staticmethod\n  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n\n    while True:\n        for j in range(0, len(X), batch_size):\n            \n            # placeholder data structures\n            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n\n            # assessing one batch at a time\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n\n                for t, word in enumerate(input_text):\n                  encoder_input_data[i, t] = word\n                for t, word in enumerate(target_text):\n                    if t<len(target_text)-1:\n                        # decoder input sequence\n                        # does not include the <EOW> token\n                        decoder_input_data[i, t] = word \n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the <SOW> token\n                        decoder_target_data[i, t - 1, word] = 1.\n                    \n            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n\n  @staticmethod\n  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n\n    while True:\n        for j in range(0, len(X), batch_size):\n\n            # placeholder data structures\n            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n            \n            # assessing one batch at a time\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text):\n                  encoder_input_data[i, t] = word\n                for t, word in enumerate(target_text):\n                    if t==0 :\n                        decoder_input_data[i, t] = 1 # decoder input seq\n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the START_ token\n                        decoder_target_data[i, t - 1, word] = 1.\n                    \n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:17.732004Z","iopub.status.busy":"2021-05-17T07:19:17.730951Z","iopub.status.idle":"2021-05-17T07:19:17.763913Z","shell.execute_reply":"2021-05-17T07:19:17.771634Z"},"id":"official-chick","papermill":{"duration":0.129157,"end_time":"2021-05-17T07:19:17.771877","exception":false,"start_time":"2021-05-17T07:19:17.64272","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer class\nclass Tokenizer:\n\n  def __init__(self, df):\n\n    self.start_token = '<SOW>'\n    self.stop_token = '<EOW>'\n    self.unknown_token = '<UNK>'\n\n    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n\n    input_words = df.input.tolist()\n    output_words = df.output.tolist()\n\n    for word in input_words:\n      tokens = str(word)\n      for token in tokens:\n        if token not in self.input_corpus:\n          self.input_corpus.append(token)\n\n    for word in output_words:\n      tokens = str(word)\n      for token in tokens:\n        if token not in self.output_corpus:\n          self.output_corpus.append(token)\n    \n    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n    \n    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n    self.decode_dict_output.update({2:''})\n\n  # takes in lists of words and returns lists of integers\n  def encode(self, X, mode='input'):\n\n    if (mode=='input'):\n      input_list = []\n      for word in X:\n        word = str(word)\n        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n        input_list.append(integer_list)\n      \n      return input_list\n    \n    if (mode=='output'):\n      output_list = []\n      for word in X:\n        word = str(word)\n        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n        output_list.append(integer_list)\n      \n      return output_list\n    \n  # takes in lists of integers and returns lists of words\n  def decode(self, X, mode='input'):\n\n    if (mode=='input'):\n      input_list = []\n      for integers in X:\n        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n        input_list.append(''.join(token_list))\n      \n      return input_list\n\n    if (mode=='output'):\n      output_list = []\n      for integers in X:\n        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[:-1]] \n        output_list.append(''.join(token_list))\n      \n      return output_list","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:17.974876Z","iopub.status.busy":"2021-05-17T07:19:17.973905Z","iopub.status.idle":"2021-05-17T07:19:17.989573Z","shell.execute_reply":"2021-05-17T07:19:17.990717Z"},"id":"numeric-exception","papermill":{"duration":0.1239,"end_time":"2021-05-17T07:19:17.99094","exception":false,"start_time":"2021-05-17T07:19:17.86704","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final function that returns the required nested datastructure\ndef return_data_dict(languages=[\"ta\"], batch_size=32):\n\n  lex = data_loader._load_raw_df(languages)\n  lex = data_loader._make_final_df(lex)\n\n  data_dict = dict()\n\n  df_train = lex['train']\n  df_val = lex['val']\n  df_test = lex['test']\n\n  tk = Tokenizer(df_train)\n\n  data_dict['in_size'] = len(tk.input_corpus) + 1\n  data_dict['out_size'] = len(tk.output_corpus) + 1\n\n  X_train = tk.encode(df_train.input.tolist(), mode='input')\n  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n  \n  X_val = tk.encode(df_val.input.tolist(), mode='input')\n  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n  \n  X_test = tk.encode(df_test.input.tolist(), mode='input')\n  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n\n\n  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n\n\n  data_dict['train']['df'] = df_train\n  data_dict['val']['df'] = df_val\n  data_dict['test']['df'] = df_test\n\n\n  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n  \n  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n  \n  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n\n\n  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'], data_dict['test']['max_source_length'])\n  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'], data_dict['test']['max_target_length'])\n\n\n  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n  \n  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n\n  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)  \n  data_dict['test']['batch_greedy'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)    \n  data_dict['test']['batch_1'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], 1)\n\n\n  data_dict['tokenizer'] = tk\n\n  return data_dict","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:18.107632Z","iopub.status.busy":"2021-05-17T07:19:18.106147Z","iopub.status.idle":"2021-05-17T07:19:18.108674Z","shell.execute_reply":"2021-05-17T07:19:18.109073Z"},"id":"instant-coordinate","papermill":{"duration":0.059174,"end_time":"2021-05-17T07:19:18.109215","exception":false,"start_time":"2021-05-17T07:19:18.050041","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final data dictionary\ndata_dict = return_data_dict()","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:18.192882Z","iopub.status.busy":"2021-05-17T07:19:18.192062Z","iopub.status.idle":"2021-05-17T07:19:23.183791Z","shell.execute_reply":"2021-05-17T07:19:23.184245Z"},"id":"chubby-ferry","papermill":{"duration":5.036059,"end_time":"2021-05-17T07:19:23.184471","exception":false,"start_time":"2021-05-17T07:19:18.148412","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving test dataframe for later use\n# data_dict['test']['df'].to_csv('/content/drive/MyDrive/Colab Notebooks/DL3/test.csv')","metadata":{"id":"MqWxPL4iJa9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention - Question 5","metadata":{"id":"christian-delicious","papermill":{"duration":3.302699,"end_time":"2021-05-17T07:19:26.531528","exception":false,"start_time":"2021-05-17T07:19:23.228829","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# attention layer\n# obtained from https://github.com/thushv89/attention_keras/\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n        # print(input_shape[0][2],input_shape[1][2])\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        # print(\"attn called\")\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:28.248741Z","iopub.status.busy":"2021-05-17T07:19:28.247733Z","iopub.status.idle":"2021-05-17T07:19:28.291057Z","shell.execute_reply":"2021-05-17T07:19:28.291766Z"},"id":"jewish-seattle","papermill":{"duration":0.141565,"end_time":"2021-05-17T07:19:28.29194","exception":false,"start_time":"2021-05-17T07:19:28.150375","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# single class for RNN\nclass attn_rnn():\n    \n  # initializing RNN\n  def __init__(self,params) :\n    in_size = params['data_dict']['in_size']\n    out_size = params['data_dict']['out_size']\n    dropout = params['dropout']\n    embed_size = params['embed_size']\n    rep_size = params['rep_size']\n\n    encoder_inputs = Input(shape=(None,))\n    decoder_inputs = Input(shape=(None,))\n\n\n    encoder_emb = Embedding(in_size, embed_size, input_length=None,mask_zero=True)\n    temp1 = encoder_emb(encoder_inputs)\n    encoder_lstm = LSTM(rep_size, return_sequences=True, return_state=True,dropout=dropout)\n    encoder_out, *encoder_state = encoder_lstm(temp1)\n    \n\n    decoder_emb = Embedding(out_size, embed_size , input_length=None,mask_zero=True)\n    temp2 = decoder_emb(decoder_inputs)\n    decoder_lstm = LSTM(rep_size, return_sequences=True, return_state=True,dropout=dropout)\n    decoder_out, *decoder_state = decoder_lstm(temp2, initial_state=encoder_state)\n\n    # Attention layer\n    attn_layer = AttentionLayer()\n    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n\n    # Concat attention input and decoder GRU output\n    decoder_concat_input = Concatenate(axis=-1)([decoder_out, attn_out])\n\n    # Dense layer\n    drop = Dropout(dropout)\n    decoder_concat_input = drop(decoder_concat_input)\n    dense = Dense(out_size, activation='softmax')\n    dense_time = TimeDistributed(dense)\n    decoder_pred = dense_time(decoder_concat_input)\n\n    # Full model\n    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n\n    batch_size = 1\n\n    \"\"\" Encoder (Inference) model \"\"\"\n    encoder_inf_inputs = Input(shape=(None,))\n    temp3 = encoder_emb(encoder_inf_inputs)\n    encoder_inf_out, *encoder_inf_state = encoder_lstm(temp3)\n    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n\n    \"\"\" Decoder (Inference) model \"\"\"\n    decoder_inf_inputs = Input(shape=(None,))\n    encoder_inf_states = Input(shape=(None, rep_size))\n    decoder_init_stateh = Input(shape=(rep_size,))\n    decoder_init_statec = Input(shape=(rep_size,))\n    decoder_init_state = [decoder_init_stateh,decoder_init_statec]\n    temp = decoder_emb(decoder_inf_inputs)\n    decoder_inf_out, *decoder_inf_state = decoder_lstm(temp, initial_state=decoder_init_state)\n    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n    decoder_inf_concat = Concatenate(axis=-1)([decoder_inf_out, attn_inf_out])\n    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n\n    self.model = full_model\n    self.encoder = encoder_model\n    self.decoder = decoder_model\n\n  # compile and fit the model\n  def compile_and_fit(self,data_dict,params):\n\n    self.model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n\n    self.model.summary()\n    batch_size=params['batch_size']\n    epochs=params['num_epochs']\n    val_samples = len(data_dict['val']['df'])\n    train_samples = len(data_dict['train']['df'])\n\n    self.model.fit_generator(generator = data_dict['train']['batch'],\n                 steps_per_epoch=train_samples//batch_size,\n                 epochs=epochs,\n                 validation_data=data_dict['val']['batch'],\n                 validation_steps=val_samples//batch_size\n                 )\n    \n  # evaluate the test dataset\n  def evaluate(self,data_dict,filename) :\n    generator = data_dict['test']['batch_1']\n    tk = data_dict['tokenizer']\n    acc = 0\n    num = len(data_dict['test']['df'])\n    \n    X = []\n    Y_true = []\n    Y_pred = []\n    attn = []\n    for j in tqdm(range(num)) :\n      (a,b),c = next(generator)\n      g = [np.argmax(c,axis=-1)[0]]\n      enc_outs, enc_last_state = self.encoder.predict(a)\n      dec_state = enc_last_state\n      attention_weights = []\n      word = []\n      for i in range(23):\n\n          dec_out, attention, dec_state = self.decoder.predict([enc_outs, dec_state, b])\n          dec_ind = np.argmax(dec_out, axis=-1)\n\n          word.append(dec_ind[0][0])\n          b = dec_ind\n          attention_weights.append(attention)\n          if dec_ind[0][0] == 2 :\n            break\n      str1 = tk.decode(g,mode='output')\n      str2 = tk.decode([word],mode='output')\n      str3 = tk.decode(a)\n      \n      ###############################################################\n      X.append(str3[0])\n      Y_true.append(str1[0])\n      Y_pred.append(str2[0])\n\n      attn.append(attention_weights)\n      # print(word)\n      # print(str1[0],str2[0])\n      if str1[0] == str2[0] :\n        acc += 1\n   \n    df = pd.DataFrame({\n      'X': X,\n      'Y_true': Y_true,\n      'Y_pred': Y_pred\n    })\n\n    df.to_csv(filename)\n    try:\n      wandb.log({'attn_greedy_csv': wandb.Table(dataframe=df)})\n    except:\n      pass\n\n    with open('/kaggle/working/attm_weights', 'wb') as f:\n        pickle.dump(attn, f)\n    \n    \n    print(\"Test accuracy : \",acc/num)\n\n  # helper function to retrieve weights\n  def get_attn_weights(self, a, b) :\n    enc_outs, enc_last_state = self.encoder.predict(a)\n    dec_state = enc_last_state\n    attention_weights = []\n    word = []\n    for i in range(23):\n\n        dec_out, attention, dec_state = self.decoder.predict([enc_outs, dec_state, b])\n        dec_ind = np.argmax(dec_out, axis=-1)\n\n        word.append(dec_ind[0][0])\n        b = dec_ind\n        attention_weights.append((dec_ind, attention))\n        if dec_ind[0][0] == 2 :\n          break\n\n    return attention_weights\n","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:32.955415Z","iopub.status.busy":"2021-05-17T07:19:32.954355Z","iopub.status.idle":"2021-05-17T07:19:32.958877Z","shell.execute_reply":"2021-05-17T07:19:32.95997Z"},"id":"organic-location","papermill":{"duration":1.353483,"end_time":"2021-05-17T07:19:32.960198","exception":false,"start_time":"2021-05-17T07:19:31.606715","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Model Params","metadata":{"id":"isADhSUYKdwL"}},{"cell_type":"code","source":"params = {\n    'rep_size' : 128,\n    'embed_size' : 16,\n    'dropout' : 0.5,\n    'num_epochs' : 15,\n    'data_dict' : data_dict,\n    'batch_size' : 32\n}","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:33.146872Z","iopub.status.busy":"2021-05-17T07:19:33.145844Z","iopub.status.idle":"2021-05-17T07:19:33.148446Z","shell.execute_reply":"2021-05-17T07:19:33.14769Z"},"id":"hybrid-bearing","papermill":{"duration":0.079244,"end_time":"2021-05-17T07:19:33.148624","exception":false,"start_time":"2021-05-17T07:19:33.06938","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing and fitting model\nnetwork = attn_rnn(params)\nnetwork.compile_and_fit(data_dict,params)","metadata":{"execution":{"iopub.execute_input":"2021-05-17T07:19:33.301333Z","iopub.status.busy":"2021-05-17T07:19:33.300497Z","iopub.status.idle":"2021-05-17T08:01:21.775013Z","shell.execute_reply":"2021-05-17T08:01:21.774469Z"},"id":"marked-process","papermill":{"duration":2508.542193,"end_time":"2021-05-17T08:01:21.775155","exception":false,"start_time":"2021-05-17T07:19:33.232962","status":"completed"},"tags":[],"outputId":"26ea0b93-08a5-44ab-9f3f-f45bd7af0ea0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate test data and publish output csv\nnetwork.evaluate(data_dict,dir+'/attn_greedy.csv')","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:01:38.398807Z","iopub.status.busy":"2021-05-17T08:01:38.398116Z","iopub.status.idle":"2021-05-17T08:52:36.88648Z","shell.execute_reply":"2021-05-17T08:52:36.887208Z"},"id":"accessory-plaza","papermill":{"duration":3066.838908,"end_time":"2021-05-17T08:52:36.887486","exception":false,"start_time":"2021-05-17T08:01:30.048578","status":"completed"},"tags":[],"outputId":"bc6f0034-4f53-41d9-f78b-2a0700915528"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam search function\ndef decode_sequence_beam_attn(input_seq,target_seq, k, encoder_model, decoder_model, tk, max_target_length=20, getall=False,alpha=0.7):\n    # encode the input as state vectors\n    enc_outs,states_value = encoder_model.predict(input_seq)\n    # generate empty target sequence of length 1.\n    # populate the first character of target sequence with the start character.\n    run_condition = [True for i in range(k)]\n    # print(len(states_value))\n    # print([target_seq] + [states_value])\n    results, attn,states_values_temp = decoder_model.predict([enc_outs , states_value,target_seq])\n    output_tokens = results\n    # print(results)\n    states_values_k = [states_values_temp for i in range(k)]\n    #get topk indices\n    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n    # print(ind)\n    bestk_ind = ind\n    output_tokens = np.array(output_tokens[0, -1, :])\n    bestk_prob = np.log(output_tokens[ind])\n    bestk_tot = [[bestk_ind[i]] for i in range(k)]\n    # print(bestk_tot)\n    # print(bestk_prob)\n\n    \n    while any(run_condition):\n        # print(\"##############\")\n        bestk_tot_new = []\n        bestk_prob_new = []\n        states_values_k_new = []\n        for i in range(k) :\n            # print(\"**\")\n            if run_condition[i] :\n                a = bestk_tot[i]\n                b = bestk_prob[i]\n                target_seq[0,0] = a[-1]\n                results,attn,states_values_temp = decoder_model.predict([enc_outs,states_values_k[i],target_seq],batch_size=1)\n\n                output_tokens = results\n\n                states_values_k_temp = [states_values_temp for m in range(k)]\n\n                states_values_k_new += states_values_k_temp\n                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n                bestk_ind = ind\n                output_tokens = np.array(output_tokens[0, -1, :])\n                bestk_prob_temp = output_tokens[ind]\n                # print(np.log(bestk_prob_temp))\n                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n                # print(bestk_tot_temp)\n                bestk_prob_temp2 = [(b*(np.power(len(bestk_tot_temp[j])-1,alpha)) + np.log(bestk_prob_temp[j]))/(np.power(len(bestk_tot_temp[j]),alpha)) for j in range(k)]\n                # print(bestk_prob_temp2)\n                bestk_prob_new += bestk_prob_temp2\n                bestk_tot_new += bestk_tot_temp\n            \n            else :\n                a = bestk_tot[i]\n                b = bestk_prob[i]\n                bestk_tot_new += [bestk_tot[i]]\n                bestk_prob_new += [b]\n                states_values_k_new += [states_values_k[i]]\n\n        bestk_prob_new = np.array(bestk_prob_new)\n        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n        bestk_tot = [bestk_tot_new[i] for i in ind]\n        states_values_k = [states_values_k_new[i] for i in ind]\n        bestk_prob = bestk_prob_new[ind]\n        run_condition = []\n        for i in range(k) :\n            a = bestk_tot[i]\n            b = bestk_prob[i]\n            if a[-1]!= 2 and len(a)<=max_target_length :\n              run_condition.append(True)\n            else :\n              run_condition.append(False)\n\n        # print(bestk_tot)\n\n    final_words = []\n    best_word = []\n    best = -5.0\n    for i in range(k) :\n      a = bestk_tot[i]\n      b = bestk_prob[i]\n      final_words += [a]\n      if b > best :\n        best_word = [a]\n        best = b\n\n    if getall :\n      return (tk.decode(final_words,'output'),best_word)\n    else :\n      return final_words,bestk_prob, best_word","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:53:26.380064Z","iopub.status.busy":"2021-05-17T08:53:26.378348Z","iopub.status.idle":"2021-05-17T08:53:26.380757Z","shell.execute_reply":"2021-05-17T08:53:26.381145Z"},"id":"attempted-nirvana","papermill":{"duration":8.108319,"end_time":"2021-05-17T08:53:26.381276","exception":false,"start_time":"2021-05-17T08:53:18.272957","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##run beam search\n\ngenerator = data_dict['test']['batch_1']\ntk = data_dict['tokenizer']\nm2 = network.encoder\nm3 = network.decoder\nacc = 0\nnum = len(data_dict['test']['df'])\nacc_k = 0\n# num = 50\nX = []\nY_true = []\nY_pred = []\nfor j in tqdm(range(num)) :\n  (a,b),c = next(generator)\n  g = [np.argmax(c,axis=-1)[0]]\n  attention_weights = []\n  K,word = decode_sequence_beam_attn(a,b,10,m2,m3,data_dict['tokenizer'],max_target_length=data_dict['max_target_length'],getall=True )\n  str1 = tk.decode(g,mode='output')\n  str2 = tk.decode(word,mode='output')\n  str3 = tk.decode(a)\n  X.append(str3[0])\n  Y_true.append(str1[0])\n  Y_pred.append(str2[0])\n\n  if str1[0] in K :\n    acc_k += 1\n  \n  if str1[0] == str2[0] :\n    acc += 1\n    \n  if (j+1) %500 == 0 :\n    print(acc/num)\n    print(acc_k/num)\n\n  df = pd.DataFrame({\n    'X': X,\n    'Y_true': Y_true,\n    'Y_pred': Y_pred\n  })\n\n  df.to_csv(dir+'/attn_beam')\n  try:\n    wandb.log({'attn_beam_csv': wandb.Table(dataframe=df)})\n  except:\n    pass\n\nprint(acc/num)\nprint(acc_k/num)","metadata":{"execution":{"iopub.execute_input":"2021-05-17T08:53:43.237083Z","iopub.status.busy":"2021-05-17T08:53:43.235017Z","iopub.status.idle":"2021-05-17T08:53:43.237798Z","shell.execute_reply":"2021-05-17T08:53:43.238301Z"},"id":"round-stage","papermill":{"duration":8.642327,"end_time":"2021-05-17T08:53:43.238472","exception":false,"start_time":"2021-05-17T08:53:34.596145","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}