{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcVxQrUVxtS"
      },
      "source": [
        "# TO DO\n",
        "1. Change structure of test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAUpzh1BLEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a180db-1765-4a3c-9c52-8ac66ab8859a"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.5MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=f2ca697826d965fda372315e0c31f49bafa4b5076224da7650235df2167fbe35\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=ab2a47dc7468a69078270dbc8c56a474f76e084bc254a66734179fe8469bfaaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, docker-pycreds, configparser, smmap, gitdb, GitPython, pathtools, shortuuid, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClAC3xAEhKS"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras.layers import LSTM, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tqdm.auto import tqdm\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "import datetime\n",
        "from math import ceil\n",
        "from pprint import pprint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9sbR5xCVo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0789b12a-9c20-4fdc-8b50-93451208706d"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "\n",
        "if not os.path.isdir('/content/dakshina_dataset_v1.0'):\n",
        "  tarfile.open(\"/content/dakshina_dataset_v1.0.tar\").extractall()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-12 19:44:26--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.188.128, 64.233.189.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   112MB/s    in 22s     \n",
            "\n",
            "2021-05-12 19:44:48 (87.0 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0fMpPf_BUUK",
        "outputId": "1ca6e242-77e2-4c78-a17b-60fef6673f1a"
      },
      "source": [
        "wandb.login(key='14394907543f59ea21931529e34b4d80d2ca8c9c')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BstzblcHmd5"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDJrkl5EUHX"
      },
      "source": [
        "class data_loader():\n",
        "\n",
        "  @staticmethod\n",
        "  def _load_raw_df(languages = [\"mr\",\"hi\"]):\n",
        "    lex = dict()\n",
        "    lex['train'], lex['val'], lex['test'] = [], [], [] \n",
        "    column_names = ['output', 'input', 'count']\n",
        "    \n",
        "    for la in languages:\n",
        "      lex['train'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.train.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['val'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.dev.tsv', sep='\\t', header=None, names=column_names))\n",
        "      lex['test'].append(pd.read_csv('/content/dakshina_dataset_v1.0/'+la+'/lexicons/'+la+'.translit.sampled.test.tsv', sep='\\t', header=None, names=column_names))\n",
        "\n",
        "    lex['train'] = pd.concat(lex['train'])\n",
        "    lex['val'] = pd.concat(lex['val'])\n",
        "    lex['test'] = pd.concat(lex['test'])\n",
        "\n",
        "    return lex    \n",
        "\n",
        "  @staticmethod\n",
        "  def _make_final_df(lex):\n",
        "    \n",
        "    for div in ['train', 'val', 'test']:\n",
        "    \n",
        "      # removing non max transliterations\n",
        "      idx = lex[div].groupby(['input'])['count'].transform(max) == lex[div]['count']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # calclulating difference in lengths of various transliterations\n",
        "      lex[div]['input_len'] = lex[div].apply(lambda x: len(str(x['input'])), axis=1)\n",
        "      lex[div]['output_len'] = lex[div].apply(lambda y: len(str(y['output'])), axis=1)\n",
        "      lex[div]['mod_dif'] = lex[div].apply(lambda z: abs(z['input_len'] - z['output_len']), axis=1) \n",
        "\n",
        "      # removing transliterations that vary by a lot in length\n",
        "      idx = lex[div].groupby(['input'])['mod_dif'].transform(min) == lex[div]['mod_dif']\n",
        "      lex[div] = lex[div][idx].reset_index(drop=True)\n",
        "\n",
        "      # removing duplicates if any remain\n",
        "      lex[div].drop_duplicates(subset='input', keep='first', inplace=True)\n",
        "\n",
        "      # removing redundant columns\n",
        "      lex[div].drop(labels=['count', 'input_len', 'output_len', 'mod_dif'], inplace=True, axis=1)\n",
        "\n",
        "      # shuffling the dataset i.e. rows of the dataset\n",
        "      lex[div] = lex[div].sample(frac=1)\n",
        "      lex[div].reset_index()\n",
        "\n",
        "    return lex\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            \n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, data_dict['max_target_length']),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "\n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t<len(target_text)-1:\n",
        "                        # decoder input sequence\n",
        "                        # does not include the <EOW> token\n",
        "                        decoder_input_data[i, t] = word \n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the <SOW> token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "  @staticmethod\n",
        "  def _generate_batch_greedy(X, y, data_dict, num_decoder_tokens, batch_size = 1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            # placeholder data structures\n",
        "            encoder_input_data = np.zeros((batch_size, data_dict['max_source_length']),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, 1),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, data_dict['max_target_length'], num_decoder_tokens),dtype='float32')\n",
        "            \n",
        "            # assessing one batch at a time\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text):\n",
        "                  encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_text):\n",
        "                    if t==0 :\n",
        "                        decoder_input_data[i, t] = 1 # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo1m6s3vDaN"
      },
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, df):\n",
        "\n",
        "    self.start_token = '<SOW>'\n",
        "    self.stop_token = '<EOW>'\n",
        "    self.unknown_token = '<UNK>'\n",
        "\n",
        "    self.input_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "    self.output_corpus = [self.start_token, self.stop_token, self.unknown_token]\n",
        "\n",
        "    input_words = df.input.tolist()\n",
        "    output_words = df.output.tolist()\n",
        "\n",
        "    for word in input_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.input_corpus:\n",
        "          self.input_corpus.append(token)\n",
        "\n",
        "    for word in output_words:\n",
        "      tokens = str(word)\n",
        "      for token in tokens:\n",
        "        if token not in self.output_corpus:\n",
        "          self.output_corpus.append(token)\n",
        "    \n",
        "    self.encode_dict_input = {self.input_corpus[i] : i+1 for i in range(len(self.input_corpus))}\n",
        "    self.decode_dict_input = {k:v for v,k in self.encode_dict_input.items()}\n",
        "    \n",
        "    \n",
        "    self.encode_dict_output = {self.output_corpus[i] : i+1 for i in range(len(self.output_corpus))}\n",
        "    self.decode_dict_output = {k:v for v,k in self.encode_dict_output.items()}\n",
        "\n",
        "  # takes in lists of words and returns lists of integers\n",
        "  def encode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list =np.array([self.encode_dict_input.get(token, self.encode_dict_input[self.unknown_token]) for token in word])\n",
        "        input_list.append(integer_list)\n",
        "      \n",
        "      return input_list\n",
        "    \n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for word in X:\n",
        "        word = str(word)\n",
        "        integer_list = np.array([self.encode_dict_output[self.start_token]] + [self.encode_dict_output.get(token, self.encode_dict_output[self.unknown_token]) for token in word] + [self.encode_dict_output[self.stop_token]])\n",
        "        output_list.append(integer_list)\n",
        "      \n",
        "      return output_list\n",
        "    \n",
        "  # takes in lists of integers and returns lists of words\n",
        "  def decode(self, X, mode='input'):\n",
        "\n",
        "    if (mode=='input'):\n",
        "      input_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_input.get(integer, '') for integer in integers] \n",
        "        input_list.append(''.join(token_list))\n",
        "      \n",
        "      return input_list\n",
        "\n",
        "    if (mode=='output'):\n",
        "      output_list = []\n",
        "      for integers in X:\n",
        "        token_list = [self.decode_dict_output.get(integer, '') for integer in integers[1:-1]] \n",
        "        output_list.append(''.join(token_list))\n",
        "      \n",
        "      return output_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mzVsRdBa4x"
      },
      "source": [
        "def return_data_dict(languages=['mr','hi'], batch_size=32):\n",
        "\n",
        "  lex = data_loader._load_raw_df(languages)\n",
        "  lex = data_loader._make_final_df(lex)\n",
        "\n",
        "  data_dict = dict()\n",
        "\n",
        "  df_train = lex['train']#.head(n=100)\n",
        "  df_val = lex['val']#.head(n=100)\n",
        "  df_test = lex['test']#.head(n=100)\n",
        "\n",
        "  tk = Tokenizer(df_train)\n",
        "\n",
        "  data_dict['in_size'] = len(tk.input_corpus) + 1\n",
        "  data_dict['out_size'] = len(tk.output_corpus) + 1\n",
        "\n",
        "  X_train = tk.encode(df_train.input.tolist(), mode='input')\n",
        "  Y_train = tk.encode(df_train.output.tolist(), mode='output')\n",
        "  \n",
        "  X_val = tk.encode(df_val.input.tolist(), mode='input')\n",
        "  Y_val = tk.encode(df_val.output.tolist(), mode='output')\n",
        "  \n",
        "  X_test = tk.encode(df_test.input.tolist(), mode='input')\n",
        "  Y_test = tk.encode(df_test.output.tolist(), mode='output')\n",
        "\n",
        "\n",
        "  data_dict['train'], data_dict['val'], data_dict['test']= dict(), dict(), dict()\n",
        "\n",
        "\n",
        "  data_dict['train']['df'] = df_train\n",
        "  data_dict['val']['df'] = df_val\n",
        "  data_dict['test']['df'] = df_test\n",
        "\n",
        "\n",
        "  data_dict['train']['max_source_length'] = np.max(np.array([len(x) for x in X_train]))\n",
        "  data_dict['train']['max_target_length'] = np.max(np.array([len(x) for x in Y_train]))\n",
        "  \n",
        "  data_dict['val']['max_source_length'] = np.max(np.array([len(x) for x in X_val]))\n",
        "  data_dict['val']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "  \n",
        "  data_dict['test']['max_source_length'] = np.max(np.array([len(x) for x in X_test]))\n",
        "  data_dict['test']['max_target_length'] = np.max(np.array([len(x) for x in Y_test]))\n",
        "\n",
        "\n",
        "  data_dict['max_source_length'] = max(data_dict['train']['max_source_length'], data_dict['val']['max_source_length'], data_dict['test']['max_source_length'])\n",
        "  data_dict['max_target_length'] = max(data_dict['train']['max_target_length'], data_dict['val']['max_target_length'], data_dict['test']['max_target_length'])\n",
        "\n",
        "\n",
        "  data_dict['train']['batch'] = data_loader._generate_batch(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['train']['batch_greedy'] = data_loader._generate_batch_greedy(X_train, Y_train, data_dict, data_dict['out_size'], batch_size)\n",
        "  \n",
        "  data_dict['val']['batch'] = data_loader._generate_batch(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "  data_dict['val']['batch_greedy'] = data_loader._generate_batch_greedy(X_val, Y_val, data_dict, data_dict['out_size'], batch_size)\n",
        "\n",
        "  data_dict['test']['batch'] = data_loader._generate_batch(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)  \n",
        "  data_dict['test']['batch_greedy'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], batch_size)    \n",
        "  data_dict['test']['batch_1'] = data_loader._generate_batch_greedy(X_test, Y_test, data_dict, data_dict['out_size'], 1)\n",
        "\n",
        "\n",
        "  data_dict['tokenizer'] = tk\n",
        "\n",
        "  return data_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJozmDWedfG"
      },
      "source": [
        "data_dict = return_data_dict(['ta'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zKjtjWJ_U9t",
        "outputId": "fadbdcda-4e63-42fb-ee56-167d3c793d31"
      },
      "source": [
        "pprint(data_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'in_size': 30,\n",
            " 'max_source_length': 30,\n",
            " 'max_target_length': 28,\n",
            " 'out_size': 50,\n",
            " 'test': {'batch': <generator object data_loader._generate_batch at 0x7fa9fc048150>,\n",
            "          'batch_1': <generator object data_loader._generate_batch_greedy at 0x7fa9fc048250>,\n",
            "          'batch_greedy': <generator object data_loader._generate_batch_greedy at 0x7fa9fc0481d0>,\n",
            "          'df':              output              input\n",
            "5219           புறா              puraa\n",
            "4030       தேர்வின்           thaervin\n",
            "346          அலகில்             alakil\n",
            "641   ஆலோசனைகளையும்  aaloasanaikalayum\n",
            "368          அழகினை           alaginai\n",
            "...             ...                ...\n",
            "5940         மேனியை           maeniyai\n",
            "5661   மற்றவர்களைத்  matrtravarkalaith\n",
            "5410   பொழுதுபோக்கு      poluthupookku\n",
            "4534        பகீரதன்        pakiirathan\n",
            "4880       பாதியில்          paathiyil\n",
            "\n",
            "[6833 rows x 2 columns],\n",
            "          'max_source_length': 23,\n",
            "          'max_target_length': 24},\n",
            " 'tokenizer': <__main__.Tokenizer object at 0x7fa9fdc31050>,\n",
            " 'train': {'batch': <generator object data_loader._generate_batch at 0x7fa9fcf7af50>,\n",
            "           'batch_greedy': <generator object data_loader._generate_batch_greedy at 0x7fa9fcf7ae50>,\n",
            "           'df':                output            input\n",
            "61168       வலயத்தில்      valayatthil\n",
            "9284             ஈரல்             iral\n",
            "8194         இருந்திட      irunththita\n",
            "32173          ஜோர்ஜ்            jorge\n",
            "43487          பகைவர்         pagaivar\n",
            "...               ...              ...\n",
            "7467         இயங்காது      iyangkaathu\n",
            "38947        தோன்றும்         thondrum\n",
            "59682          லூகாஸ்           luukas\n",
            "56215  முடிவெடுத்தார்  mudiveduththaar\n",
            "65205           வேடம்           vaetam\n",
            "\n",
            "[66286 rows x 2 columns],\n",
            "           'max_source_length': 30,\n",
            "           'max_target_length': 28},\n",
            " 'val': {'batch': <generator object data_loader._generate_batch at 0x7fa9fc048050>,\n",
            "         'batch_greedy': <generator object data_loader._generate_batch_greedy at 0x7fa9fc0480d0>,\n",
            "         'df':                output             input\n",
            "2655  சம்பிரதாயத்தில்  sambradhaayathil\n",
            "4329    நிர்வாகத்தைப்      nirvakathaip\n",
            "1645     கரும்புலிகள்      karumpuligal\n",
            "6668           வேலைத்      thittaththin\n",
            "5821      முற்பட்டனர்       murpattanar\n",
            "...               ...               ...\n",
            "2419  கோட்பாடுகளுக்கு  kootpaatukalukku\n",
            "124        அதிகமானோர்     athigamaanoor\n",
            "2501          கௌரவமான      kouravamaana\n",
            "1871          காரணமாக      kaaranamaaga\n",
            "2400      கொள்வதாகும்      kolvadhaagum\n",
            "\n",
            "[6806 rows x 2 columns],\n",
            "         'max_source_length': 23,\n",
            "         'max_target_length': 24}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_63arXtzSt"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQl8eh2y35p"
      },
      "source": [
        "# class BeamSearchCallBack(keras.callbacks.Callback):\n",
        "#   def __init__(self, details, test_data, tokenizer, out_size, num_val=200, beam=3) :\n",
        "#     self.details = details\n",
        "#     self.test_data = test_data\n",
        "#     self.tokenizer = tokenizer\n",
        "#     self.out_size = out_size\n",
        "#     self.num_val = num_val\n",
        "#     self.beam = beam\n",
        "\n",
        "#   def on_epoch_end(self, epoch, logs=None) :\n",
        "    \n",
        "#     # defining the encoder model\n",
        "#     encoder_model = Model(self.details['encoder_inputs'], self.details['encoder_states'])\n",
        "    \n",
        "#     # hidden representation size\n",
        "#     rep_size = self.details['params']['rep_size']\n",
        "\n",
        "#     # initializing decoder state input\n",
        "#     decoder_state_input = []\n",
        "\n",
        "\n",
        "#     for i in range(len(self.details['encoder_states'])) :\n",
        "#         new_state = Input(shape=(rep_size,))\n",
        "#         decoder_state_input.append(new_state)\n",
        "#     decoder_inputs = self.details['decoder_inputs']\n",
        "#     x = self.details['decoder_embedding'](decoder_inputs)\n",
        "    \n",
        "#     for layer in self.details['decoder_layers'] :\n",
        "#       x, *decoder_states = layer(x,initial_state=decoder_state_input)\n",
        "\n",
        "#     x = self.details['decoder_dense'](x)\n",
        "#     decoder_model = Model(\n",
        "#         [decoder_inputs] + decoder_state_input,\n",
        "#         [x] + decoder_states )\n",
        "#     inp = self.tokenizer.encode(self.test_data['df'].input.tolist())\n",
        "#     out = self.tokenizer.encode(self.test_data['df'].output.tolist(),mode='output')\n",
        "#     out_size = self.out_size\n",
        "#     val_gen = data_loader._generate_batch(inp,out,self.test_data,self.out_size)\n",
        "#     acc = 0\n",
        "#     for i in tqdm(range(self.num_val)) :\n",
        "#       (input_seq,ans) , _ = next(val_gen)\n",
        "#       _,best = decode_sequence_beam(input_seq,self.beam,encoder_model,decoder_model,self.tokenizer,self.test_data['max_target_length'])\n",
        "#       w1 = self.tokenizer.decode(best,mode='output')\n",
        "#       w2 = self.tokenizer.decode(ans,mode='output')\n",
        "#       comp = (w1==w2)\n",
        "#       if comp :\n",
        "#         acc += 1    \n",
        "\n",
        "#     acc /= len(inp)\n",
        "#     print(\"Val Accuracy : \"+str(acc))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cdgAqTLyAr"
      },
      "source": [
        "def decode_sequence_beam(input_seq, k, encoder_model, decoder_model, tk, max_target_length=20, getall=False):\n",
        "    # encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq,batch_size=1,use_multiprocessing=True)\n",
        "    # generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = 1 \n",
        "    run_condition = [True for i in range(k)]\n",
        "    # print(len(states_value))\n",
        "    # print([target_seq] + [states_value])\n",
        "    results, *states_values_temp = decoder_model.predict([target_seq] + [states_value])\n",
        "    output_tokens = results\n",
        "\n",
        "    states_values_k = [states_values_temp for i in range(k)]\n",
        "    #get topk indices\n",
        "    ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "    bestk_ind = ind\n",
        "    output_tokens = np.array(output_tokens[0, -1, :])\n",
        "    bestk_prob = output_tokens[ind]\n",
        "    bestk_tot = [[1,bestk_ind[i]] for i in range(k)]\n",
        "    # print(bestk_tot)\n",
        "\n",
        "    \n",
        "    while any(run_condition):\n",
        "        bestk_tot_new = []\n",
        "        bestk_prob_new = []\n",
        "        states_values_k_new = []\n",
        "        for i in range(k) :\n",
        "            if run_condition[i] :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                target_seq[0,0] = a[-1]\n",
        "                results,*states_values_temp = decoder_model.predict([target_seq] + states_values_k[i],batch_size=1)\n",
        "                output_tokens = results\n",
        "\n",
        "                states_values_k_temp = [states_values_temp for m in range(k)]\n",
        "\n",
        "                states_values_k_new += states_values_k_temp\n",
        "                ind = np.argpartition(np.array(output_tokens[0, -1, :]), -k)[-k:]\n",
        "                bestk_ind = ind\n",
        "                output_tokens = np.array(output_tokens[0, -1, :])\n",
        "                bestk_prob_temp = output_tokens[ind]\n",
        "                bestk_tot_temp = [a+[bestk_ind[j]] for j in range(k)]\n",
        "                bestk_prob_temp2 = [b*bestk_prob_temp[j] for j in range(k)]\n",
        "                bestk_prob_new += bestk_prob_temp2\n",
        "                bestk_tot_new += bestk_tot_temp\n",
        "            \n",
        "            else :\n",
        "                a = bestk_tot[i]\n",
        "                b = bestk_prob[i]\n",
        "                bestk_tot_new += [bestk_tot[i]]\n",
        "                bestk_prob_new += [b]\n",
        "                states_values_k_new += [states_values_k[i]]\n",
        "\n",
        "        bestk_prob_new = np.array(bestk_prob_new)\n",
        "        # print(len(bestk_prob_new),len(bestk_tot_new),len(states_values_k_new))\n",
        "        ind = np.argpartition(bestk_prob_new,-k)[-k:]\n",
        "        bestk_tot = [bestk_tot_new[i] for i in ind]\n",
        "        states_values_k = [states_values_k_new[i] for i in ind]\n",
        "        bestk_prob = bestk_prob_new[ind]\n",
        "        run_condition = []\n",
        "        for i in range(k) :\n",
        "            a = bestk_tot[i]\n",
        "            b = bestk_prob[i]\n",
        "            if a[-1]!= 2 and len(a)<=max_target_length :\n",
        "              run_condition.append(True)\n",
        "            else :\n",
        "              run_condition.append(False)\n",
        "\n",
        "        # print(bestk_tot)\n",
        "\n",
        "    final_words = []\n",
        "    best_word = []\n",
        "    best = 0.0\n",
        "    for i in range(k) :\n",
        "      a = bestk_tot[i]\n",
        "      b = bestk_prob[i]\n",
        "      final_words += [a]\n",
        "      if b > best :\n",
        "        best_word = [a]\n",
        "\n",
        "    if getall :\n",
        "      return (tk.decode(final_words,'output'),best_word)\n",
        "    else :\n",
        "      return final_words,best_word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJm8eZrt5mA"
      },
      "source": [
        "class rnn():\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \n",
        "    num_encode_layers = params['num_encode_layers']\n",
        "    num_decode_layers = params['num_decode_layers']\n",
        "    data_dict = params['data_dict']\n",
        "    in_size = params['data_dict']['in_size']\n",
        "    out_size = params['data_dict']['out_size']\n",
        "    cell_type = params['cell_type']\n",
        "    dropout = params['dropout']\n",
        "    embed_size = params['embed_size']\n",
        "    rep_size = params['rep_size']\n",
        "        \n",
        "    ###################### ENCODER NETWORK ######################\n",
        "    \n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    x = Embedding(in_size, embed_size ,mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    encoder_layers = []\n",
        "    \n",
        "    for j in range(num_encode_layers-1) :   \n",
        "      curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_sequences=True)\n",
        "      encoder_layers.append(curr_layer)\n",
        "      x = curr_layer(x)\n",
        "\n",
        "    curr_layer = getattr(layers, cell_type)(rep_size, dropout=dropout, return_state=True)\n",
        "    encoder_layers.append(curr_layer)\n",
        "    x, *encoder_states = curr_layer(x)\n",
        "\n",
        "    ###################### DECODER NETWORK ######################\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    decoder_embedding =  Embedding(out_size, embed_size, mask_zero=True)\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_layers = []    \n",
        "    \n",
        "    for j in range(num_decode_layers) :\n",
        "      curr_layer = getattr(layers, cell_type)(rep_size,dropout=dropout,return_state=True, return_sequences=True)\n",
        "      decoder_layers.append(curr_layer)\n",
        "      x, *decoder_states = curr_layer(x, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(units=out_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "    # define the model that will turn `encoder_inputs` & `decoder_inputs` into `decoder_outputs`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ????? \n",
        "    self.model = model\n",
        "    self.encoder_inputs = encoder_inputs\n",
        "    self.encoder_layers = encoder_layers\n",
        "    self.decoder_inputs = decoder_inputs\n",
        "    self.decoder_embedding = decoder_embedding\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.decoder_dense = decoder_dense\n",
        "    self.encoder_states = encoder_states\n",
        "    self.params = params\n",
        "    self.details = {\n",
        "        'model' : self.model,\n",
        "        'encoder_inputs' : self.encoder_inputs,\n",
        "        'encoder_layers' :self.encoder_layers ,\n",
        "        'decoder_inputs' :self.decoder_inputs ,\n",
        "        'decoder_embedding' : self.decoder_embedding,\n",
        "        'decoder_layers' : self.decoder_layers,\n",
        "        'decoder_dense' : self.decoder_dense,\n",
        "        'encoder_states' : self.encoder_states ,\n",
        "        'params' :self.params,\n",
        "    }\n",
        "\n",
        "  def compile_and_fit(self, data_dict, params):\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "    \n",
        "    # printing the summary of the model\n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples\n",
        "    val_samples = len(data_dict['val']['df'])    \n",
        "    \n",
        "    # batch size\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs']\n",
        "\n",
        "    # steps per epoch\n",
        "    steps_per_epoch = ceil(train_samples/batch_size) - 1\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch'],\n",
        "                                           steps_per_epoch = steps_per_epoch,\n",
        "                                           epochs=num_epochs,\n",
        "                                           # callbacks=[\n",
        "                                           # BeamSearchCallBack(self.details,data_dict['val'],data_dict['tokenizer'],data_dict['out_size'],num_val=num_val_samples,beam=beam),\n",
        "                                                        # wandb.keras.WandbCallback()\n",
        "                                                        # ],\n",
        "                                           validation_data = data_dict['val']['batch'], \n",
        "                                           validation_steps = val_samples//batch_size\n",
        "                                          )\n",
        "\n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmfE-kGOvjk8",
        "outputId": "fe6388bf-0a59-4dd1-9639-f9b15b5859c0"
      },
      "source": [
        "10/4"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0m1582SU6W8"
      },
      "source": [
        "class rnn_second() :\n",
        "  def __init__(self, details) :\n",
        "\n",
        "    # copying required details\n",
        "    self.details = details\n",
        "\n",
        "    # copying decoder state input\n",
        "    decoder_state_input = self.details['encoder_states']\n",
        "\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "\n",
        "    # copying hidden representation size\n",
        "    rep_size = self.details['params']['rep_size']\n",
        "\n",
        "    # copying decoder inputs\n",
        "    decoder_inputs = self.details['decoder_inputs']\n",
        "\n",
        "    # the decoder model\n",
        "    x = self.details['decoder_embedding'](decoder_inputs)\n",
        "  \n",
        "    all_outputs = []\n",
        "    for _ in range(self.details['params']['data_dict']['max_target_length']) :\n",
        "        for layer in self.details['decoder_layers'] :\n",
        "            x, *decoder_states = layer(x, initial_state=decoder_state_input)\n",
        "\n",
        "        x = self.details['decoder_dense'](x)\n",
        "\n",
        "        # appending the softmax output\n",
        "        all_outputs.append(x)\n",
        "\n",
        "        # taking the argmax to feed into the next time step\n",
        "        x = tf.math.argmax(x, 2)  \n",
        "        x = self.details['decoder_embedding'](x)\n",
        "        \n",
        "        # decoder state input for the next time step\n",
        "        decoder_state_input = decoder_states\n",
        "\n",
        "    ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "    # where do we evaluate stop condition?\n",
        "\n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    model = Model([self.details['encoder_inputs'], decoder_inputs], decoder_outputs)\n",
        "    self.model = model\n",
        "\n",
        "  def compile_and_fit(self,data_dict,params) :\n",
        "\n",
        "    # compiling the model\n",
        "    self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "     # printing the summary of the model   \n",
        "    summary = self.model.summary()\n",
        "\n",
        "    # plotting the model figure\n",
        "    plot = plot_model(self.model, show_shapes=True)\n",
        "    \n",
        "    # total training samples\n",
        "    train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "    # total validation samples \n",
        "    val_samples = len(data_dict['val']['df'])\n",
        "\n",
        "    # batch size   \n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    # number of epochs\n",
        "    num_epochs = params['num_epochs_2']\n",
        "\n",
        "    # training the model\n",
        "    run_details = self.model.fit_generator(generator = data_dict['train']['batch_greedy'],\n",
        "                                            steps_per_epoch = train_samples//batch_size,\n",
        "                                            epochs=num_epochs,\n",
        "                                            # callbacks=[\n",
        "                                                      # BeamSearchCallBack(self.details,data_dict['val'],data_dict['tokenizer'],data_dict['out_size'],num_val=num_val_samples,beam=beam),\n",
        "                                                      # wandb.keras.WandbCallback()\n",
        "                                                      # ],\n",
        "                                            validation_data = data_dict['val']['batch_greedy'],\n",
        "                                            validation_steps = val_samples//batch_size)\n",
        "   \n",
        "    return {\n",
        "        'run_details' : run_details\n",
        "    }\n",
        "\n",
        "  def evaluate(self, data_dict) :\n",
        "\n",
        "    # test batch generator\n",
        "    test_gen = data_dict['val']['batch_greedy']\n",
        "    \n",
        "    # number of test samples\n",
        "    test_samples = len(data_dict['val']['df'])\n",
        "  \n",
        "    batch_size=32\n",
        "\n",
        "    num_hits = 0\n",
        "    \n",
        "    for _ in range(test_samples//batch_size) :\n",
        "      (a,b),c = next(test_gen)\n",
        "      l1 = data_dict['tokenizer'].decode(np.argmax(c, axis=2), mode='output')\n",
        "      out = self.model.predict([a,b])\n",
        "      out = np.argmax(out,axis=2) \n",
        "      l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "      print(l1, l2)\n",
        "      num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "      break\n",
        "\n",
        "    print(\"Test Acc \", num_hits/test_samples)\n",
        "    wandb.log({\"Final Test Accuracy \":  num_hits/test_samples})"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5_HtEoSFcu"
      },
      "source": [
        "def evaluate(net, data_dict) :\n",
        "\n",
        "  # test batch generator\n",
        "  test_gen = data_dict['val']['batch_greedy']\n",
        "  \n",
        "  # number of test samples\n",
        "  test_samples = len(data_dict['val']['df'])\n",
        "\n",
        "  batch_size=32\n",
        "\n",
        "  num_hits = 0\n",
        "  \n",
        "  for _ in range(test_samples//batch_size) :\n",
        "    (a,b),c = next(test_gen)\n",
        "    l1 = data_dict['tokenizer'].decode(np.argmax(c, axis=2), mode='output')\n",
        "    out = net.model.predict([a,b])\n",
        "    out = np.argmax(out,axis=2) \n",
        "    l2 = data_dict['tokenizer'].decode(out, mode='output')\n",
        "    print(l1[1], l2[1])\n",
        "    num_hits += np.sum(np.array(l1)==np.array(l2))\n",
        "    break\n",
        "\n",
        "  print(\"Test Acc \", num_hits/test_samples)\n",
        "  wandb.log({\"Final Test Accuracy \":  num_hits/test_samples})"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDLMoTWfBn86"
      },
      "source": [
        "class tools:\n",
        "  def init_params(config,data_dict):\n",
        "  \n",
        "    # returning parameters\n",
        "    params = {\n",
        "        'num_encode_layers' : config.num_encode_layers,\n",
        "        'num_decode_layers' : config.num_decode_layers,\n",
        "        'cell_type' : config.cell_type,\n",
        "        'rep_size' : config.rep_size,\n",
        "        'embed_size' : config.embed_size,\n",
        "        'dropout' : config.dropout,\n",
        "        'num_epochs' : config.num_epochs,\n",
        "        'num_epochs_2' : config.num_epochs_2,\n",
        "        'data_dict' : data_dict,\n",
        "        'batch_size' : config.batch_size\n",
        "    }\n",
        "    return params"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AWE1lLSDDXh"
      },
      "source": [
        "# sweep configuration\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'val_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM', 'GRU', 'SimpleRNN']  \n",
        "        },\n",
        "        'embed_size': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'rep_size': {\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'num_epochs_2' : {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'num_encode_layers': {\n",
        "            'values': [1, 2, 4]\n",
        "        },\n",
        "        'num_decode_layers': {\n",
        "            'values': [1, 2, 4]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeqg-Otpnbo"
      },
      "source": [
        "# # sweep configuration\n",
        "# sweep_config = {\n",
        "#     'method' : 'bayes',\n",
        "#     'metric' : {\n",
        "#         'name' : 'val_acc',\n",
        "#         'goal' : 'maximize'\n",
        "#     },\n",
        "#     'parameters': {\n",
        "#         'cell_type' : {\n",
        "#             'values': ['LSTM']  \n",
        "#         },\n",
        "#         'embed_size': {\n",
        "#             'values': [10]\n",
        "#         },\n",
        "#         'rep_size': {\n",
        "#             'values': [32]\n",
        "#         },\n",
        "#         'dropout': {\n",
        "#             'values': [0]\n",
        "#         },\n",
        "#         'batch_size': {\n",
        "#             'values': [32]\n",
        "#         },\n",
        "#         'num_epochs': {\n",
        "#             'values': [1]\n",
        "#         },\n",
        "#         'num_epochs_2' : {\n",
        "#             'values': [1]\n",
        "#         },\n",
        "#         'num_encode_layers': {\n",
        "#             'values': [1]\n",
        "#         },\n",
        "#         'num_decode_layers': {\n",
        "#             'values': [1]\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqDamLUoDQWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaad00ef-a4d2-4349-8ec7-10d70c2d8271"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='dakshina_v2')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: v6gbveds\n",
            "Sweep URL: https://wandb.ai/ramkamal/dakshina_v2/sweeps/v6gbveds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez3f-pjxDTU8"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      # copying the config \n",
        "      config = wandb.config\n",
        " \n",
        "      # naming the run\n",
        "      # wandb.run.name = 'fil:'+str(config['num_filters_'])+'_type:'+config['type_of_filters'][0]+'_aug:'+str(config['augmentation'])[0]+'_dro:'+str(config['dropout'])[0]\n",
        "      wandb.run.name = 'typ:'+config['cell_type'][:4]+ '_' + 'emb:'+str(config['embed_size'])+ '_' + 'enc:' + str(config['num_encode_layers'])+ '_' + 'dec:'+str(config['num_decode_layers'])\n",
        "      \n",
        "      # returning the data dictionairy\n",
        "      ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "      data_dict = return_data_dict(batch_size=config.batch_size)\n",
        "\n",
        "      # copying the parameters\n",
        "      params = tools.init_params(config,data_dict)\n",
        "\n",
        "      # creating and training the first model\n",
        "      network = rnn(params)\n",
        "      run_details = network.compile_and_fit(data_dict, params)\n",
        "\n",
        "      # creating and training/ fine-tuning the second model\n",
        "      rnn_2 = rnn_second(network.details)\n",
        "      run_details_2 = rnn_2.compile_and_fit(data_dict,params)\n",
        "      \n",
        "      ##### ????? ????? ????? ????? ????? ????? ????? ????? ????? ?????\n",
        "      rnn_2.evaluate(data_dict)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QBd1gJDV-R"
      },
      "source": [
        "# sweep_id = '7g0porer'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCq49t4zDZod"
      },
      "source": [
        "# performing the sweep\n",
        "# wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBotvdIuDczo"
      },
      "source": [
        "# Run One Model separate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfaOoDTt-Zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c34155f0-f8a9-40b2-d467-6aa87c1e7d2a"
      },
      "source": [
        "params = {\n",
        "    'num_encode_layers' : 2,\n",
        "    'num_decode_layers' : 2,\n",
        "    'cell_type' : 'LSTM', \n",
        "    'rep_size' : 256,\n",
        "    'embed_size' : 10,\n",
        "    'dropout' : 0,\n",
        "    'num_epochs' : 25,\n",
        "    'num_epochs_2' : 1,\n",
        "    'data_dict' : data_dict,\n",
        "    'batch_size' : 32\n",
        "}\n",
        "\n",
        "network = rnn(params)\n",
        "plot_model(network.model, show_shapes=True)\n",
        "network.compile_and_fit(data_dict, params)\n",
        "rnn_2 = rnn_second(network.details)\n",
        "run_details_2 = rnn_2.compile_and_fit(data_dict, params)\n",
        "rnn_2.evaluate(data_dict)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 256)    273408      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     500         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 525312      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  273408      embedding_1[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  525312      lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 50)     12850       lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,611,090\n",
            "Trainable params: 1,611,090\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2071/2071 [==============================] - 88s 23ms/step - loss: 0.7727 - acc: 0.3733 - val_loss: 0.3076 - val_acc: 0.7314\n",
            "Epoch 2/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.2336 - acc: 0.8136 - val_loss: 0.1619 - val_acc: 0.8657\n",
            "Epoch 3/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.1085 - acc: 0.9168 - val_loss: 0.1378 - val_acc: 0.8865\n",
            "Epoch 4/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0680 - acc: 0.9483 - val_loss: 0.1358 - val_acc: 0.8904\n",
            "Epoch 5/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0464 - acc: 0.9647 - val_loss: 0.1393 - val_acc: 0.8924\n",
            "Epoch 6/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0334 - acc: 0.9744 - val_loss: 0.1464 - val_acc: 0.8921\n",
            "Epoch 7/25\n",
            "2071/2071 [==============================] - 44s 21ms/step - loss: 0.0248 - acc: 0.9809 - val_loss: 0.1545 - val_acc: 0.8919\n",
            "Epoch 8/25\n",
            "2071/2071 [==============================] - 44s 21ms/step - loss: 0.0196 - acc: 0.9847 - val_loss: 0.1619 - val_acc: 0.8919\n",
            "Epoch 9/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0162 - acc: 0.9870 - val_loss: 0.1679 - val_acc: 0.8911\n",
            "Epoch 10/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0135 - acc: 0.9888 - val_loss: 0.1744 - val_acc: 0.8916\n",
            "Epoch 11/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0119 - acc: 0.9901 - val_loss: 0.1833 - val_acc: 0.8914\n",
            "Epoch 12/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0103 - acc: 0.9916 - val_loss: 0.1890 - val_acc: 0.8900\n",
            "Epoch 13/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0100 - acc: 0.9914 - val_loss: 0.1972 - val_acc: 0.8894\n",
            "Epoch 14/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0089 - acc: 0.9924 - val_loss: 0.1942 - val_acc: 0.8930\n",
            "Epoch 15/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0082 - acc: 0.9931 - val_loss: 0.2007 - val_acc: 0.8917\n",
            "Epoch 16/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0080 - acc: 0.9931 - val_loss: 0.2060 - val_acc: 0.8888\n",
            "Epoch 17/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0077 - acc: 0.9933 - val_loss: 0.2005 - val_acc: 0.8934\n",
            "Epoch 18/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0068 - acc: 0.9941 - val_loss: 0.2086 - val_acc: 0.8916\n",
            "Epoch 19/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0070 - acc: 0.9938 - val_loss: 0.2103 - val_acc: 0.8904\n",
            "Epoch 20/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0071 - acc: 0.9938 - val_loss: 0.2144 - val_acc: 0.8912\n",
            "Epoch 21/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0065 - acc: 0.9944 - val_loss: 0.2195 - val_acc: 0.8910\n",
            "Epoch 22/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0065 - acc: 0.9943 - val_loss: 0.2178 - val_acc: 0.8918\n",
            "Epoch 23/25\n",
            "2071/2071 [==============================] - 44s 21ms/step - loss: 0.0068 - acc: 0.9941 - val_loss: 0.2215 - val_acc: 0.8897\n",
            "Epoch 24/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0063 - acc: 0.9945 - val_loss: 0.2305 - val_acc: 0.8887\n",
            "Epoch 25/25\n",
            "2071/2071 [==============================] - 43s 21ms/step - loss: 0.0063 - acc: 0.9944 - val_loss: 0.2237 - val_acc: 0.8914\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 10)     300         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 256)    273408      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 10)     500         input_2[0][0]                    \n",
            "                                                                 tf.math.argmax[0][0]             \n",
            "                                                                 tf.math.argmax_1[0][0]           \n",
            "                                                                 tf.math.argmax_2[0][0]           \n",
            "                                                                 tf.math.argmax_3[0][0]           \n",
            "                                                                 tf.math.argmax_4[0][0]           \n",
            "                                                                 tf.math.argmax_5[0][0]           \n",
            "                                                                 tf.math.argmax_6[0][0]           \n",
            "                                                                 tf.math.argmax_7[0][0]           \n",
            "                                                                 tf.math.argmax_8[0][0]           \n",
            "                                                                 tf.math.argmax_9[0][0]           \n",
            "                                                                 tf.math.argmax_10[0][0]          \n",
            "                                                                 tf.math.argmax_11[0][0]          \n",
            "                                                                 tf.math.argmax_12[0][0]          \n",
            "                                                                 tf.math.argmax_13[0][0]          \n",
            "                                                                 tf.math.argmax_14[0][0]          \n",
            "                                                                 tf.math.argmax_15[0][0]          \n",
            "                                                                 tf.math.argmax_16[0][0]          \n",
            "                                                                 tf.math.argmax_17[0][0]          \n",
            "                                                                 tf.math.argmax_18[0][0]          \n",
            "                                                                 tf.math.argmax_19[0][0]          \n",
            "                                                                 tf.math.argmax_20[0][0]          \n",
            "                                                                 tf.math.argmax_21[0][0]          \n",
            "                                                                 tf.math.argmax_22[0][0]          \n",
            "                                                                 tf.math.argmax_23[0][0]          \n",
            "                                                                 tf.math.argmax_24[0][0]          \n",
            "                                                                 tf.math.argmax_25[0][0]          \n",
            "                                                                 tf.math.argmax_26[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 525312      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  273408      embedding_1[1][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 lstm_3[1][1]                     \n",
            "                                                                 lstm_3[1][2]                     \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 lstm_3[2][1]                     \n",
            "                                                                 lstm_3[2][2]                     \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 lstm_3[3][1]                     \n",
            "                                                                 lstm_3[3][2]                     \n",
            "                                                                 embedding_1[5][0]                \n",
            "                                                                 lstm_3[4][1]                     \n",
            "                                                                 lstm_3[4][2]                     \n",
            "                                                                 embedding_1[6][0]                \n",
            "                                                                 lstm_3[5][1]                     \n",
            "                                                                 lstm_3[5][2]                     \n",
            "                                                                 embedding_1[7][0]                \n",
            "                                                                 lstm_3[6][1]                     \n",
            "                                                                 lstm_3[6][2]                     \n",
            "                                                                 embedding_1[8][0]                \n",
            "                                                                 lstm_3[7][1]                     \n",
            "                                                                 lstm_3[7][2]                     \n",
            "                                                                 embedding_1[9][0]                \n",
            "                                                                 lstm_3[8][1]                     \n",
            "                                                                 lstm_3[8][2]                     \n",
            "                                                                 embedding_1[10][0]               \n",
            "                                                                 lstm_3[9][1]                     \n",
            "                                                                 lstm_3[9][2]                     \n",
            "                                                                 embedding_1[11][0]               \n",
            "                                                                 lstm_3[10][1]                    \n",
            "                                                                 lstm_3[10][2]                    \n",
            "                                                                 embedding_1[12][0]               \n",
            "                                                                 lstm_3[11][1]                    \n",
            "                                                                 lstm_3[11][2]                    \n",
            "                                                                 embedding_1[13][0]               \n",
            "                                                                 lstm_3[12][1]                    \n",
            "                                                                 lstm_3[12][2]                    \n",
            "                                                                 embedding_1[14][0]               \n",
            "                                                                 lstm_3[13][1]                    \n",
            "                                                                 lstm_3[13][2]                    \n",
            "                                                                 embedding_1[15][0]               \n",
            "                                                                 lstm_3[14][1]                    \n",
            "                                                                 lstm_3[14][2]                    \n",
            "                                                                 embedding_1[16][0]               \n",
            "                                                                 lstm_3[15][1]                    \n",
            "                                                                 lstm_3[15][2]                    \n",
            "                                                                 embedding_1[17][0]               \n",
            "                                                                 lstm_3[16][1]                    \n",
            "                                                                 lstm_3[16][2]                    \n",
            "                                                                 embedding_1[18][0]               \n",
            "                                                                 lstm_3[17][1]                    \n",
            "                                                                 lstm_3[17][2]                    \n",
            "                                                                 embedding_1[19][0]               \n",
            "                                                                 lstm_3[18][1]                    \n",
            "                                                                 lstm_3[18][2]                    \n",
            "                                                                 embedding_1[20][0]               \n",
            "                                                                 lstm_3[19][1]                    \n",
            "                                                                 lstm_3[19][2]                    \n",
            "                                                                 embedding_1[21][0]               \n",
            "                                                                 lstm_3[20][1]                    \n",
            "                                                                 lstm_3[20][2]                    \n",
            "                                                                 embedding_1[22][0]               \n",
            "                                                                 lstm_3[21][1]                    \n",
            "                                                                 lstm_3[21][2]                    \n",
            "                                                                 embedding_1[23][0]               \n",
            "                                                                 lstm_3[22][1]                    \n",
            "                                                                 lstm_3[22][2]                    \n",
            "                                                                 embedding_1[24][0]               \n",
            "                                                                 lstm_3[23][1]                    \n",
            "                                                                 lstm_3[23][2]                    \n",
            "                                                                 embedding_1[25][0]               \n",
            "                                                                 lstm_3[24][1]                    \n",
            "                                                                 lstm_3[24][2]                    \n",
            "                                                                 embedding_1[26][0]               \n",
            "                                                                 lstm_3[25][1]                    \n",
            "                                                                 lstm_3[25][2]                    \n",
            "                                                                 embedding_1[27][0]               \n",
            "                                                                 lstm_3[26][1]                    \n",
            "                                                                 lstm_3[26][2]                    \n",
            "                                                                 embedding_1[28][0]               \n",
            "                                                                 lstm_3[27][1]                    \n",
            "                                                                 lstm_3[27][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  525312      lstm_2[1][0]                     \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 lstm_2[2][0]                     \n",
            "                                                                 lstm_3[1][1]                     \n",
            "                                                                 lstm_3[1][2]                     \n",
            "                                                                 lstm_2[3][0]                     \n",
            "                                                                 lstm_3[2][1]                     \n",
            "                                                                 lstm_3[2][2]                     \n",
            "                                                                 lstm_2[4][0]                     \n",
            "                                                                 lstm_3[3][1]                     \n",
            "                                                                 lstm_3[3][2]                     \n",
            "                                                                 lstm_2[5][0]                     \n",
            "                                                                 lstm_3[4][1]                     \n",
            "                                                                 lstm_3[4][2]                     \n",
            "                                                                 lstm_2[6][0]                     \n",
            "                                                                 lstm_3[5][1]                     \n",
            "                                                                 lstm_3[5][2]                     \n",
            "                                                                 lstm_2[7][0]                     \n",
            "                                                                 lstm_3[6][1]                     \n",
            "                                                                 lstm_3[6][2]                     \n",
            "                                                                 lstm_2[8][0]                     \n",
            "                                                                 lstm_3[7][1]                     \n",
            "                                                                 lstm_3[7][2]                     \n",
            "                                                                 lstm_2[9][0]                     \n",
            "                                                                 lstm_3[8][1]                     \n",
            "                                                                 lstm_3[8][2]                     \n",
            "                                                                 lstm_2[10][0]                    \n",
            "                                                                 lstm_3[9][1]                     \n",
            "                                                                 lstm_3[9][2]                     \n",
            "                                                                 lstm_2[11][0]                    \n",
            "                                                                 lstm_3[10][1]                    \n",
            "                                                                 lstm_3[10][2]                    \n",
            "                                                                 lstm_2[12][0]                    \n",
            "                                                                 lstm_3[11][1]                    \n",
            "                                                                 lstm_3[11][2]                    \n",
            "                                                                 lstm_2[13][0]                    \n",
            "                                                                 lstm_3[12][1]                    \n",
            "                                                                 lstm_3[12][2]                    \n",
            "                                                                 lstm_2[14][0]                    \n",
            "                                                                 lstm_3[13][1]                    \n",
            "                                                                 lstm_3[13][2]                    \n",
            "                                                                 lstm_2[15][0]                    \n",
            "                                                                 lstm_3[14][1]                    \n",
            "                                                                 lstm_3[14][2]                    \n",
            "                                                                 lstm_2[16][0]                    \n",
            "                                                                 lstm_3[15][1]                    \n",
            "                                                                 lstm_3[15][2]                    \n",
            "                                                                 lstm_2[17][0]                    \n",
            "                                                                 lstm_3[16][1]                    \n",
            "                                                                 lstm_3[16][2]                    \n",
            "                                                                 lstm_2[18][0]                    \n",
            "                                                                 lstm_3[17][1]                    \n",
            "                                                                 lstm_3[17][2]                    \n",
            "                                                                 lstm_2[19][0]                    \n",
            "                                                                 lstm_3[18][1]                    \n",
            "                                                                 lstm_3[18][2]                    \n",
            "                                                                 lstm_2[20][0]                    \n",
            "                                                                 lstm_3[19][1]                    \n",
            "                                                                 lstm_3[19][2]                    \n",
            "                                                                 lstm_2[21][0]                    \n",
            "                                                                 lstm_3[20][1]                    \n",
            "                                                                 lstm_3[20][2]                    \n",
            "                                                                 lstm_2[22][0]                    \n",
            "                                                                 lstm_3[21][1]                    \n",
            "                                                                 lstm_3[21][2]                    \n",
            "                                                                 lstm_2[23][0]                    \n",
            "                                                                 lstm_3[22][1]                    \n",
            "                                                                 lstm_3[22][2]                    \n",
            "                                                                 lstm_2[24][0]                    \n",
            "                                                                 lstm_3[23][1]                    \n",
            "                                                                 lstm_3[23][2]                    \n",
            "                                                                 lstm_2[25][0]                    \n",
            "                                                                 lstm_3[24][1]                    \n",
            "                                                                 lstm_3[24][2]                    \n",
            "                                                                 lstm_2[26][0]                    \n",
            "                                                                 lstm_3[25][1]                    \n",
            "                                                                 lstm_3[25][2]                    \n",
            "                                                                 lstm_2[27][0]                    \n",
            "                                                                 lstm_3[26][1]                    \n",
            "                                                                 lstm_3[26][2]                    \n",
            "                                                                 lstm_2[28][0]                    \n",
            "                                                                 lstm_3[27][1]                    \n",
            "                                                                 lstm_3[27][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 50)     12850       lstm_3[1][0]                     \n",
            "                                                                 lstm_3[2][0]                     \n",
            "                                                                 lstm_3[3][0]                     \n",
            "                                                                 lstm_3[4][0]                     \n",
            "                                                                 lstm_3[5][0]                     \n",
            "                                                                 lstm_3[6][0]                     \n",
            "                                                                 lstm_3[7][0]                     \n",
            "                                                                 lstm_3[8][0]                     \n",
            "                                                                 lstm_3[9][0]                     \n",
            "                                                                 lstm_3[10][0]                    \n",
            "                                                                 lstm_3[11][0]                    \n",
            "                                                                 lstm_3[12][0]                    \n",
            "                                                                 lstm_3[13][0]                    \n",
            "                                                                 lstm_3[14][0]                    \n",
            "                                                                 lstm_3[15][0]                    \n",
            "                                                                 lstm_3[16][0]                    \n",
            "                                                                 lstm_3[17][0]                    \n",
            "                                                                 lstm_3[18][0]                    \n",
            "                                                                 lstm_3[19][0]                    \n",
            "                                                                 lstm_3[20][0]                    \n",
            "                                                                 lstm_3[21][0]                    \n",
            "                                                                 lstm_3[22][0]                    \n",
            "                                                                 lstm_3[23][0]                    \n",
            "                                                                 lstm_3[24][0]                    \n",
            "                                                                 lstm_3[25][0]                    \n",
            "                                                                 lstm_3[26][0]                    \n",
            "                                                                 lstm_3[27][0]                    \n",
            "                                                                 lstm_3[28][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax (TFOpLambda)     (None, None)         0           dense[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_1 (TFOpLambda)   (None, None)         0           dense[2][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_2 (TFOpLambda)   (None, None)         0           dense[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_3 (TFOpLambda)   (None, None)         0           dense[4][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_4 (TFOpLambda)   (None, None)         0           dense[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_5 (TFOpLambda)   (None, None)         0           dense[6][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_6 (TFOpLambda)   (None, None)         0           dense[7][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_7 (TFOpLambda)   (None, None)         0           dense[8][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_8 (TFOpLambda)   (None, None)         0           dense[9][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_9 (TFOpLambda)   (None, None)         0           dense[10][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_10 (TFOpLambda)  (None, None)         0           dense[11][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_11 (TFOpLambda)  (None, None)         0           dense[12][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_12 (TFOpLambda)  (None, None)         0           dense[13][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_13 (TFOpLambda)  (None, None)         0           dense[14][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_14 (TFOpLambda)  (None, None)         0           dense[15][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_15 (TFOpLambda)  (None, None)         0           dense[16][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_16 (TFOpLambda)  (None, None)         0           dense[17][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_17 (TFOpLambda)  (None, None)         0           dense[18][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_18 (TFOpLambda)  (None, None)         0           dense[19][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_19 (TFOpLambda)  (None, None)         0           dense[20][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_20 (TFOpLambda)  (None, None)         0           dense[21][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_21 (TFOpLambda)  (None, None)         0           dense[22][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_22 (TFOpLambda)  (None, None)         0           dense[23][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_23 (TFOpLambda)  (None, None)         0           dense[24][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_24 (TFOpLambda)  (None, None)         0           dense[25][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_25 (TFOpLambda)  (None, None)         0           dense[26][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmax_26 (TFOpLambda)  (None, None)         0           dense[27][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 50)     0           dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "                                                                 dense[6][0]                      \n",
            "                                                                 dense[7][0]                      \n",
            "                                                                 dense[8][0]                      \n",
            "                                                                 dense[9][0]                      \n",
            "                                                                 dense[10][0]                     \n",
            "                                                                 dense[11][0]                     \n",
            "                                                                 dense[12][0]                     \n",
            "                                                                 dense[13][0]                     \n",
            "                                                                 dense[14][0]                     \n",
            "                                                                 dense[15][0]                     \n",
            "                                                                 dense[16][0]                     \n",
            "                                                                 dense[17][0]                     \n",
            "                                                                 dense[18][0]                     \n",
            "                                                                 dense[19][0]                     \n",
            "                                                                 dense[20][0]                     \n",
            "                                                                 dense[21][0]                     \n",
            "                                                                 dense[22][0]                     \n",
            "                                                                 dense[23][0]                     \n",
            "                                                                 dense[24][0]                     \n",
            "                                                                 dense[25][0]                     \n",
            "                                                                 dense[26][0]                     \n",
            "                                                                 dense[27][0]                     \n",
            "                                                                 dense[28][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,611,090\n",
            "Trainable params: 1,611,090\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "2071/2071 [==============================] - 623s 223ms/step - loss: 0.5329 - acc: 0.2266 - val_loss: 0.3595 - val_acc: 0.2450\n",
            "Test Acc  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-31f1f0e07741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrnn_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_second\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mrun_details_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrnn_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-592c7078d648>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Acc \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hits\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Final Test Accuracy \"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mnum_hits\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPreInitCallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: N802\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must call wandb.init() before {}()\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89DZv8QtQrc-",
        "outputId": "60de8ea5-96d5-4ad5-e22f-5d0644e4049c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "evaluate(rnn_2, data_dict)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ர்வத்துடன்<EOW> ர்வத்துனன்<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\n",
            "Test Acc  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f04cdf36a847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-e36459238d1c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(net, data_dict)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Acc \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hits\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Final Test Accuracy \"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mnum_hits\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPreInitCallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: N802\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must call wandb.init() before {}()\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYA7_UMavifg"
      },
      "source": [
        "rnn_2.evaluate(data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utq4Wtvg85ms"
      },
      "source": [
        "pprint(network.details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMHkTtHSTBmD"
      },
      "source": [
        "plot_model(rnn_2.model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YHfKL5_YTKk"
      },
      "source": [
        "# !pip install editdistance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtqXB5sXLLYV"
      },
      "source": [
        "test_gen = data_dict['test']['batch_greedy']\n",
        "test_samples = len(data_dict['test']['df'])\n",
        "batch_size=32\n",
        "acc = 0\n",
        "for _ in range(test_samples//batch_size) :\n",
        "  (a,b),c = next(test_gen)\n",
        "  l1 = data_dict['tokenizer'].decode(np.argmax(c,axis=2),mode='output')\n",
        "  out = rnn_2.model.predict([a,b])\n",
        "  out = np.argmax(out,axis=2)\n",
        "  l2 = data_dict['tokenizer'].decode(out,mode='output')\n",
        "  acc += np.sum(np.array(l1)==np.array(l2))\n",
        "\n",
        "print(\"Val Accuracy : \",acc/test_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yd31nMSSdxh"
      },
      "source": [
        "l1 = ['andiranchi', 'ake', 'irangula', 'ndha', 'ulakhatee', 'alal', 'urvajanche', 'urunath', 'amjhunga', 'maging', 'tutya', 'stitvachi', 'nudit', 'addhtipeksha', 'aam', 'uloos', 'eneth', 'eevandan', 'ndreas', 'ugnata', 'alvon', 'stitvatil', 'anchyabaddal', 'otipeksha', 'umak', 'haktte', 'ubhati', 'hhavi', 'ang', 'azipur', 'illi', 'hodi']\n",
        "l2 = ['andyyaachi', 'ake', 'irnngda', 'ndha', 'ulakatii', 'alalt', 'urvajaache', 'urunach', 'amaaaaagt', 'minangt', 'tutya', 'sttyaache', 'nduit', 'adhititaassh', 'amttt', 'ulustt', 'anich', 'ivanaaa', 'ndyiiist', 'uganta', 'alvoo', 'stttaaaii', 'anthaaaaaadd', 'otipissha', 'umak', 'hakttt', 'ubhate', 'hhali', 'ang', 'ajiprrt', 'hlll', 'hodit']\n",
        "np.sum(np.array(l1)==np.array(l2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzbbHG-33UB"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "num_encoder_tokens = data_dict['in_size']\n",
        "latent_dim = 256\n",
        "num_decoder_tokens = data_dict['out_size']\n",
        "batch_size=32\n",
        "epochs=20\n",
        "val_samples = 4981\n",
        "train_samples = 45444\n",
        "\n",
        "# The first part is unchanged\n",
        "encoder_inputs1 = Input(shape=(None,))\n",
        "encoder_inputs=Embedding(data_dict['in_size'], 64,mask_zero=True)(encoder_inputs1)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, which will only process one timestep at a time.\n",
        "decoder_inputs1 = Input(shape=(1,))\n",
        "decoder_embedding =  Embedding(data_dict['out_size'], 64,mask_zero=True)\n",
        "decoder_inputs = decoder_embedding(decoder_inputs1)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "all_outputs = []\n",
        "inputs = decoder_inputs\n",
        "for _ in range(23):\n",
        "    # Run the decoder on one timestep\n",
        "    outputs, state_h, state_c = decoder_lstm(inputs,\n",
        "                                             initial_state=states)\n",
        "    outputs = decoder_dense(outputs)\n",
        "    # Store the current prediction (we will concatenate all predictions later)\n",
        "    all_outputs.append(outputs)\n",
        "    # Reinject the outputs as inputs for the next loop iteration\n",
        "    # as well as update the states\n",
        "    outputs1 = tf.math.argmax(outputs,2)\n",
        "    inputs = decoder_embedding(outputs1)\n",
        "    states = [state_h, state_c]\n",
        "\n",
        "# Concatenate all predictions\n",
        "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "\n",
        "# Define and compile model as previously\n",
        "model = Model([encoder_inputs1, decoder_inputs1], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Prepare decoder input data that just contains the start character\n",
        "# Note that we could have made it a constant hard-coded in the model\n",
        "\n",
        "# # Train model as previously\n",
        "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_split=0.2)\n",
        "\n",
        "model.fit_generator(generator = data_dict['train']['batch_greedy'],\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = data_dict['val']['batch_greedy'],\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjtrtMiHDbiy"
      },
      "source": [
        "generator=data_dict['test']['batch_greedy']\n",
        "(a,b),c = next(generator)\n",
        "print(a,b,np.argmax(c,axis=2))\n",
        "out = model.predict([a,b])\n",
        "out = np.argmax(out,axis=2)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7o9Y_NBzelI"
      },
      "source": [
        "plot_model(model,to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqiICTfUid26"
      },
      "source": [
        "# Attention?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glswCuYJlume"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS-7MTO4lyP7"
      },
      "source": [
        "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.python.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NWc0APTli6i"
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        print(input_shape[0][2],input_shape[1][2])\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7drOpPqmltW3"
      },
      "source": [
        "def define_nmt(hidden_size, batch_size, max_source_length, source_vsize, max_target_length, target_vsize):\n",
        "\n",
        "    print(\"max_source_length, source_vsize, max_target_length, target_vsize\")\n",
        "    print(max_source_length, source_vsize, max_target_length, target_vsize)\n",
        "    # Define an input sequence and process it\n",
        "    if batch_size:\n",
        "        encoder_inputs = Input(batch_shape=(batch_size, max_source_length, source_vsize))\n",
        "        decoder_inputs = Input(batch_shape=(batch_size, max_target_length - 1, target_vsize))\n",
        "    else:\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    # Encoder GRU\n",
        "    encoder_emb = Embedding(data_dict['in_size'], source_vsize, input_length=max_source_length)\n",
        "    temp1 = encoder_emb(encoder_inputs)\n",
        "    encoder_gru = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "    encoder_out, *encoder_state = encoder_gru(temp1)\n",
        "    \n",
        "    # Set up the decoder GRU, using `encoder_states` as initial state\n",
        "    decoder_emb = Embedding(data_dict['out_size'], target_vsize, input_length=max_target_length)\n",
        "    temp2 = decoder_emb(decoder_inputs)\n",
        "    decoder_gru = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "    decoder_out, *decoder_state = decoder_gru(temp2, initial_state=encoder_state)\n",
        "\n",
        "    # Attention layer\n",
        "    attn_layer = AttentionLayer()\n",
        "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "    # Concat attention input and decoder GRU output\n",
        "    decoder_concat_input = Concatenate(axis=-1)([decoder_out, attn_out])\n",
        "\n",
        "    # Dense layer\n",
        "    dense = Dense(target_vsize, activation='softmax')\n",
        "    dense_time = TimeDistributed(dense)\n",
        "    decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "    # Full model\n",
        "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "    full_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "    full_model.summary()\n",
        "\n",
        "    \"\"\" Inference model \"\"\"\n",
        "    batch_size = 1\n",
        "\n",
        "    \"\"\" Encoder (Inference) model \"\"\"\n",
        "    encoder_inf_inputs = Input(shape=(max_source_length,))\n",
        "    temp3 = encoder_emb(encoder_inf_inputs)\n",
        "    encoder_inf_out, *encoder_inf_state = encoder_gru(temp3)\n",
        "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "    \"\"\" Decoder (Inference) model \"\"\"\n",
        "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1))\n",
        "    encoder_inf_states = Input(batch_shape=(batch_size, None, hidden_size))\n",
        "    decoder_init_stateh = Input(batch_shape=(batch_size, hidden_size))\n",
        "    decoder_init_statec = Input(batch_shape=(batch_size, hidden_size))\n",
        "    decoder_init_state = [decoder_init_stateh,decoder_init_statec]\n",
        "    temp = decoder_emb(decoder_inf_inputs)\n",
        "    decoder_inf_out, *decoder_inf_state = decoder_gru(temp, initial_state=decoder_init_state)\n",
        "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "    decoder_inf_concat = Concatenate(axis=-1)([decoder_inf_out, attn_inf_out])\n",
        "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
        "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "\n",
        "    return full_model, encoder_model, decoder_model\n",
        "    # return full_model\n",
        "\n",
        "\n",
        "m1, m2, m3 = define_nmt(256, None, None, 40, None, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJlOmqBwpWAS"
      },
      "source": [
        "plot_model(m1, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0QSXRudzevC"
      },
      "source": [
        "batch_size=32\n",
        "epochs=10\n",
        "val_samples = len(data_dict['val']['df'])\n",
        "train_samples = len(data_dict['train']['df'])\n",
        "\n",
        "m1.fit_generator(generator = data_dict['train']['batch'],\n",
        "                 steps_per_epoch=train_samples//batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=data_dict['val']['batch'],\n",
        "                 validation_steps=val_samples//batch_size\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "namCfz0sFqRx"
      },
      "source": [
        "acc = 0\n",
        "generator = data_dict['test']['batch_1']\n",
        "tk = data_dict['tokenizer']\n",
        "for j in tqdm(range(4967)) :\n",
        "  (a,b),c = next(generator)\n",
        "  g = [[1] + list(np.argmax(c,axis=-1)[0])]\n",
        "  enc_outs, enc_last_state = m2.predict(a)\n",
        "  dec_state = enc_last_state\n",
        "  attention_weights = []\n",
        "  word = [1]\n",
        "  for i in range(23):\n",
        "\n",
        "      dec_out, attention, dec_state = m3.predict([enc_outs, dec_state, b])\n",
        "      dec_ind = np.argmax(dec_out, axis=-1)\n",
        "\n",
        "      word.append(dec_ind[0][0])\n",
        "      b = dec_ind\n",
        "      attention_weights.append((dec_ind, attention))\n",
        "      if dec_ind[0][0] == 2 :\n",
        "        break\n",
        "  str1 = tk.decode(g,mode='output')\n",
        "  str2 = tk.decode([word],mode='output')\n",
        "  if str1 == str2 :\n",
        "    acc += 1\n",
        "\n",
        "print(acc/4967)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItqoHkE2JTO1"
      },
      "source": [
        "# Romanized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8dFDOHGbEb"
      },
      "source": [
        "ta_rom = dict()\n",
        "ta_rom['rejoined'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['rejoined_aligned_cased'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv', sep='\\t', header=None, error_bad_lines=False) \n",
        "ta_rom['rejoined_aligned'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv', sep='\\t', header=None, error_bad_lines=False)\n",
        "ta_rom['split'] = pd.read_csv('/content/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv', sep='\\t', header=None, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-WGvG_RJqsr"
      },
      "source": [
        "list(ta_rom['rejoined'].iloc[0, 0])[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75rYpZkNCJV"
      },
      "source": [
        "ta_rom['rejoined_aligned_cased']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E9hQuQMULO"
      },
      "source": [
        "ta_rom['rejoined_aligned']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAnW0rAKDY5"
      },
      "source": [
        "ta_rom['split']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD7CedwSKaoS"
      },
      "source": [
        "l1 = [1,4,2,3]\n",
        "l2 = [1,4,2,5]\n",
        "print(np.array(l1[1:-1])==np.array(l2[1:-1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}